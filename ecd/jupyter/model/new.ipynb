{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b0ed1e1c24428f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load processed data from: ../../data/sentences_eeg.pkl\n",
      "Successfully loaded processed data.\n",
      "Loaded 16270 items.\n",
      "Example of first item: H\n",
      "Example of first item: 1\n",
      "Example of first item: THE QUICK DOG JUMPED OVER\n",
      "Example of first item: [array([[-1.2835948 , -0.8696827 , -0.63357687, ..., -0.8235423 ,\n",
      "        -0.7878795 , -0.60256517],\n",
      "       [-1.289872  , -0.85476553, -0.64955145, ..., -0.6701227 ,\n",
      "        -0.5584078 , -0.4237045 ],\n",
      "       [-1.2481786 , -0.7888243 , -0.64355093, ..., -0.5354535 ,\n",
      "        -0.35739008, -0.2570909 ],\n",
      "       ...,\n",
      "       [ 0.6414426 ,  0.82679033,  0.8949905 , ...,  0.7397481 ,\n",
      "         0.8634642 ,  0.8789248 ],\n",
      "       [ 0.5591661 ,  0.81037956,  0.9656031 , ...,  0.7616999 ,\n",
      "         0.9122697 ,  0.9680821 ],\n",
      "       [ 0.43155485,  0.71449965,  0.9222388 , ...,  0.7240543 ,\n",
      "         0.8748701 ,  1.0227684 ]], dtype=float32)]\n",
      "Example of first item: [[1.490e-02 1.490e-02]\n",
      " [1.490e-02 1.490e-02]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [4.500e-03 4.500e-03]\n",
      " [4.500e-03 4.500e-03]\n",
      " [2.000e-04 2.000e-04]\n",
      " [2.000e-04 2.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.700e-03 1.700e-03]\n",
      " [1.700e-03 1.700e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [9.564e-01 9.564e-01]\n",
      " [9.564e-01 9.564e-01]\n",
      " [9.400e-03 9.400e-03]\n",
      " [9.400e-03 9.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [3.400e-03 3.400e-03]\n",
      " [3.400e-03 3.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.200e-03 1.200e-03]\n",
      " [1.200e-03 1.200e-03]\n",
      " [6.000e-04 6.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [1.200e-03 1.200e-03]\n",
      " [1.200e-03 1.200e-03]\n",
      " [2.500e-03 2.500e-03]\n",
      " [2.500e-03 2.500e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.100e-03 1.100e-03]\n",
      " [1.100e-03 1.100e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [5.000e-04 5.000e-04]\n",
      " [5.000e-04 5.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from bundle.DataCraft import * \n",
    "\n",
    "data = load_sentence_eeg_prob_data()\n",
    "\n",
    "if data:\n",
    "    # Print example of converted item\n",
    "    print(f\"Loaded {len(data)} items.\")\n",
    "    if data:\n",
    "        print(\"Example of first item:\", data[1][\"character\"])\n",
    "        print(\"Example of first item:\", data[1][\"char_idx_in_sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"eeg_chunk\"][0:1])\n",
    "        print(\"Example of first item:\", data[1][\"prob_chunk\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T09:53:35.639871900Z",
     "start_time": "2025-06-12T09:53:35.349694400Z"
    }
   },
   "id": "ab3dd2088ef08dd0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout, TimeDistributed\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "\n",
    "def create_cnn_model(eeg_input_shape, prob_input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a Convolutional Neural Network (CNN) model for character prediction\n",
    "    using EEG and probability chunk data.\n",
    "\n",
    "    The model consists of two branches:\n",
    "    1. EEG Branch: Processes EEG time-series data using 1D convolutional layers.\n",
    "    2. Probability Branch: Processes probability chunk data using 1D convolutional layers.\n",
    "\n",
    "    The outputs of both branches are concatenated and fed into dense layers\n",
    "    for final character classification.\n",
    "\n",
    "    Args:\n",
    "        eeg_input_shape (tuple): Shape of the EEG input data (e.g., (30, 78, 64)).\n",
    "        prob_input_shape (tuple): Shape of the probability input data (e.g., (78, 2)).\n",
    "        num_classes (int): Number of unique characters to classify (e.g., 26 for A-Z).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled CNN model.\n",
    "    \"\"\"\n",
    "    # EEG branch: Designed to capture temporal features from multi-channel EEG data\n",
    "    # Use TimeDistributed to apply Conv1D to each of the 30 sequences within the EEG data\n",
    "    eeg_input = Input(shape=eeg_input_shape, name='eeg_input') # e.g., (30, 78, 64)\n",
    "    \n",
    "    # Apply Conv1D and MaxPooling1D to each time step (78, 64) independently\n",
    "    eeg_conv1 = TimeDistributed(Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'))(eeg_input)\n",
    "    eeg_pool1 = TimeDistributed(MaxPooling1D(pool_size=2))(eeg_conv1)\n",
    "    eeg_dropout1 = TimeDistributed(Dropout(0.25))(eeg_pool1) \n",
    "    \n",
    "    eeg_conv2 = TimeDistributed(Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'))(eeg_dropout1)\n",
    "    eeg_pool2 = TimeDistributed(MaxPooling1D(pool_size=2))(eeg_conv2)\n",
    "    eeg_dropout2 = TimeDistributed(Dropout(0.25))(eeg_pool2) \n",
    "    \n",
    "    # Flatten the output of each time step, then flatten across time steps\n",
    "    eeg_flatten_timedist = TimeDistributed(Flatten())(eeg_dropout2)\n",
    "    eeg_flatten = Flatten()(eeg_flatten_timedist) # Flatten the (batch_size, 30, features) to (batch_size, 30 * features)\n",
    "\n",
    "    # Probability branch: Processes the probability chunk data\n",
    "    prob_input = Input(shape=prob_input_shape, name='prob_input')\n",
    "    prob_conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(prob_input)\n",
    "    prob_pool1 = MaxPooling1D(pool_size=2)(prob_conv1)\n",
    "    prob_dropout1 = Dropout(0.25)(prob_pool1) \n",
    "    prob_flatten = Flatten()(prob_dropout1)\n",
    "\n",
    "    # Concatenate branches: Merges the features extracted from both EEG and probability data\n",
    "    merged = concatenate([eeg_flatten, prob_flatten])\n",
    "\n",
    "    # Dense layers for classification\n",
    "    dense1 = Dense(256, activation='relu')(merged)\n",
    "    # Another dropout layer before the final output to further prevent overfitting\n",
    "    dropout_dense = Dropout(0.5)(dense1) \n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    output = Dense(num_classes, activation='softmax')(dropout_dense)\n",
    "\n",
    "    # Create the model with two inputs and one output\n",
    "    model = Model(inputs=[eeg_input, prob_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw data loaded from load_sentence_eeg_prob_data.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing \\'character\\', \\'eeg_chunk\\', and \\'prob_chunk\\'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - eeg_chunks (np.array): Preprocessed EEG data.\n",
    "            - prob_chunks (np.array): Preprocessed probability data.\n",
    "            - one_hot_characters (np.array): One-hot encoded character labels.\n",
    "            - classes (list): List of unique characters corresponding to the one-hot encoding.\n",
    "    \"\"\"\n",
    "    eeg_chunks = []\n",
    "    prob_chunks = []\n",
    "    characters = []\n",
    "\n",
    "    for item in data:\n",
    "        # Convert eeg_chunk to numpy array if it\\'s a list\n",
    "        if isinstance(item[\"eeg_chunk\"], list):\n",
    "            eeg_chunks.append(np.array(item[\"eeg_chunk\"])) # Convert inner list to numpy array\n",
    "        else:\n",
    "            eeg_chunks.append(item[\"eeg_chunk\"])\n",
    "        prob_chunks.append(item[\"prob_chunk\"])\n",
    "        characters.append(item[\"character\"])\n",
    "\n",
    "    eeg_chunks = np.array(eeg_chunks)\n",
    "    prob_chunks = np.array(prob_chunks)\n",
    "\n",
    "    # Normalize EEG data: Standard scaling is applied to EEG data to ensure zero mean and unit variance.\n",
    "    # This helps in stabilizing the training process and improving model performance.\n",
    "    eeg_chunks = (eeg_chunks - np.mean(eeg_chunks)) / np.std(eeg_chunks)\n",
    "\n",
    "    # One-hot encode characters: Converts categorical character labels into a numerical format\n",
    "    # that can be used by machine learning algorithms. Each character becomes a binary vector.\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    one_hot_characters = label_binarizer.fit_transform(characters)\n",
    "\n",
    "    return eeg_chunks, prob_chunks, one_hot_characters, label_binarizer.classes_\n",
    "\n",
    "# Check for GPU availability\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be set right at the start of the program\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        print(\"Using GPU for training.\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "else:\n",
    "    print(\"No GPU found, using CPU for training.\")\n",
    "\n",
    "print(\"Starting data preprocessing...\")\n",
    "\n",
    "\n",
    "if data is None:\n",
    "    print(\"Failed to load data. Exiting.\")\n",
    "else:\n",
    "    # 2. Preprocess data: Transforms raw data into a format suitable for model training.\n",
    "    eeg_chunks, prob_chunks, one_hot_characters, classes = preprocess_data(data)\n",
    "    print(\"Data preprocessing complete.\")\n",
    "\n",
    "    # 3. Split data into training and validation sets: Divides the dataset into two subsets\n",
    "    # to train the model on one and evaluate its performance on unseen data with the other.\n",
    "    eeg_train, eeg_val, prob_train, prob_val, char_train, char_val = train_test_split(\n",
    "        eeg_chunks, prob_chunks, one_hot_characters, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"EEG train shape: {eeg_train.shape}\")\n",
    "    print(f\"Prob train shape: {prob_train.shape}\")\n",
    "    print(f\"Character train shape: {char_train.shape}\")\n",
    "\n",
    "    # 4. Create and compile the model: Initializes the CNN model and configures it for training.\n",
    "    # \\'adam\\' optimizer is chosen for its efficiency, and \\'categorical_crossentropy\\' is used\n",
    "    # as the loss function for multi-class classification.\n",
    "    eeg_input_shape = eeg_train.shape[1:] # This will now be (30, 78, 64)\n",
    "    prob_input_shape = prob_train.shape[1:] # This will be (78, 2)\n",
    "    num_classes = char_train.shape[1]\n",
    "\n",
    "    model = create_cnn_model(eeg_input_shape, prob_input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # 5. Train the model: The core training loop where the model learns from the training data.\n",
    "    # \\'epochs\\' determines the number of complete passes through the training dataset.\n",
    "    # \\'batch_size\\' defines the number of samples per gradient update.\n",
    "    # \\'validation_data\\' is used to monitor the model\\'s performance on unseen data during training.\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        {'eeg_input': eeg_train, 'prob_input': prob_train},\n",
    "        char_train,\n",
    "        epochs=10,  # Number of epochs can be adjusted for better convergence\n",
    "        batch_size=32, # Batch size can be adjusted based on memory and performance\n",
    "        validation_data=(\n",
    "            {'eeg_input': eeg_val, 'prob_input': prob_val},\n",
    "            char_val\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel training complete.\")\n",
    "    print(\"Training History:\")\n",
    "    for key, value in history.history.items():\n",
    "        print(f\"  {key}: {value[-1]:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-06-12T09:53:35.639871900Z"
    }
   },
   "id": "9e25d8fffada0d06"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
