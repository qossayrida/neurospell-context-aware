{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b0ed1e1c24428f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load processed data from: ../../data/sentences_eeg.pkl\n",
      "Successfully loaded processed data.\n",
      "Loaded 16270 items.\n",
      "Example of first item: H\n",
      "Example of first item: 1\n",
      "Example of first item: THE QUICK DOG JUMPED OVER\n",
      "Example of first item: [array([[-1.2835948 , -0.8696827 , -0.63357687, ..., -0.8235423 ,\n",
      "        -0.7878795 , -0.60256517],\n",
      "       [-1.289872  , -0.85476553, -0.64955145, ..., -0.6701227 ,\n",
      "        -0.5584078 , -0.4237045 ],\n",
      "       [-1.2481786 , -0.7888243 , -0.64355093, ..., -0.5354535 ,\n",
      "        -0.35739008, -0.2570909 ],\n",
      "       ...,\n",
      "       [ 0.6414426 ,  0.82679033,  0.8949905 , ...,  0.7397481 ,\n",
      "         0.8634642 ,  0.8789248 ],\n",
      "       [ 0.5591661 ,  0.81037956,  0.9656031 , ...,  0.7616999 ,\n",
      "         0.9122697 ,  0.9680821 ],\n",
      "       [ 0.43155485,  0.71449965,  0.9222388 , ...,  0.7240543 ,\n",
      "         0.8748701 ,  1.0227684 ]], dtype=float32)]\n",
      "Example of first item: [[1.490e-02 1.490e-02]\n",
      " [1.490e-02 1.490e-02]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [4.500e-03 4.500e-03]\n",
      " [4.500e-03 4.500e-03]\n",
      " [2.000e-04 2.000e-04]\n",
      " [2.000e-04 2.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.700e-03 1.700e-03]\n",
      " [1.700e-03 1.700e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [9.564e-01 9.564e-01]\n",
      " [9.564e-01 9.564e-01]\n",
      " [9.400e-03 9.400e-03]\n",
      " [9.400e-03 9.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [3.400e-03 3.400e-03]\n",
      " [3.400e-03 3.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.200e-03 1.200e-03]\n",
      " [1.200e-03 1.200e-03]\n",
      " [6.000e-04 6.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [1.200e-03 1.200e-03]\n",
      " [1.200e-03 1.200e-03]\n",
      " [2.500e-03 2.500e-03]\n",
      " [2.500e-03 2.500e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.100e-03 1.100e-03]\n",
      " [1.100e-03 1.100e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [5.000e-04 5.000e-04]\n",
      " [5.000e-04 5.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from bundle.DataCraft import * \n",
    "\n",
    "data = load_sentence_eeg_prob_data()\n",
    "\n",
    "if data:\n",
    "    # Print example of converted item\n",
    "    print(f\"Loaded {len(data)} items.\")\n",
    "    if data:\n",
    "        print(\"Example of first item:\", data[1][\"character\"])\n",
    "        print(\"Example of first item:\", data[1][\"char_idx_in_sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"eeg_chunk\"][0:1])\n",
    "        print(\"Example of first item:\", data[1][\"prob_chunk\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T07:05:27.540581500Z",
     "start_time": "2025-06-12T07:05:27.360670500Z"
    }
   },
   "id": "ab3dd2088ef08dd0"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data loading and preprocessing...\n",
      "Attempting to load processed data from: ../../data/sentences_eeg.pkl\n",
      "Successfully loaded processed data.\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 9.08 GiB for an array with shape (16270, 30, 78, 64) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mMemoryError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 112\u001B[0m\n\u001B[0;32m    109\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to load data. Exiting.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    111\u001B[0m     \u001B[38;5;66;03m# 2. Preprocess data: Transforms raw data into a format suitable for model training.\u001B[39;00m\n\u001B[1;32m--> 112\u001B[0m     eeg_chunks, prob_chunks, one_hot_characters, classes \u001B[38;5;241m=\u001B[39m \u001B[43mpreprocess_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    113\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mData preprocessing complete.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    115\u001B[0m     \u001B[38;5;66;03m# 3. Split data into training and validation sets: Divides the dataset into two subsets\u001B[39;00m\n\u001B[0;32m    116\u001B[0m     \u001B[38;5;66;03m# to train the model on one and evaluate its performance on unseen data with the other.\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[2], line 90\u001B[0m, in \u001B[0;36mpreprocess_data\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m     87\u001B[0m     prob_chunks\u001B[38;5;241m.\u001B[39mappend(item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprob_chunk\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m     88\u001B[0m     characters\u001B[38;5;241m.\u001B[39mappend(item[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcharacter\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 90\u001B[0m eeg_chunks \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43meeg_chunks\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     91\u001B[0m prob_chunks \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(prob_chunks)\n\u001B[0;32m     93\u001B[0m \u001B[38;5;66;03m# Normalize EEG data: Standard scaling is applied to EEG data to ensure zero mean and unit variance.\u001B[39;00m\n\u001B[0;32m     94\u001B[0m \u001B[38;5;66;03m# This helps in stabilizing the training process and improving model performance.\u001B[39;00m\n",
      "\u001B[1;31mMemoryError\u001B[0m: Unable to allocate 9.08 GiB for an array with shape (16270, 30, 78, 64) and data type float32"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "\n",
    "def create_cnn_model(eeg_input_shape, prob_input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    Creates a Convolutional Neural Network (CNN) model for character prediction\n",
    "    using EEG and probability chunk data.\n",
    "\n",
    "    The model consists of two branches:\n",
    "    1. EEG Branch: Processes EEG time-series data using 1D convolutional layers.\n",
    "    2. Probability Branch: Processes probability chunk data using 1D convolutional layers.\n",
    "\n",
    "    The outputs of both branches are concatenated and fed into dense layers\n",
    "    for final character classification.\n",
    "\n",
    "    Args:\n",
    "        eeg_input_shape (tuple): Shape of the EEG input data (e.g., (78, 64)).\n",
    "        prob_input_shape (tuple): Shape of the probability input data (e.g., (78, 2)).\n",
    "        num_classes (int): Number of unique characters to classify (e.g., 26 for A-Z).\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The compiled CNN model.\n",
    "    \"\"\"\n",
    "    # EEG branch: Designed to capture temporal features from multi-channel EEG data\n",
    "    eeg_input = Input(shape=eeg_input_shape, name='eeg_input')\n",
    "    # Conv1D layer with 64 filters, kernel size 3, ReLU activation, and same padding\n",
    "    eeg_conv1 = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same')(eeg_input)\n",
    "    # MaxPooling1D to reduce dimensionality and provide translational invariance\n",
    "    eeg_pool1 = MaxPooling1D(pool_size=2)(eeg_conv1)\n",
    "    # Dropout layer to prevent overfitting by randomly setting a fraction of input units to 0 at each update during training\n",
    "    eeg_dropout1 = Dropout(0.25)(eeg_pool1) \n",
    "    eeg_conv2 = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(eeg_dropout1)\n",
    "    eeg_pool2 = MaxPooling1D(pool_size=2)(eeg_conv2)\n",
    "    eeg_dropout2 = Dropout(0.25)(eeg_pool2) \n",
    "    # Flatten the output to feed into dense layers\n",
    "    eeg_flatten = Flatten()(eeg_dropout2)\n",
    "\n",
    "    # Probability branch: Processes the probability chunk data\n",
    "    prob_input = Input(shape=prob_input_shape, name='prob_input')\n",
    "    prob_conv1 = Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(prob_input)\n",
    "    prob_pool1 = MaxPooling1D(pool_size=2)(prob_conv1)\n",
    "    prob_dropout1 = Dropout(0.25)(prob_pool1) \n",
    "    prob_flatten = Flatten()(prob_dropout1)\n",
    "\n",
    "    # Concatenate branches: Merges the features extracted from both EEG and probability data\n",
    "    merged = concatenate([eeg_flatten, prob_flatten])\n",
    "\n",
    "    # Dense layers for classification\n",
    "    dense1 = Dense(256, activation='relu')(merged)\n",
    "    # Another dropout layer before the final output to further prevent overfitting\n",
    "    dropout_dense = Dropout(0.5)(dense1) \n",
    "    # Output layer with softmax activation for multi-class classification\n",
    "    output = Dense(num_classes, activation='softmax')(dropout_dense)\n",
    "\n",
    "    # Create the model with two inputs and one output\n",
    "    model = Model(inputs=[eeg_input, prob_input], outputs=output)\n",
    "    return model\n",
    "\n",
    "def preprocess_data(data):\n",
    "    \"\"\"\n",
    "    Preprocesses the raw data loaded from load_sentence_eeg_prob_data.\n",
    "\n",
    "    Args:\n",
    "        data (list): A list of dictionaries, each containing \\'character\\', \\'eeg_chunk\\', and \\'prob_chunk\\'.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing:\n",
    "            - eeg_chunks (np.array): Preprocessed EEG data.\n",
    "            - prob_chunks (np.array): Preprocessed probability data.\n",
    "            - one_hot_characters (np.array): One-hot encoded character labels.\n",
    "            - classes (list): List of unique characters corresponding to the one-hot encoding.\n",
    "    \"\"\"\n",
    "    eeg_chunks = []\n",
    "    prob_chunks = []\n",
    "    characters = []\n",
    "\n",
    "    for item in data:\n",
    "        # Convert eeg_chunk to numpy array if it's a list\n",
    "        if isinstance(item[\"eeg_chunk\"], list):\n",
    "            eeg_chunks.append(np.array(item[\"eeg_chunk\"])) # Convert inner list to numpy array\n",
    "        else:\n",
    "            eeg_chunks.append(item[\"eeg_chunk\"])\n",
    "        prob_chunks.append(item[\"prob_chunk\"])\n",
    "        characters.append(item[\"character\"])\n",
    "\n",
    "    eeg_chunks = np.array(eeg_chunks)\n",
    "    prob_chunks = np.array(prob_chunks)\n",
    "\n",
    "    # Normalize EEG data: Standard scaling is applied to EEG data to ensure zero mean and unit variance.\n",
    "    # This helps in stabilizing the training process and improving model performance.\n",
    "    eeg_chunks = (eeg_chunks - np.mean(eeg_chunks)) / np.std(eeg_chunks)\n",
    "\n",
    "    # One-hot encode characters: Converts categorical character labels into a numerical format\n",
    "    # that can be used by machine learning algorithms. Each character becomes a binary vector.\n",
    "    label_binarizer = LabelBinarizer()\n",
    "    one_hot_characters = label_binarizer.fit_transform(characters)\n",
    "\n",
    "    return eeg_chunks, prob_chunks, one_hot_characters, label_binarizer.classes_\n",
    "\n",
    "print(\"Starting data loading and preprocessing...\")\n",
    "# 1. Load data: This step loads the actual dataset from the provided pickle file.\n",
    "data = load_sentence_eeg_prob_data()\n",
    "\n",
    "if data is None:\n",
    "    print(\"Failed to load data. Exiting.\")\n",
    "else:\n",
    "    # 2. Preprocess data: Transforms raw data into a format suitable for model training.\n",
    "    eeg_chunks, prob_chunks, one_hot_characters, classes = preprocess_data(data)\n",
    "    print(\"Data preprocessing complete.\")\n",
    "\n",
    "    # 3. Split data into training and validation sets: Divides the dataset into two subsets\n",
    "    # to train the model on one and evaluate its performance on unseen data with the other.\n",
    "    eeg_train, eeg_val, prob_train, prob_val, char_train, char_val = train_test_split(\n",
    "        eeg_chunks, prob_chunks, one_hot_characters, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\"EEG train shape: {eeg_train.shape}\")\n",
    "    print(f\"Prob train shape: {prob_train.shape}\")\n",
    "    print(f\"Character train shape: {char_train.shape}\")\n",
    "\n",
    "    # 4. Create and compile the model: Initializes the CNN model and configures it for training.\n",
    "    # \\'adam\\' optimizer is chosen for its efficiency, and \\'categorical_crossentropy\\' is used\n",
    "    # as the loss function for multi-class classification.\n",
    "    eeg_input_shape = eeg_train.shape[1:]\n",
    "    prob_input_shape = prob_train.shape[1:]\n",
    "    num_classes = char_train.shape[1]\n",
    "\n",
    "    model = create_cnn_model(eeg_input_shape, prob_input_shape, num_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    print(\"Model summary:\")\n",
    "    model.summary()\n",
    "\n",
    "    # 5. Train the model: The core training loop where the model learns from the training data.\n",
    "    # \\'epochs\\' determines the number of complete passes through the training dataset.\n",
    "    # \\'batch_size\\' defines the number of samples per gradient update.\n",
    "    # \\'validation_data\\' is used to monitor the model\\'s performance on unseen data during training.\n",
    "    print(\"\\nStarting model training...\")\n",
    "    history = model.fit(\n",
    "        {'eeg_input': eeg_train, 'prob_input': prob_train},\n",
    "        char_train,\n",
    "        epochs=10,  # Number of epochs can be adjusted for better convergence\n",
    "        batch_size=32, # Batch size can be adjusted based on memory and performance\n",
    "        validation_data=(\n",
    "            {'eeg_input': eeg_val, 'prob_input': prob_val},\n",
    "            char_val\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(\"\\nModel training complete.\")\n",
    "    print(\"Training History:\")\n",
    "    for key, value in history.history.items():\n",
    "        print(f\"  {key}: {value[-1]:.4f}\")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-12T07:05:59.044652Z",
     "start_time": "2025-06-12T07:05:27.540581500Z"
    }
   },
   "id": "92c5b28b38ac5601"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
