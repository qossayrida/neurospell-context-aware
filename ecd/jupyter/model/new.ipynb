{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b0ed1e1c24428f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load processed data from: ../../data/sentences_eeg.pkl\n",
      "Successfully loaded processed data.\n",
      "Loaded 16270 items.\n",
      "Example of first item: H\n",
      "Example of first item: 1\n",
      "Example of first item: THE QUICK DOG JUMPED OVER\n",
      "Example of first item: [array([[ 0.22776094,  0.46970123,  0.70324725, ...,  0.14635862,\n",
      "         1.0238168 , -0.9463383 ],\n",
      "       [ 0.08742154,  0.2278345 ,  0.34740365, ..., -0.1466554 ,\n",
      "         0.61614096, -1.0300957 ],\n",
      "       [-0.06776999, -0.04082734, -0.06774805, ..., -0.3656779 ,\n",
      "         0.22884615, -1.0415238 ],\n",
      "       ...,\n",
      "       [-1.3213954 , -0.93309444, -0.07610805, ..., -0.46508783,\n",
      "        -0.98153764,  0.23164782],\n",
      "       [-1.4564943 , -1.0470396 , -0.07786182, ..., -0.53110176,\n",
      "        -1.0577328 ,  0.05647672],\n",
      "       [-1.6199298 , -1.1973821 , -0.17433581, ..., -0.52290905,\n",
      "        -1.0656656 , -0.04597962]], dtype=float32)]\n",
      "Example of first item: [[1.490e-02 1.490e-02]\n",
      " [4.000e-04 4.000e-04]\n",
      " [4.000e-04 4.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [4.500e-03 4.500e-03]\n",
      " [2.000e-04 2.000e-04]\n",
      " [6.000e-04 6.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.700e-03 1.700e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [9.564e-01 9.564e-01]\n",
      " [9.400e-03 9.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [3.400e-03 3.400e-03]\n",
      " [1.000e-04 1.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.200e-03 1.200e-03]\n",
      " [6.000e-04 6.000e-04]\n",
      " [1.200e-03 1.200e-03]\n",
      " [2.500e-03 2.500e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [1.100e-03 1.100e-03]\n",
      " [0.000e+00 0.000e+00]\n",
      " [5.000e-04 5.000e-04]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]\n",
      " [0.000e+00 0.000e+00]]\n"
     ]
    }
   ],
   "source": [
    "from bundle.DataCraft import * \n",
    "\n",
    "data = load_sentence_eeg_prob_data()\n",
    "\n",
    "if data:\n",
    "    # Print example of converted item\n",
    "    print(f\"Loaded {len(data)} items.\")\n",
    "    if data:\n",
    "        print(\"Example of first item:\", data[1][\"character\"])\n",
    "        print(\"Example of first item:\", data[1][\"char_idx_in_sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"sentence\"])\n",
    "        print(\"Example of first item:\", data[1][\"eeg_chunk\"][0:1])\n",
    "        print(\"Example of first item:\", data[1][\"prob_chunk\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:20:12.375505300Z",
     "start_time": "2025-06-08T13:20:12.197593900Z"
    }
   },
   "id": "ab3dd2088ef08dd0"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n",
      "Data size: 16270 items\n",
      "Data prepared:\n",
      "  EEG samples shape: (0,)\n",
      "  Probability samples shape: (0,)\n",
      "  Number of classes: 0\n",
      "  Classes: []\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[21], line 328\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Classes: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabel_encoder\u001B[38;5;241m.\u001B[39mclasses_\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    327\u001B[0m \u001B[38;5;66;03m# Split data into train, validation, and test sets\u001B[39;00m\n\u001B[1;32m--> 328\u001B[0m X_train_eeg, X_test_eeg, X_train_prob, X_test_prob, y_train, y_test \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_test_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    329\u001B[0m \u001B[43m    \u001B[49m\u001B[43meeg_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprob_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoded_labels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrandom_state\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m42\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstratify\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoded_labels\u001B[49m\n\u001B[0;32m    330\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    332\u001B[0m X_train_eeg, X_val_eeg, X_train_prob, X_val_prob, y_train, y_val \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[0;32m    333\u001B[0m     X_train_eeg, X_train_prob, y_train, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.25\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my_train\n\u001B[0;32m    334\u001B[0m )\n\u001B[0;32m    336\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTrain set: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(y_train)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m samples\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    210\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    211\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    212\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    213\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    214\u001B[0m         )\n\u001B[0;32m    215\u001B[0m     ):\n\u001B[1;32m--> 216\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    217\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    218\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    219\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    220\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    221\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    222\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    223\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    224\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    225\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    226\u001B[0m     )\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2851\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[0;32m   2848\u001B[0m arrays \u001B[38;5;241m=\u001B[39m indexable(\u001B[38;5;241m*\u001B[39marrays)\n\u001B[0;32m   2850\u001B[0m n_samples \u001B[38;5;241m=\u001B[39m _num_samples(arrays[\u001B[38;5;241m0\u001B[39m])\n\u001B[1;32m-> 2851\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[43m_validate_shuffle_split\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   2852\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_samples\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdefault_test_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.25\u001B[39;49m\n\u001B[0;32m   2853\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   2855\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m shuffle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[0;32m   2856\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m stratify \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2481\u001B[0m, in \u001B[0;36m_validate_shuffle_split\u001B[1;34m(n_samples, test_size, train_size, default_test_size)\u001B[0m\n\u001B[0;32m   2478\u001B[0m n_train, n_test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mint\u001B[39m(n_train), \u001B[38;5;28mint\u001B[39m(n_test)\n\u001B[0;32m   2480\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m-> 2481\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   2482\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWith n_samples=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, test_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m and train_size=\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m, the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2483\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresulting train set will be empty. Adjust any of the \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   2484\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124maforementioned parameters.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(n_samples, test_size, train_size)\n\u001B[0;32m   2485\u001B[0m     )\n\u001B[0;32m   2487\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m n_train, n_test\n",
      "\u001B[1;31mValueError\u001B[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time\n",
    "\n",
    "# Define a custom dataset class\n",
    "class EEGProbDataset(Dataset):\n",
    "    def __init__(self, eeg_data, prob_data, labels):\n",
    "        self.eeg_data = eeg_data\n",
    "        self.prob_data = prob_data\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        eeg = self.eeg_data[idx]\n",
    "        prob = self.prob_data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return eeg, prob, label\n",
    "\n",
    "# Define the RNN model\n",
    "class EEGProbRNN(nn.Module):\n",
    "    def __init__(self, eeg_input_size, prob_input_size, hidden_size, num_layers, num_classes, dropout=0.2):\n",
    "        super(EEGProbRNN, self).__init__()\n",
    "        \n",
    "        # EEG branch\n",
    "        self.eeg_rnn = nn.LSTM(\n",
    "            input_size=eeg_input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Probability branch\n",
    "        self.prob_rnn = nn.LSTM(\n",
    "            input_size=prob_input_size,\n",
    "            hidden_size=hidden_size // 2,  # Smaller hidden size for prob branch\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for EEG\n",
    "        self.eeg_attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism for Prob\n",
    "        self.prob_attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        # Fusion layer\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2 + hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, eeg, prob):\n",
    "        # Process EEG data\n",
    "        eeg_out, _ = self.eeg_rnn(eeg)\n",
    "        \n",
    "        # Apply attention to EEG\n",
    "        eeg_attn_weights = self.eeg_attention(eeg_out)\n",
    "        eeg_attn_weights = torch.softmax(eeg_attn_weights, dim=1)\n",
    "        eeg_context = torch.sum(eeg_out * eeg_attn_weights, dim=1)\n",
    "        \n",
    "        # Process probability data\n",
    "        prob_out, _ = self.prob_rnn(prob)\n",
    "        \n",
    "        # Apply attention to Prob\n",
    "        prob_attn_weights = self.prob_attention(prob_out)\n",
    "        prob_attn_weights = torch.softmax(prob_attn_weights, dim=1)\n",
    "        prob_context = torch.sum(prob_out * prob_attn_weights, dim=1)\n",
    "        \n",
    "        # Concatenate features from both branches\n",
    "        combined = torch.cat((eeg_context, prob_context), dim=1)\n",
    "        \n",
    "        # Fusion layer\n",
    "        fused = self.fusion(combined)\n",
    "        \n",
    "        # Output layer\n",
    "        output = self.fc(fused)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Function to prepare data for training\n",
    "def prepare_data(data):\n",
    "    eeg_samples = []\n",
    "    prob_samples = []\n",
    "    labels = []\n",
    "    \n",
    "    for item in data:\n",
    "        if 'eeg_chunk' in item and item['eeg_chunk'] and 'converted_data' in item and 'character' in item:\n",
    "            # Get the first EEG sample from the chunk\n",
    "            eeg_sample = item['eeg_chunk'][0]\n",
    "            \n",
    "            # Get the probability matrix\n",
    "            prob_sample = item['converted_data']\n",
    "            \n",
    "            # Get the character label\n",
    "            label = item['character']\n",
    "            \n",
    "            eeg_samples.append(eeg_sample)\n",
    "            prob_samples.append(prob_sample)\n",
    "            labels.append(label)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    eeg_samples = np.array(eeg_samples)\n",
    "    prob_samples = np.array(prob_samples)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(labels)\n",
    "    \n",
    "    return eeg_samples, prob_samples, encoded_labels, label_encoder\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=10, device='cpu'):\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accs = []\n",
    "    val_accs = []\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_val_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for eeg, prob, labels in train_loader:\n",
    "            eeg = eeg.float().to(device)\n",
    "            prob = prob.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(eeg, prob)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item() * eeg.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accs.append(epoch_acc)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for eeg, prob, labels in val_loader:\n",
    "                eeg = eeg.float().to(device)\n",
    "                prob = prob.float().to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(eeg, prob)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_running_loss += loss.item() * eeg.size(0)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_epoch_loss = val_running_loss / val_total\n",
    "        val_epoch_acc = val_correct / val_total\n",
    "        val_losses.append(val_epoch_loss)\n",
    "        val_accs.append(val_epoch_acc)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_epoch_acc > best_val_acc:\n",
    "            best_val_acc = val_epoch_acc\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        \n",
    "        # Print statistics\n",
    "        time_elapsed = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1}/{num_epochs} | Time: {time_elapsed:.2f}s')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}')\n",
    "        print(f'Val Loss: {val_epoch_loss:.4f} | Val Acc: {val_epoch_acc:.4f}')\n",
    "        print('-' * 60)\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# Function to plot training history\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_dir='./results'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Plot losses\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title('Loss over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{save_dir}/loss_history.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot accuracies\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accs, label='Train Accuracy')\n",
    "    plt.plot(val_accs, label='Validation Accuracy')\n",
    "    plt.title('Accuracy over epochs')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f'{save_dir}/accuracy_history.png')\n",
    "    plt.close()\n",
    "\n",
    "# Function to evaluate model on test set\n",
    "def evaluate_model(model, test_loader, criterion, device='cpu', label_encoder=None):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for eeg, prob, labels in test_loader:\n",
    "            eeg = eeg.float().to(device)\n",
    "            prob = prob.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(eeg, prob)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            test_loss += loss.item() * eeg.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    test_loss = test_loss / total\n",
    "    test_acc = correct / total\n",
    "    \n",
    "    print(f'Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}')\n",
    "    \n",
    "    # If label encoder is provided, print some examples\n",
    "    if label_encoder is not None:\n",
    "        print(\"\\nSample predictions:\")\n",
    "        for i in range(min(10, len(all_labels))):\n",
    "            true_label = label_encoder.inverse_transform([all_labels[i]])[0]\n",
    "            pred_label = label_encoder.inverse_transform([all_preds[i]])[0]\n",
    "            print(f\"True: {true_label} | Predicted: {pred_label} | {'✓' if true_label == pred_label else '✗'}\")\n",
    "    \n",
    "    return test_loss, test_acc, all_preds, all_labels\n",
    "\n",
    "# Save model function\n",
    "def save_model(model, label_encoder, save_dir='../../models/rnn'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), f'{save_dir}/eeg_prob_rnn_model.pth')\n",
    "    \n",
    "    # Save label encoder\n",
    "    with open(f'{save_dir}/label_encoder.pkl', 'wb') as f:\n",
    "        pickle.dump(label_encoder, f)\n",
    "    \n",
    "    print(f\"Model and label encoder saved to {save_dir}\")\n",
    "\n",
    "# Main execution\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "if data:\n",
    "    # Prepare data\n",
    "    print(\"Preparing data...\")\n",
    "    print(f\"Data size: {len(data)} items\")\n",
    "    eeg_samples, prob_samples, encoded_labels, label_encoder = prepare_data(data)\n",
    "    \n",
    "    print(f\"Data prepared:\")\n",
    "    print(f\"  EEG samples shape: {eeg_samples.shape}\")\n",
    "    print(f\"  Probability samples shape: {prob_samples.shape}\")\n",
    "    print(f\"  Number of classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"  Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    # Split data into train, validation, and test sets\n",
    "    X_train_eeg, X_test_eeg, X_train_prob, X_test_prob, y_train, y_test = train_test_split(\n",
    "        eeg_samples, prob_samples, encoded_labels, test_size=0.2, random_state=42, stratify=encoded_labels\n",
    "    )\n",
    "    \n",
    "    X_train_eeg, X_val_eeg, X_train_prob, X_val_prob, y_train, y_val = train_test_split(\n",
    "        X_train_eeg, X_train_prob, y_train, test_size=0.25, random_state=42, stratify=y_train\n",
    "    )\n",
    "    \n",
    "    print(f\"Train set: {len(y_train)} samples\")\n",
    "    print(f\"Validation set: {len(y_val)} samples\")\n",
    "    print(f\"Test set: {len(y_test)} samples\")\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = EEGProbDataset(X_train_eeg, X_train_prob, y_train)\n",
    "    val_dataset = EEGProbDataset(X_val_eeg, X_val_prob, y_val)\n",
    "    test_dataset = EEGProbDataset(X_test_eeg, X_test_prob, y_test)\n",
    "    \n",
    "    # Create data loaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Get input dimensions from data\n",
    "    eeg_input_size = eeg_samples.shape[2]  # Number of channels\n",
    "    prob_input_size = prob_samples.shape[2]  # Number of columns in prob matrix\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    # Initialize model\n",
    "    hidden_size = 128\n",
    "    num_layers = 2\n",
    "    model = EEGProbRNN(\n",
    "        eeg_input_size=eeg_input_size,\n",
    "        prob_input_size=prob_input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        num_classes=num_classes,\n",
    "        dropout=0.3\n",
    "    ).to(device)\n",
    "    \n",
    "    # Print model summary\n",
    "    print(model)\n",
    "    \n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Train model\n",
    "    print(\"Starting training...\")\n",
    "    model, train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=device\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    test_loss, test_acc, all_preds, all_labels = evaluate_model(\n",
    "        model, test_loader, criterion, device=device, label_encoder=label_encoder\n",
    "    )\n",
    "    \n",
    "    # Save model\n",
    "    save_model(model, label_encoder)\n",
    "    \n",
    "    print(\"\\nTraining and evaluation complete!\")\n",
    "else:\n",
    "    print(\"Failed to load data. Please check the file path.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-08T13:20:12.507433800Z",
     "start_time": "2025-06-08T13:20:12.372489100Z"
    }
   },
   "id": "92c5b28b38ac5601"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
