{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CNN2a.ipynb",
   "provenance": [
    {
     "file_id": "11CKvMWbYn2HYpLAfu2xRhNmw9HDnkG5B",
     "timestamp": 1575826262900
    },
    {
     "file_id": "1nbofAi2dLnhJvkcLl1Xag7I1vx4MPR2s",
     "timestamp": 1575818931262
    },
    {
     "file_id": "1tTQLWB1ve20LC-Cxir5CrLKQ_vDyzeI2",
     "timestamp": 1575815207334
    }
   ],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAgCWuBBRPSJ",
    "colab_type": "text"
   },
   "source": [
    "# CNN_B SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9nw7AE3rLWlk",
    "colab_type": "code",
    "outputId": "e0effdc6-0da1-4a1a-fe03-fe3a94f4abf8",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425785236,
     "user_tz": -60,
     "elapsed": 5307,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 200
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.438450800Z",
     "start_time": "2025-05-14T14:32:09.309060800Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np     \n",
    "import random           \n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import warnings\n",
    "import string\n",
    "import os\n",
    "\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import mne\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "contributor_selected = \"II\"                                 \n",
    "contributor_train_file_path = '../data/Contributor_' + contributor_selected + '_Train.mat'\n",
    "contributor_test_file_path = '../data/Contributor_' + contributor_selected + '_Test.mat'\n",
    "channel_name_file_path = '../data/channels.csv'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Channel selection\n",
    "channels = [10, 33, 48, 50, 52, 55, 59, 61]            "
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mne'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 23\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m scale\n\u001B[0;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m MinMaxScaler\n\u001B[1;32m---> 23\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmne\u001B[39;00m\n\u001B[0;32m     28\u001B[0m contributor_selected \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mII\u001B[39m\u001B[38;5;124m\"\u001B[39m                                 \n\u001B[0;32m     29\u001B[0m contributor_train_file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/Contributor_\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;241m+\u001B[39m contributor_selected \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_Train.mat\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'mne'"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVrJuHCMFCSl",
    "colab_type": "text"
   },
   "source": [
    "# Training set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signals from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 8)**;\n",
    "6.   Obtain **(noP300 / P300)** class ratio to balance out dataset during training;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "78aU0w6-Lp0y",
    "colab_type": "code",
    "outputId": "ff8a0f22-2471-4da8-a514-8c73a8f1fa28",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425785491,
     "user_tz": -60,
     "elapsed": 5544,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.440445800Z",
     "start_time": "2025-05-14T14:32:18.439444400Z"
    }
   },
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "from bundle.DataCraft import * \n",
    "\n",
    "data_train = loadmat(contributor_train_file_path)\n",
    "\n",
    "\n",
    "# Take only the 8 selected channels of the singal [Fz, Cz, Pz, P3, P4, Po7, Po8, Oz]\n",
    "signals_train = data_train['Signal']\n",
    "signals_train = signals_train[:, :, channels]\n",
    "flashing_train = data_train['Flashing']\n",
    "stimulus_train = data_train['StimulusType']\n",
    "word_train = data_train['TargetChar']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_train = (len(signals_train)) * (len(signals_train[0])) / (sampling_frequency * 60)\n",
    "trials_train = len(word_train[0])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print_data(signals_train,word_train, contributor_selected, sampling_frequency)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXuVC7puKcLt",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.442449900Z",
     "start_time": "2025-05-14T14:32:18.441445Z"
    }
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "    \n",
    "    # Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_train[0])))\n",
    "\n",
    "signals_train = signals_train[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_train = flashing_train[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_train = stimulus_train[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_train[0])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bGaRKwu9MIvP",
    "colab_type": "code",
    "outputId": "85f686e4-eaf4-4197-8374-a8e5b8d6331e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425786791,
     "user_tz": -60,
     "elapsed": 6805,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.443449200Z",
     "start_time": "2025-05-14T14:32:18.443449200Z"
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 78, 8)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTVrfKzJH2aE",
    "colab_type": "text"
   },
   "source": [
    "# Testing set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signas from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 8)**;\n",
    "6.   Calculate weights vector to balance samples importance and obtain correct accuracy estimation;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nm-bZZsKTliz",
    "colab_type": "code",
    "outputId": "8b30f513-b130-4de4-8cbf-8a35263b7f67",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425793782,
     "user_tz": -60,
     "elapsed": 13745,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.456444800Z",
     "start_time": "2025-05-14T14:32:18.445446700Z"
    }
   },
   "source": [
    "data_test = loadmat(contributor_test_file_path)\n",
    "\n",
    "# Get the variables of interest from the loaded dictionary\n",
    "signals_test = data_test['Signal']\n",
    "signals_test = signals_test[:, :, channels]\n",
    "flashing_test = data_test['Flashing']\n",
    "word_test =  data_test['TargetChar']\n",
    "stimulus_code_test = data_test['StimulusCode']\n",
    "\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_test = (len(signals_test)) * (len(signals_test[0])) / (sampling_frequency * 60)\n",
    "trials_test = len(word_test[0])\n",
    "samples_per_trial_test = len(signals_test[0])\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print_data(signals_test,word_test, contributor_selected, sampling_frequency)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SvCresQJm25A",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.447445700Z"
    }
   },
   "source": [
    "# Create characters matrix\n",
    "char_matrix = [[0 for j in range(6)] for i in range(6)]\n",
    "s = string.ascii_uppercase + '1' + '2' + '3' + '4' + '5' + '6' + '7' + '8' + '9' + '_'\n",
    "\n",
    "# Append cols and rows in a list\n",
    "list_matrix = []\n",
    "for i in range(6):\n",
    "    col = [s[j] for j in range(i, 36, 6)]\n",
    "    list_matrix.append(col)\n",
    "for i in range(6):\n",
    "    row = [s[j] for j in range(i * 6, i * 6 + 6)]\n",
    "    list_matrix.append(row)\n",
    "\n",
    "# Create StimulusType array for the test set (missing from the given database)\n",
    "stimulus_test = [[0 for j in range(samples_per_trial_test)] for i in range(trials_test)]\n",
    "stimulus_test = np.array(stimulus_test)\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    counter=0\n",
    "    for sample in range(samples_per_trial_test):\n",
    "        index = int(stimulus_code_test[trial, sample]) - 1\n",
    "        if not index == -1:\n",
    "            if word_test[0][trial] in list_matrix[index]:\n",
    "                stimulus_test[trial, sample] = 1\n",
    "            else:\n",
    "                stimulus_test[trial, sample] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UNy5oI8ooMAw",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.449445200Z"
    }
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_test):\n",
    "    signals_test[trial, :, :] = signal.filtfilt(b, a, signals_test[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_test[0])))\n",
    "\n",
    "signals_test = signals_test[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_test = flashing_test[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_test = stimulus_test[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_test[0])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "48sta55OVXFy",
    "colab_type": "code",
    "outputId": "7f8a7ef1-12a1-4f7c-d8f4-7843f2a4ab36",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425796086,
     "user_tz": -60,
     "elapsed": 15994,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.451445Z"
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "samples_per_trial_test = len(signals_train[0])\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "windowed_stimulus = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    for sample in (range(samples_per_trial_test)):\n",
    "        if (sample == 0) or (flashing_test[trial, sample-1] == 0 and flashing_test[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_test[trial, lower_sample:upper_sample, :]\n",
    "            # Extracting number of row/col in a window\n",
    "            number_stimulus = int(stimulus_code_test[trial, sample])\n",
    "            windowed_stimulus.append(number_stimulus)\n",
    "            # Features extraction\n",
    "            test_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_test[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                test_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                test_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get test weights to take into account the number of classes \n",
    "test_weights = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 1:\n",
    "        test_weights.append(len(test_labels)/count_positive)\n",
    "    else:\n",
    "        test_weights.append(len(test_labels)/count_negative)\n",
    "test_weights = np.array(test_weights)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# 3D tensor (SAMPLES, 78, 64)\n",
    "dim_test = test_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_test))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(test_features)):\n",
    "    test_features[pattern] = scale(test_features[pattern], axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VK4MFXTjHoM",
    "colab_type": "text"
   },
   "source": [
    "# CNN_B model definition, training and testing\n",
    "\n",
    "1.   Function definiton for randomization of weights and biases;\n",
    "2.   **Scaled_tanh(x)** activation function definition;\n",
    "3.   ANN model definition (2 Conv1D layers, 2 dense layers);\n",
    "4.   Training of the network over weighted datasets;\n",
    "5.   CNN2a performance assessment;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eTIc_w_TcOmK",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.452448200Z"
    }
   },
   "source": [
    "# Randomizing function for bias and weights of the network\n",
    "def cecotti_normal(shape, dtype = None, partition_info = None):\n",
    "    if len(shape) == 1:\n",
    "        fan_in = shape[0]\n",
    "    elif len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        receptive_field_size = 1\n",
    "        for dim in shape[:-2]:\n",
    "            receptive_field_size *= dim\n",
    "        fan_in = shape[-2] * receptive_field_size\n",
    "    return K.random_normal(shape, mean = 0.0, stddev = (1.0 / fan_in))\n",
    "\n",
    "# Custom tanh activation function\n",
    "def scaled_tanh(z):\n",
    "    return 1.7159 * K.tanh((2.0 / 3.0) * z)\n",
    "\n",
    "# Build the model\n",
    "def CNN_B_model(channels=8, filters=10):\n",
    "    model = Sequential([\n",
    "        Conv1D(\n",
    "            filters = filters,\n",
    "            kernel_size = 1,\n",
    "            padding = \"same\",\n",
    "            bias_initializer = cecotti_normal,\n",
    "            kernel_initializer = cecotti_normal,\n",
    "            use_bias = True,\n",
    "            activation = scaled_tanh,\n",
    "            input_shape = (78, channels)\n",
    "        ),\n",
    "        Conv1D(\n",
    "            filters = 50,\n",
    "            kernel_size = 13,\n",
    "            padding = \"valid\",\n",
    "            strides = 11,\n",
    "            bias_initializer = cecotti_normal,\n",
    "            kernel_initializer = cecotti_normal,\n",
    "            use_bias = True,\n",
    "            activation = scaled_tanh,\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"sigmoid\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 200\n",
    "VALID_SPLIT = 0.2\n",
    "SHUFFLE = 1 # set to 1 to shuffle subsets during training\n",
    "\n",
    "# Model summary\n",
    "# Model definition\n",
    "model = CNN_B_model(channels=8, filters=10)\n",
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GEPpgyxyTl-u",
    "colab_type": "code",
    "outputId": "a2fc69c4-652c-4a13-ae19-5b23e5c45ac2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425832466,
     "user_tz": -60,
     "elapsed": 52318,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.455450500Z"
    }
   },
   "source": [
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                              mode = 'min', \n",
    "                              patience = 50, \n",
    "                              restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history = model.fit(x=train_features, \n",
    "                    y=train_labels, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_split=VALID_SPLIT, \n",
    "                    callbacks=[earlystop],\n",
    "                    shuffle=SHUFFLE,\n",
    "                    class_weight={0: 1., 1: train_ratio})\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history.history['val_loss'][best_epoch]}\")\n",
    "\n",
    "# Reload the best weights manually\n",
    "best_weights = model.get_weights()\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Define a new model to reuse the best configuration\n",
    "best_model = CNN_B_model(channels=8, filters=10)\n",
    "best_model.set_weights(best_weights)\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJHaj2f1VFYZ",
    "colab_type": "code",
    "outputId": "c218f69b-ffc9-4e6c-f66e-cdf013e4c2a2",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425833149,
     "user_tz": -60,
     "elapsed": 52981,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.457446100Z"
    }
   },
   "source": [
    "plt.figure(figsize=(15,4))\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Plots of loss curves during training\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label=\"training loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plots of accuracy curves during training\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label=\"training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U9-pnknkO9PN",
    "colab_type": "code",
    "outputId": "e58dfabf-3f94-460c-a03b-58c1b0b087a7",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425835503,
     "user_tz": -60,
     "elapsed": 55312,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.461446100Z",
     "start_time": "2025-05-14T14:32:18.459446Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import csv\n",
    "\n",
    "# Read CSV file into a Python list\n",
    "all_electrode_names = []\n",
    "\n",
    "with open(channel_name_file_path, \"r\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        all_electrode_names.extend(row)  # Add each row to the list (handles single-column CSV)\n",
    "\n",
    "\n",
    "# Create array has the electrode_names has this index channels = [10, 33, 48, 50, 52, 55, 59, 61]  \n",
    "electrode_names = [all_electrode_names[i] for i in channels]\n",
    "print(electrode_names)\n",
    "\n",
    "# Remove the electrodes 'Po8' from the electrode_names list\n",
    "electrode_names.remove('Po7')\n",
    "electrode_names.remove('Po8')\n",
    "print(electrode_names)\n",
    "\n",
    "# Create array has the original index for electrode we need to remove from the weights\n",
    "index = [5,6]\n",
    "print(index)\n",
    "    \n",
    "montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "\n",
    "# Create random weights for 10 filters (20 channels)\n",
    "n_channels = len(electrode_names)\n",
    "n_filters = 10\n",
    "\n",
    "# Create a list of weights for all filters\n",
    "nf = []\n",
    "for filt in range(10):\n",
    "    nf.append(abs(np.array(best_model.get_weights()[0])[0, :, filt]))\n",
    "# remove the channels from the weights \n",
    "nf = np.delete(nf, index, axis=1)\n",
    "\n",
    "# Create an Info object with EEG channel names\n",
    "info = mne.create_info(ch_names=electrode_names, sfreq=1000, ch_types=\"eeg\")\n",
    "info.set_montage(montage)\n",
    "\n",
    "# Plot topomap for each filter\n",
    "# Set give me array of background color\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6), facecolor=\"#000000\")  # Set overall background to black\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_filters):\n",
    "    ax = axes[i]\n",
    "    mne.viz.plot_topomap(\n",
    "        nf[i], info, axes=ax, cmap=\"Spectral_r\", sphere=1.2, show=False  # Adjust `sphere` to scale brain size\n",
    "    )\n",
    "    ax.set_title(f\"Filter #{i + 1}\", color=\"white\")  # Set title color to white for better visibility\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EzbVkDAwUmfa",
    "colab_type": "code",
    "outputId": "6af973b1-0600-4db6-e37b-c7134d956121",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425836443,
     "user_tz": -60,
     "elapsed": 56237,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "ExecuteTime": {
     "end_time": "2025-05-14T14:32:18.461446100Z",
     "start_time": "2025-05-14T14:32:18.461446100Z"
    }
   },
   "source": [
    "# Accuracy over the testing set\n",
    "predictions = best_model.predict(test_features)\n",
    "predictions = np.round(predictions)\n",
    "\n",
    "score = np.array(best_model.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights))\n",
    "print(\"Model performance on test set:\\t[ Loss: {}\\tAccuracy: {} ]\".format(*score.round(4)))\n",
    "print(\"\\nPredictions: {}\\nSolutions:   {}\".format(list(map(int, predictions))[:50], list(map(int, test_labels))[:50]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DVKxqyKZpyg8",
    "colab_type": "code",
    "outputId": "9ec2f454-69ad-4640-dadf-cd855dea7781",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425836873,
     "user_tz": -60,
     "elapsed": 56651,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.463445Z"
    }
   },
   "source": [
    "# Weighted confusion matrix (noP300: 80%, P300: 20%)\n",
    "data_train = confusion_matrix(y_true=test_labels, y_pred=predictions, sample_weight=test_weights)\n",
    "\n",
    "# Normalized confusion matrix (values in range 0-1)\n",
    "data_norm = data_train / np.full(data_train.shape, len(test_labels))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "df_cm = pd.DataFrame(data_norm, columns=np.unique(test_labels), index = np.unique(test_labels))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (6,5))\n",
    "sns.set(font_scale = 1.4)\n",
    "cm = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws = {\"size\": 16}, vmin=0, vmax=1)\n",
    "cm.axes.set_title(\"CNN B confusion matrix\\n\", fontsize=20)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "CkdtywrVpz0r",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.465449500Z"
    }
   },
   "source": [
    "# Model metrics (sens, spec, ppv, npv)\n",
    "def model_metrics(conf_matrix):\n",
    "    tn, fp, fn, tp = list(data_norm.flatten())\n",
    "    sens = round(tp/(tp+fn),4) # Sensitivity\n",
    "    spec = round(tn/(tn+fp),4) # Specificity\n",
    "    ppv = round(tp/(tp+fp),4) # Positive Predicted Value\n",
    "    npv = round(tn/(tn+fn),4) # Negative Predicted Value\n",
    "    return {\"Sensitivity\":sens, \"Specificity\":spec, \"PPV\":ppv, \"NPV\":npv}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4zpRP7GZp0B-",
    "colab_type": "code",
    "outputId": "88ca45b9-9fee-4d9f-bc04-2de7a2657415",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576425836875,
     "user_tz": -60,
     "elapsed": 56622,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "ExecuteTime": {
     "start_time": "2025-05-14T14:32:18.467445700Z"
    }
   },
   "source": [
    "# Put model metrics into a table\n",
    "metrics = model_metrics(data_norm)\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(5,1))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Hide graph outlines\n",
    "for item in [fig, ax]:\n",
    "    item.patch.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Table definition\n",
    "table = ax.table(cellText=[list(metrics.values())], \n",
    "                     colLabels=list(metrics.keys()),\n",
    "                     loc=\"center\",\n",
    "                     cellLoc=\"center\",\n",
    "                     colColours=[\"c\"]*4)\n",
    "table.set_fontsize(16)\n",
    "table.scale(2,2)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
