{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create EEG Sentences\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e74093c265c6a51"
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "import numpy as np                \n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "contributor_selected = \"I\"                                 \n",
    "contributor_train_file_path = '../data/Contributor_' + contributor_selected + '_Train.mat'\n",
    "contributor_test_file_path = '../data/Contributor_' + contributor_selected + '_Test.mat'\n",
    "channel_name_file_path = '../data/channels.csv'\n",
    "channels = [i for i in range(64)]\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:15.068620Z",
     "start_time": "2025-05-15T12:11:15.028131400Z"
    }
   },
   "id": "189d752f13e679c2"
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Contributor     Sampling Freq. (Hz)  Recording (min)      Trials     Spelled Word                  \n",
      "==============================================================================================================\n",
      "I               240.00               46.01                85         EAEVQTDOJG8RBRGONCEDHCTUIDBPUH\n",
      "                                                                     MEM6OUXOCFOUKWA4VJEFRZROLHYNQD\n",
      "                                                                     W_EKTLBWXEPOUIKZERYOOTHQI     \n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "from bundle.DataCraft import * \n",
    "\n",
    "\n",
    "data_train = loadmat(contributor_train_file_path)\n",
    "signals_train = data_train['Signal']\n",
    "flashing_train = data_train['Flashing']\n",
    "stimulus_train = data_train['StimulusType']\n",
    "word_train = data_train['TargetChar']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_train = (len(signals_train)) * (len(signals_train[0])) / (sampling_frequency * 60)\n",
    "trials_train = len(word_train[0])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print_data(signals_train, word_train, contributor_selected, sampling_frequency)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:15.146719700Z",
     "start_time": "2025-05-15T12:11:15.038681400Z"
    }
   },
   "id": "374de39660eac2dd"
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n"
     ]
    }
   ],
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_train[0])))\n",
    "\n",
    "signals_train = signals_train[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_train = flashing_train[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_train = stimulus_train[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_train[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:18.445050200Z",
     "start_time": "2025-05-15T12:11:15.153596100Z"
    }
   },
   "id": "a22fcd76032f5d23"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: (15300, 78, 64)\n"
     ]
    }
   ],
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 64, 78)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:23.621197600Z",
     "start_time": "2025-05-15T12:11:18.452052500Z"
    }
   },
   "id": "c1ed8d5edff2a1bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# =======================================================\n",
    "# =======================================================\n",
    "# ======================================================="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48782c33112f2e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: (15300, 78, 64)\n",
      "Generated 50 sentences from 715 characters.\n",
      "Sentence 0: HE WRITES GAMES.\n",
      "Sentence 1: I SEE WATER.\n",
      "Sentence 2: HE READS PAPER.\n",
      "Sentence 3: THEY WRITE GAMES.\n",
      "Sentence 4: I MAKE FOOD.\n",
      "Character to sentence mapping (first 10 characters):\n",
      "Char 0: 'H', Sentence 0, Position 0\n",
      "Char 1: 'E', Sentence 0, Position 1\n",
      "Char 2: ' ', Sentence 0, Position 2\n",
      "Char 3: 'W', Sentence 0, Position 3\n",
      "Char 4: 'R', Sentence 0, Position 4\n",
      "Char 5: 'I', Sentence 0, Position 5\n",
      "Char 6: 'T', Sentence 0, Position 6\n",
      "Char 7: 'E', Sentence 0, Position 7\n",
      "Char 8: 'S', Sentence 0, Position 8\n",
      "Char 9: ' ', Sentence 0, Position 9\n",
      "\n",
      "Mapping 715 P300 responses to characters in sentences\n",
      "Created ECD training dataset with 715 samples\n",
      "Created 665 context samples for NLP model training\n",
      "Created 665 combined samples for DFM training\n",
      "\n",
      "Example of ECD training data:\n",
      "Target character: 'H'\n",
      "From sentence 0: 'HE WRITES GAMES.'\n",
      "At position 0\n",
      "Feature shape: (78, 64)\n",
      "\n",
      "Example of NLP context data:\n",
      "Context: 'H'\n",
      "Target: 'E'\n",
      "From sentence 0: 'HE WRITES GAMES.'\n",
      "\n",
      "Example of combined DFM data:\n",
      "Context: 'H'\n",
      "Target: 'E'\n",
      "From sentence 0: 'HE WRITES GAMES.'\n",
      "Feature shape: (78, 64)\n"
     ]
    }
   ],
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 64, 78)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)\n",
    "\n",
    "# Generate synthetic sentences from collected data\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_sentences(num_sentences=50, mean_length=10, std_length=3):\n",
    "    \"\"\"Generate random but somewhat plausible sentences.\"\"\"\n",
    "    subjects = ['I', 'YOU', 'HE', 'SHE', 'THEY', 'WE']\n",
    "    verbs = ['WRITE', 'LIKE', 'EAT', 'READ', 'SEE', 'USE', 'HELP', 'MAKE', 'PLAY', 'WANT']\n",
    "    verbs_s = ['WRITES', 'LIKES', 'EATS', 'READS', 'SEES', 'USES', 'HELPS', 'MAKES', 'PLAYS', 'WANTS']\n",
    "    objects = ['FOOD', 'MUSIC', 'BOOKS', 'GAMES', 'POEMS', 'PIZZA', 'WATER', 'PAPER', 'PHONES', 'EMAILS']\n",
    "    \n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        subject = random.choice(subjects)\n",
    "        if subject in ['I', 'YOU', 'THEY', 'WE']:\n",
    "            verb = random.choice(verbs)\n",
    "        else:\n",
    "            verb = random.choice(verbs_s)\n",
    "        obj = random.choice(objects)\n",
    "        \n",
    "        sentence = f\"{subject} {verb} {obj}\"\n",
    "        # Add a period sometimes\n",
    "        if random.random() > 0.3:\n",
    "            sentence += \".\"\n",
    "            \n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Generate sentences\n",
    "num_sentences = 50\n",
    "sentences = generate_sentences(num_sentences)\n",
    "\n",
    "# Flatten sentences into a character stream\n",
    "all_chars = []\n",
    "char_to_sentence_map = []  # Will store (sentence_idx, position_in_sentence) for each char\n",
    "\n",
    "for sent_idx, sentence in enumerate(sentences):\n",
    "    for pos_idx, char in enumerate(sentence):\n",
    "        all_chars.append(char)\n",
    "        char_to_sentence_map.append((sent_idx, pos_idx))\n",
    "\n",
    "# Print the first few sentences\n",
    "print(f\"Generated {len(sentences)} sentences from {len(all_chars)} characters.\")\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")\n",
    "\n",
    "# Print the character to sentence mapping for the first 10 characters\n",
    "print(\"Character to sentence mapping (first 10 characters):\")\n",
    "for i in range(min(10, len(all_chars))):\n",
    "    sent_idx, pos_idx = char_to_sentence_map[i]\n",
    "    print(f\"Char {i}: '{all_chars[i]}', Sentence {sent_idx}, Position {pos_idx}\")\n",
    "\n",
    "# Now let's prepare the data structure for the ECD model training\n",
    "# Based on the Wadsworth BCI dataset documentation, we use the proper variable names\n",
    "# In the dataset, StimulusCode indicates which row/column was flashed:\n",
    "# 1-6 for columns, 7-12 for rows\n",
    "\n",
    "# First, let's create a mapping of our extracted P300 responses to characters for the ECD training\n",
    "# We'll create a simplified version since we don't have access to the exact StimulusCode values\n",
    "\n",
    "# Create a random mapping of P300 responses to characters for demonstration\n",
    "# In a real implementation, we'd use the actual target characters from the dataset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming we have a certain number of P300 responses identified by train_labels == 1\n",
    "p300_indices = np.where(train_labels == 1)[0]\n",
    "\n",
    "# Map a subset of P300 responses to characters in our generated sentences\n",
    "num_chars_to_map = min(len(p300_indices), len(all_chars))\n",
    "print(f\"\\nMapping {num_chars_to_map} P300 responses to characters in sentences\")\n",
    "\n",
    "ecd_train_data = []\n",
    "for i in range(num_chars_to_map):\n",
    "    ecd_train_data.append({\n",
    "        'feature': train_features[p300_indices[i]],\n",
    "        'target_char': all_chars[i],\n",
    "        'sentence_idx': char_to_sentence_map[i][0],\n",
    "        'position_idx': char_to_sentence_map[i][1]\n",
    "    })\n",
    "\n",
    "print(f\"Created ECD training dataset with {len(ecd_train_data)} samples\")\n",
    "\n",
    "# Create context data for the NLP model\n",
    "sentence_contexts = []\n",
    "for i in range(len(sentences)):\n",
    "    # For each position in the sentence, create a context\n",
    "    sentence = sentences[i]\n",
    "    for j in range(len(sentence)):\n",
    "        context = sentence[:j]\n",
    "        target = sentence[j]\n",
    "        if context:  # Only add if there's actual context\n",
    "            sentence_contexts.append({\n",
    "                'context': context,\n",
    "                'target': target,\n",
    "                'sentence_idx': i,\n",
    "                'position_idx': j\n",
    "            })\n",
    "\n",
    "print(f\"Created {len(sentence_contexts)} context samples for NLP model training\")\n",
    "\n",
    "# Finally, create the combined dataset for the Decision Fusion Model\n",
    "dfm_train_data = []\n",
    "for ecd_sample in ecd_train_data:\n",
    "    sent_idx = ecd_sample['sentence_idx']\n",
    "    pos_idx = ecd_sample['position_idx']\n",
    "    \n",
    "    # Find matching NLP context\n",
    "    matching_contexts = [c for c in sentence_contexts \n",
    "                         if c['sentence_idx'] == sent_idx and c['position_idx'] == pos_idx]\n",
    "    \n",
    "    if matching_contexts:\n",
    "        nlp_sample = matching_contexts[0]\n",
    "        dfm_train_data.append({\n",
    "            'ecd_feature': ecd_sample['feature'],\n",
    "            'nlp_context': nlp_sample['context'],\n",
    "            'target_char': ecd_sample['target_char'],\n",
    "            'sentence_idx': sent_idx,\n",
    "            'position_idx': pos_idx\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(dfm_train_data)} combined samples for DFM training\")\n",
    "\n",
    "# Save a few examples of the generated data\n",
    "print(\"\\nExample of ECD training data:\")\n",
    "if ecd_train_data:\n",
    "    example = ecd_train_data[0]\n",
    "    print(f\"Target character: '{example['target_char']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "    print(f\"At position {example['position_idx']}\")\n",
    "    print(f\"Feature shape: {example['feature'].shape}\")\n",
    "    \n",
    "\n",
    "print(\"\\nExample of NLP context data:\")\n",
    "if sentence_contexts:\n",
    "    example = sentence_contexts[0]\n",
    "    print(f\"Context: '{example['context']}'\")\n",
    "    print(f\"Target: '{example['target']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "\n",
    "print(\"\\nExample of combined DFM data:\")\n",
    "if dfm_train_data:\n",
    "    example = dfm_train_data[0]\n",
    "    print(f\"Context: '{example['nlp_context']}'\")\n",
    "    print(f\"Target: '{example['target_char']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "    print(f\"Feature shape: {example['ecd_feature'].shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:28.811050300Z",
     "start_time": "2025-05-15T12:11:23.622250800Z"
    }
   },
   "id": "af5439f909b4474d"
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 62\n",
      "Model loaded from ../model/api/char_predictor.pth.\n",
      "\n",
      "Added NLP probability vectors to each ECD sample.\n",
      "Target character: I\n",
      "Context prefix: 'HE WR'\n",
      "values: {'E': 0.2353, 'O': 0.2265, 'I': 0.111, 'A': 0.0716, 'o': 0.0601, 'T': 0.0355}\n"
     ]
    }
   ],
   "source": [
    "# Add next-char prediction probabilities to each DFM sample as additional feature\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "\n",
    "\n",
    "# Define character set (a-z, A-Z, 0-9)\n",
    "all_chars = list(string.ascii_lowercase + string.ascii_uppercase + string.digits)\n",
    "char2idx = {ch: idx for idx, ch in enumerate(all_chars)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "vocab_size = len(all_chars)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "class CharPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):\n",
    "        super(CharPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embed)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "    \n",
    "def load_model(path=\"../model/api/char_predictor.pth\"):\n",
    "    model = CharPredictor(vocab_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_next_chars(model, sentence, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = [char2idx[ch] for ch in sentence if ch in char2idx]\n",
    "        if not input_seq:\n",
    "            raise ValueError(\"Input sentence must contain at least one known character.\")\n",
    "\n",
    "        input_seq = torch.tensor(input_seq).unsqueeze(0)\n",
    "        output = model(input_seq)\n",
    "        probs = F.softmax(output, dim=-1).squeeze(0)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "        result = {}\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            result[idx2char[idx.item()]] = round(prob.item(), 4)\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load pretrained NLP model\n",
    "nlp_model = load_model()\n",
    "\n",
    "# Add NLP vector to each ECD sample\n",
    "for sample in ecd_train_data:\n",
    "    sent_idx = sample[\"sentence_idx\"]\n",
    "    pos_idx = sample[\"position_idx\"]\n",
    "    \n",
    "    if pos_idx == 0 :\n",
    "        continue\n",
    "    \n",
    "    full_sentence = sentences[sent_idx]\n",
    "    prefix = full_sentence[:pos_idx]  # context before the target char\n",
    "    \n",
    "    nlp_prob_dict = predict_next_chars(nlp_model, prefix, top_k=6)\n",
    "\n",
    "    # Convert to vector form over consistent vocabulary\n",
    "    # prob_vector = np.array([nlp_prob_dict.get(char, 0.0) for char in idx2char.values()])\n",
    "    \n",
    "    sample[\"nlp_prob_vector\"] = nlp_prob_dict\n",
    "\n",
    "print(\"\\nAdded NLP probability vectors to each ECD sample.\")\n",
    "\n",
    "# Show an example with the added vector\n",
    "if ecd_train_data:\n",
    "    example = ecd_train_data[5]\n",
    "    print(f\"Target character: {example['target_char']}\")\n",
    "    print(f\"Context prefix: '{sentences[example['sentence_idx']][:example['position_idx']]}'\")\n",
    "    # print(f\"NLP prob vector shape: {example['nlp_prob_vector'].shape}\")\n",
    "    print(f\"values: {example['nlp_prob_vector']}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-15T12:11:29.108453900Z",
     "start_time": "2025-05-15T12:11:28.815978Z"
    }
   },
   "id": "e5f5f70563f4e1cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
