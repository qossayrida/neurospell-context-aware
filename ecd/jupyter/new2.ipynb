{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-12T16:41:20.155266100Z",
     "start_time": "2025-05-12T16:41:11.482493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for Contributor I...\n",
      "Data loaded. Trials: 85, Sampling rate: 120Hz\n",
      "Extracted 1 sentences from 85 characters\n",
      "Created mapping for 84 characters\n",
      "\n",
      "Sample sentences:\n",
      "Sentence 0: EAEVQTDOJG8RBRGONCEDHCTUIDBPUHMEM6OUXOCFOUKWA4VJEFRZROLHYNQDW EKTLBWXEPOUIKZERYOOTHQI\n",
      "\n",
      "Created sentence-based dataset with 1 sentences\n",
      "\n",
      "Created 84 contextual sequences with context size 2\n",
      "\n",
      "Sample contexts:\n",
      "Target: 'E', Context: 'AE', Sentence: 0, Position: 0\n",
      "Target: 'A', Context: 'EEV', Sentence: 0, Position: 1\n",
      "Target: 'E', Context: 'EAVQ', Sentence: 0, Position: 2\n",
      "\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (180x15360 and 1104x128)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 559\u001B[0m\n\u001B[0;32m    547\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m    548\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m: model,\n\u001B[0;32m    549\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124msentences\u001B[39m\u001B[38;5;124m'\u001B[39m: sentences,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    554\u001B[0m         \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcontexts\u001B[39m\u001B[38;5;124m'\u001B[39m: contexts\n\u001B[0;32m    555\u001B[0m     }\n\u001B[0;32m    557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    558\u001B[0m     \u001B[38;5;66;03m# Execute the integration\u001B[39;00m\n\u001B[1;32m--> 559\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[43mintegrate_sentence_context\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mI\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    561\u001B[0m     \u001B[38;5;66;03m# Access components\u001B[39;00m\n\u001B[0;32m    562\u001B[0m     model \u001B[38;5;241m=\u001B[39m results[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "Cell \u001B[1;32mIn[5], line 530\u001B[0m, in \u001B[0;36mintegrate_sentence_context\u001B[1;34m(contributor_selected)\u001B[0m\n\u001B[0;32m    528\u001B[0m model \u001B[38;5;241m=\u001B[39m ContextEnhancedP300Model()\n\u001B[0;32m    529\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mTraining model...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 530\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_sequences\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m16\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    532\u001B[0m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[0;32m    533\u001B[0m model\u001B[38;5;241m.\u001B[39msave_model(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m../model/contextual_p300_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcontributor_selected\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pth\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[1;32mIn[5], line 344\u001B[0m, in \u001B[0;36mContextEnhancedP300Model.train\u001B[1;34m(self, X_sequences, y_sequences, contexts, epochs, batch_size, validate)\u001B[0m\n\u001B[0;32m    341\u001B[0m labels \u001B[38;5;241m=\u001B[39m batch_y[i]\n\u001B[0;32m    343\u001B[0m \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[1;32m--> 344\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    345\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcriterion(outputs, labels)\n\u001B[0;32m    347\u001B[0m \u001B[38;5;66;03m# Backward and optimize\u001B[39;00m\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[5], line 285\u001B[0m, in \u001B[0;36mContextualP300Classifier.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    282\u001B[0m x \u001B[38;5;241m=\u001B[39m x_spatial_flat\n\u001B[0;32m    284\u001B[0m \u001B[38;5;66;03m# Apply fully connected layers\u001B[39;00m\n\u001B[1;32m--> 285\u001B[0m x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfc1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    286\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(x)\n\u001B[0;32m    287\u001B[0m x \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msigmoid(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x))\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32mD:\\University\\Year 4 semester 2\\ENCS5300\\BrainWaveResearch\\ecd\\venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (180x15360 and 1104x128)"
     ]
    }
   ],
   "source": [
    "# Sentence Context Integration for EEG-based BCI\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def load_and_preprocess_data(file_path, apply_filtering=True):\n",
    "    \"\"\"\n",
    "    Load and preprocess EEG data\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    data = loadmat(file_path)\n",
    "    signals = data['Signal']\n",
    "    flashing = data['Flashing']\n",
    "    stimulus_type = data['StimulusType']\n",
    "    target_chars = data['TargetChar']\n",
    "    \n",
    "    # Get data dimensions\n",
    "    trials = len(target_chars[0])\n",
    "    sampling_frequency = 240\n",
    "    \n",
    "    if apply_filtering:\n",
    "        # Apply 4th-order Butterworth bandpass filter (0.1-20 Hz)\n",
    "        b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "        for trial in range(trials):\n",
    "            signals[trial, :, :] = signal.filtfilt(b, a, signals[trial, :, :], axis=0)\n",
    "    \n",
    "    # Downsample from 240Hz to 120Hz\n",
    "    down_sampling_frequency = 120\n",
    "    scale_factor = round(sampling_frequency / down_sampling_frequency)\n",
    "    \n",
    "    signals = signals[:, 0:-1:scale_factor, :]\n",
    "    flashing = flashing[:, 0:-1:scale_factor]\n",
    "    stimulus_type = stimulus_type[:, 0:-1:scale_factor]\n",
    "    \n",
    "    return signals, flashing, stimulus_type, target_chars, trials, down_sampling_frequency\n",
    "\n",
    "def extract_sentences_from_char_sequence(target_chars):\n",
    "    \"\"\"\n",
    "    Extract sentences from character sequence\n",
    "    \"\"\"\n",
    "    chars_sequence = []\n",
    "    for i in range(len(target_chars[0])):\n",
    "        char = ''.join([c for c in target_chars[0][i] if c.strip()])\n",
    "        chars_sequence.append(char)\n",
    "    \n",
    "    # Convert to string for easier visualization\n",
    "    full_text = ''.join(chars_sequence)\n",
    "    \n",
    "    # Split into sentences based on special characters or patterns\n",
    "    sentences = []\n",
    "    current_sentence = \"\"\n",
    "    \n",
    "    for char in chars_sequence:\n",
    "        if char in ['_']:  # Space character\n",
    "            current_sentence += \" \"\n",
    "        elif char in ['.', '!', '?']:  # End of sentence markers\n",
    "            current_sentence += char\n",
    "            sentences.append(current_sentence.strip())\n",
    "            current_sentence = \"\"\n",
    "        else:\n",
    "            current_sentence += char\n",
    "    \n",
    "    # Add last sentence if not empty\n",
    "    if current_sentence.strip():\n",
    "        sentences.append(current_sentence.strip())\n",
    "    \n",
    "    return chars_sequence, sentences\n",
    "\n",
    "def create_char_to_sentence_mapping(chars_sequence, sentences):\n",
    "    \"\"\"\n",
    "    Create mapping from character index to sentence index and position\n",
    "    \"\"\"\n",
    "    char_to_sentence = {}\n",
    "    char_idx = 0\n",
    "    \n",
    "    for sent_idx, sentence in enumerate(sentences):\n",
    "        # Remove spaces for matching with original char sequence\n",
    "        clean_sentence = sentence.replace(\" \", \"\")\n",
    "        \n",
    "        for pos_in_sent, _ in enumerate(clean_sentence):\n",
    "            if char_idx < len(chars_sequence):\n",
    "                char_to_sentence[char_idx] = {\n",
    "                    'sentence_idx': sent_idx,\n",
    "                    'position_in_sentence': pos_in_sent,\n",
    "                    'character': chars_sequence[char_idx],\n",
    "                    'full_sentence': sentence\n",
    "                }\n",
    "                char_idx += 1\n",
    "    \n",
    "    return char_to_sentence\n",
    "\n",
    "def extract_trial_features(signals, flashing, stimulus_type, trial_idx, \n",
    "                          window_duration=650, sampling_rate=120):\n",
    "    \"\"\"\n",
    "    Extract features and labels for a single trial\n",
    "    \"\"\"\n",
    "    # Calculate window samples\n",
    "    window_samples = round(sampling_rate * (window_duration / 1000))\n",
    "    samples_per_trial = len(flashing[trial_idx])\n",
    "    \n",
    "    trial_features = []\n",
    "    trial_labels = []\n",
    "    trial_stimulus_codes = []\n",
    "    flash_indices = []\n",
    "    \n",
    "    for sample in range(samples_per_trial):\n",
    "        if (sample == 0) or (flashing[trial_idx, sample - 1] == 0 and flashing[trial_idx, sample] == 1):\n",
    "            # This is a flash onset\n",
    "            lower_sample = sample\n",
    "            upper_sample = min(sample + window_samples, samples_per_trial)\n",
    "            \n",
    "            # Extract window\n",
    "            window = signals[trial_idx, lower_sample:upper_sample, :]\n",
    "            \n",
    "            # Check if window is complete (some might be cut off at the end)\n",
    "            if window.shape[0] == window_samples:\n",
    "                trial_features.append(window)\n",
    "                flash_indices.append(sample)\n",
    "                \n",
    "                # Add label (1 for P300, 0 for no P300)\n",
    "                if stimulus_type[trial_idx, sample] == 1:\n",
    "                    trial_labels.append(1)\n",
    "                else:\n",
    "                    trial_labels.append(0)\n",
    "    \n",
    "    return np.array(trial_features), np.array(trial_labels), flash_indices\n",
    "\n",
    "def create_sentence_based_dataset(signals, flashing, stimulus_type, target_chars, trials, char_to_sentence):\n",
    "    \"\"\"\n",
    "    Create a dataset with sentence context\n",
    "    \"\"\"\n",
    "    # Dataset structure\n",
    "    sentence_data = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    # Extract features for each trial/character\n",
    "    for trial in range(trials):\n",
    "        # Extract features for this trial\n",
    "        trial_features, trial_labels, _ = extract_trial_features(\n",
    "            signals, flashing, stimulus_type, trial)\n",
    "        \n",
    "        # Normalize features\n",
    "        for i in range(len(trial_features)):\n",
    "            trial_features[i] = scale(trial_features[i], axis=0)\n",
    "        \n",
    "        # Get sentence context for this character\n",
    "        if trial in char_to_sentence:\n",
    "            sent_idx = char_to_sentence[trial]['sentence_idx'] \n",
    "            pos = char_to_sentence[trial]['position_in_sentence']\n",
    "            char = char_to_sentence[trial]['character']\n",
    "            \n",
    "            # Store in structured format\n",
    "            sentence_data[sent_idx][pos] = {\n",
    "                'character': char,\n",
    "                'features': trial_features,\n",
    "                'labels': trial_labels,\n",
    "                'trial_idx': trial\n",
    "            }\n",
    "    \n",
    "    return sentence_data\n",
    "\n",
    "def create_contextual_sequences(sentence_data, context_size=3):\n",
    "    \"\"\"\n",
    "    Create sequences with context_size characters before and after each target\n",
    "    \"\"\"\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    contexts = []\n",
    "    \n",
    "    for sent_idx, sent_data in sentence_data.items():\n",
    "        # Get all positions in this sentence\n",
    "        positions = sorted(list(sent_data.keys()))\n",
    "        \n",
    "        for i, pos in enumerate(positions):\n",
    "            # Get context positions\n",
    "            context_positions = []\n",
    "            for c in range(-context_size, context_size + 1):\n",
    "                if c != 0 and 0 <= i + c < len(positions):\n",
    "                    context_positions.append(positions[i + c])\n",
    "            \n",
    "            target_features = sent_data[pos]['features']\n",
    "            target_labels = sent_data[pos]['labels']\n",
    "            target_char = sent_data[pos]['character']\n",
    "            \n",
    "            # Get context features\n",
    "            context_chars = []\n",
    "            for ctx_pos in context_positions:\n",
    "                ctx_char = sent_data[ctx_pos]['character']\n",
    "                context_chars.append(ctx_char)\n",
    "            \n",
    "            # Create context string\n",
    "            context_str = ''.join(context_chars)\n",
    "            \n",
    "            # Add to dataset\n",
    "            X_sequences.append(target_features)\n",
    "            y_sequences.append(target_labels)\n",
    "            contexts.append({\n",
    "                'target_char': target_char,\n",
    "                'context': context_str,\n",
    "                'sentence_idx': sent_idx,\n",
    "                'position': pos\n",
    "            })\n",
    "    \n",
    "    return X_sequences, y_sequences, contexts\n",
    "\n",
    "def create_sequence_batches(X_sequences, y_sequences, contexts, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches for training\n",
    "    \"\"\"\n",
    "    indices = np.arange(len(X_sequences))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for start_idx in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[start_idx:start_idx + batch_size]\n",
    "        \n",
    "        batch_X = [X_sequences[i] for i in batch_indices]\n",
    "        batch_y = [y_sequences[i] for i in batch_indices]\n",
    "        batch_contexts = [contexts[i] for i in batch_indices]\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        batch_X = [torch.tensor(x, dtype=torch.float32) for x in batch_X]\n",
    "        batch_y = [torch.tensor(y, dtype=torch.float32) for y in batch_y]\n",
    "        \n",
    "        yield batch_X, batch_y, batch_contexts\n",
    "\n",
    "class ContextualP300Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for P300 classification with contextual information\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=64, input_time_points=78, \n",
    "                 filter_size=10, hidden_size=128, dropout_rate=0.5):\n",
    "        super(ContextualP300Classifier, self).__init__()\n",
    "        \n",
    "        # Spatial filtering - reduce 64 channels to smaller representation\n",
    "        self.spatial_filter = nn.Conv2d(1, filter_size, (input_channels, 1))\n",
    "        \n",
    "        # Temporal convolution\n",
    "        self.temporal_conv = nn.Conv2d(1, filter_size, (1, 10))\n",
    "        \n",
    "        # Feature size after convolutions\n",
    "        self.feature_size = filter_size * (input_time_points - 10 + 1)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(self.feature_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Input shape: [batch_size, time_points, channels]\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Spatial filtering\n",
    "        # Reshape for spatial convolution\n",
    "        x_spatial = x.unsqueeze(1)  # [batch_size, 1, time_points, channels]\n",
    "        x_spatial = self.spatial_filter(x_spatial)  # [batch_size, filter_size, time_points, 1]\n",
    "        x_spatial = x_spatial.squeeze(-1)  # [batch_size, filter_size, time_points]\n",
    "        \n",
    "        # Temporal convolution\n",
    "        # Reshape for temporal convolution\n",
    "        x_temporal = x.permute(0, 2, 1).unsqueeze(1)  # [batch_size, 1, channels, time_points]\n",
    "        x_temporal = self.temporal_conv(x_temporal)  # [batch_size, filter_size, channels, time_points-filter+1]\n",
    "        x_temporal = x_temporal.squeeze(2)  # [batch_size, filter_size, time_points-filter+1]\n",
    "        \n",
    "        # Combine features\n",
    "        x_spatial_flat = x_spatial.reshape(batch_size, -1)\n",
    "        x_temporal_flat = x_temporal.reshape(batch_size, -1)\n",
    "        \n",
    "        # Use spatial features for now (can be extended to combine both)\n",
    "        x = x_spatial_flat\n",
    "        \n",
    "        # Apply fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "        \n",
    "        return x.squeeze(1)\n",
    "\n",
    "\n",
    "def visualize_sentence_structure(sentence_data):\n",
    "    \"\"\"\n",
    "    Visualize the structure of sentence data\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(15, 8))\n",
    "    \n",
    "    y_positions = []\n",
    "    x_positions = []\n",
    "    labels = []\n",
    "    \n",
    "    for sent_idx, sent_data in sentence_data.items():\n",
    "        for pos, char_data in sent_data.items():\n",
    "            y_positions.append(sent_idx)\n",
    "            x_positions.append(pos)\n",
    "            labels.append(char_data['character'])\n",
    "    \n",
    "    ax.scatter(x_positions, y_positions, c='blue', s=100)\n",
    "    \n",
    "    # Add character labels\n",
    "    for i, txt in enumerate(labels):\n",
    "        ax.annotate(txt, (x_positions[i], y_positions[i]), fontsize=12, \n",
    "                   ha='center', va='center', color='white')\n",
    "    \n",
    "    ax.set_xlabel('Position in Sentence')\n",
    "    ax.set_ylabel('Sentence Index')\n",
    "    ax.set_title('Character Distribution in Sentences')\n",
    "    ax.grid(True)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def visualize_p300_responses(features, labels, char_to_sentence, trial_idx, n_channels=4):\n",
    "    \"\"\"\n",
    "    Visualize P300 responses for a specific trial\n",
    "    \"\"\"\n",
    "    # Get a subset of channels to visualize\n",
    "    channels_to_plot = [0, 10, 20, 30]  # Example channels\n",
    "    \n",
    "    if trial_idx in char_to_sentence:\n",
    "        sent_info = char_to_sentence[trial_idx]\n",
    "        title = f\"Trial {trial_idx}: '{sent_info['character']}' (Position {sent_info['position_in_sentence']} in Sentence {sent_info['sentence_idx']})\"\n",
    "    else:\n",
    "        title = f\"Trial {trial_idx}\"\n",
    "    \n",
    "    fig, axes = plt.subplots(n_channels, 1, figsize=(15, 10), sharex=True)\n",
    "    \n",
    "    # Split by P300 presence\n",
    "    p300_features = features[labels == 1]\n",
    "    non_p300_features = features[labels == 0]\n",
    "    \n",
    "    for i, channel in enumerate(channels_to_plot):\n",
    "        # Plot P300 responses\n",
    "        if len(p300_features) > 0:\n",
    "            p300_mean = np.mean(p300_features[:, :, channel], axis=0)\n",
    "            axes[i].plot(p300_mean, 'r-', linewidth=2, label='P300')\n",
    "        \n",
    "        # Plot non-P300 responses\n",
    "        if len(non_p300_features) > 0:\n",
    "            non_p300_mean = np.mean(non_p300_features[:, :, channel], axis=0)\n",
    "            axes[i].plot(non_p300_mean, 'b-', linewidth=2, label='Non-P300')\n",
    "        \n",
    "        axes[i].set_ylabel(f'Channel {channel}')\n",
    "        axes[i].legend()\n",
    "        axes[i].grid(True)\n",
    "    \n",
    "    axes[-1].set_xlabel('Time (samples)')\n",
    "    fig.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return fig\n",
    "\n",
    "def integrate_sentence_context(contributor_selected=\"I\"):\n",
    "    \"\"\"\n",
    "    Main function to integrate sentence context into EEG data\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    contributor_train_file_path = f'../data/Contributor_{contributor_selected}_Train.mat'\n",
    "    \n",
    "    print(f\"Processing data for Contributor {contributor_selected}...\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    signals, flashing, stimulus_type, target_chars, trials, sampling_frequency = load_and_preprocess_data(\n",
    "        contributor_train_file_path)\n",
    "    \n",
    "    print(f\"Data loaded. Trials: {trials}, Sampling rate: {sampling_frequency}Hz\")\n",
    "    \n",
    "    # Extract sentences from character sequence\n",
    "    chars_sequence, sentences = extract_sentences_from_char_sequence(target_chars)\n",
    "    print(f\"Extracted {len(sentences)} sentences from {len(chars_sequence)} characters\")\n",
    "    \n",
    "    # Create character to sentence mapping\n",
    "    char_to_sentence = create_char_to_sentence_mapping(chars_sequence, sentences)\n",
    "    print(f\"Created mapping for {len(char_to_sentence)} characters\")\n",
    "    \n",
    "    # Print a few sentences\n",
    "    print(\"\\nSample sentences:\")\n",
    "    for i, sentence in enumerate(sentences[:3]):\n",
    "        print(f\"Sentence {i}: {sentence}\")\n",
    "    \n",
    "    # Create sentence-based dataset\n",
    "    sentence_data = create_sentence_based_dataset(\n",
    "        signals, flashing, stimulus_type, target_chars, trials, char_to_sentence)\n",
    "    print(f\"\\nCreated sentence-based dataset with {len(sentence_data)} sentences\")\n",
    "    \n",
    "    # Visualize sentence structure\n",
    "    fig = visualize_sentence_structure(sentence_data)\n",
    "    plt.savefig(f\"../output/sentence_structure_{contributor_selected}.png\")\n",
    "    plt.close(fig)\n",
    "    \n",
    "    # Create contextual sequences\n",
    "    X_sequences, y_sequences, contexts = create_contextual_sequences(sentence_data, context_size=2)\n",
    "    print(f\"\\nCreated {len(X_sequences)} contextual sequences with context size 2\")\n",
    "    \n",
    "    # Print some context examples\n",
    "    print(\"\\nSample contexts:\")\n",
    "    for i in range(3):\n",
    "        ctx = contexts[i]\n",
    "        print(f\"Target: '{ctx['target_char']}', Context: '{ctx['context']}', \" +\n",
    "              f\"Sentence: {ctx['sentence_idx']}, Position: {ctx['position']}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Visualize P300 responses for a few trials\n",
    "    for trial_idx in range(3):\n",
    "        # Extract features for this trial\n",
    "        trial_features, trial_labels, _ = extract_trial_features(\n",
    "            signals, flashing, stimulus_type, trial_idx)\n",
    "        \n",
    "        fig = visualize_p300_responses(trial_features, trial_labels, char_to_sentence, trial_idx)\n",
    "        plt.savefig(f\"../output/p300_trial_{trial_idx}_{contributor_selected}.png\")\n",
    "        plt.close(fig)\n",
    "    \n",
    "    print(\"\\nSentence context integration complete.\")\n",
    "    return {\n",
    "        'sentences': sentences,\n",
    "        'char_to_sentence': char_to_sentence,\n",
    "        'sentence_data': sentence_data,\n",
    "        'X_sequences': X_sequences,\n",
    "        'y_sequences': y_sequences,\n",
    "        'contexts': contexts\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the integration\n",
    "    results = integrate_sentence_context(\"I\")\n",
    "    \n",
    "    # Access components\n",
    "    sentence_data = results['sentence_data']\n",
    "    \n",
    "    print(f\"\\nSuccess! Model and sentence data ready for use.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
