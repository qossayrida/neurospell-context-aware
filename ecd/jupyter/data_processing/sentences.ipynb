{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create EEG Sentences\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e74093c265c6a51"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load sample groups from: ../data/sample_groups.pkl\n",
      "Successfully loaded sample groups dictionary.\n",
      "\n",
      "--- Example Access --- \n",
      "Character A, Set set1 has 4 chunks.\n",
      "  First chunk contains 30 samples.\n",
      "    Shape of the first sample in the first chunk: (78, 64)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_sample_groups(filepath=\"../data/sample_groups.pkl\"):\n",
    "    \"\"\"Loads the grouped samples dictionary from a pickle file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str): The path to the pickle file containing the sample groups dictionary.\n",
    "                        Defaults to \t\"sample_groups.pkl\t\" in the current directory.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded sample_groups dictionary, or None if loading fails.\n",
    "              Structure: {char: {set_name: [chunk1, chunk2, ...], ...}, ...}\n",
    "              where each chunk is a list of numpy arrays (samples).\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load sample groups from: {filepath}\")\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: File not found at {filepath}. Please run the grouping script first.\")\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            loaded_groups = pickle.load(f)\n",
    "        print(\"Successfully loaded sample groups dictionary.\")\n",
    "        # Optional: Add a check to ensure it\t\"s a dictionary\n",
    "        if isinstance(loaded_groups, dict):\n",
    "            return loaded_groups\n",
    "        else:\n",
    "            print(f\"Error: Loaded object is not a dictionary (type: {type(loaded_groups)}). Returning None.\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except pickle.UnpicklingError:\n",
    "        print(f\"Error: Could not unpickle data from {filepath}. File might be corrupted.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during loading: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "sample_groups_data = load_sample_groups()\n",
    "\n",
    "if sample_groups_data:\n",
    "    print(\"\\n--- Example Access --- \")\n",
    "    target_char = \t\"A\" # Example character\n",
    "    target_set = \t\"set1\" # Example set\n",
    "\n",
    "    if target_char in sample_groups_data and target_set in sample_groups_data[target_char]:\n",
    "        groups_for_set = sample_groups_data[target_char][target_set]\n",
    "        num_chunks = len(groups_for_set)\n",
    "        print(f\"Character {target_char}, Set {target_set} has {num_chunks} chunks.\")\n",
    "        \n",
    "        if num_chunks > 0:\n",
    "            first_chunk = groups_for_set[0] # Get the first chunk (which is a list of samples)\n",
    "            num_samples_in_chunk = len(first_chunk)\n",
    "            print(f\"  First chunk contains {num_samples_in_chunk} samples.\")\n",
    "            \n",
    "            if num_samples_in_chunk > 0:\n",
    "                first_sample_in_chunk = first_chunk[0] # Get the first sample (numpy array)\n",
    "                print(f\"    Shape of the first sample in the first chunk: {first_sample_in_chunk.shape}\")\n",
    "                # You can now work with \t\"first_sample_in_chunk\t\" or the entire \t\"first_chunk\t\" list\n",
    "    else:\n",
    "        print(f\"Could not find data for Character {target_char} and Set {target_set}.\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:46:00.212558200Z",
     "start_time": "2025-05-31T15:45:55.787624200Z"
    }
   },
   "id": "189d752f13e679c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Add next-char prediction probabilities"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dbead4f00a928323"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Add next-char prediction probabilities to each DFM sample as additional feature\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import string\n",
    "\n",
    "\n",
    "# Define character set (a-z, A-Z, 0-9)\n",
    "all_chars = list(string.ascii_lowercase + string.ascii_uppercase + string.digits)\n",
    "char2idx = {ch: idx for idx, ch in enumerate(all_chars)}\n",
    "idx2char = {idx: ch for ch, idx in char2idx.items()}\n",
    "vocab_size = len(all_chars)\n",
    "\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "class CharPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):\n",
    "        super(CharPredictor, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embed = self.embedding(x)\n",
    "        _, (hidden, _) = self.lstm(embed)\n",
    "        hidden = hidden.squeeze(0)\n",
    "        out = self.fc(hidden)\n",
    "        return out\n",
    "    \n",
    "def load_model(path=\"../model/api/char_predictor.pth\"):\n",
    "    model = CharPredictor(vocab_size)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_next_chars(model, sentence, top_k=5):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input_seq = [char2idx[ch] for ch in sentence if ch in char2idx]\n",
    "        if not input_seq:\n",
    "            raise ValueError(\"Input sentence must contain at least one known character.\")\n",
    "\n",
    "        input_seq = torch.tensor(input_seq).unsqueeze(0)\n",
    "        output = model(input_seq)\n",
    "        probs = F.softmax(output, dim=-1).squeeze(0)\n",
    "\n",
    "        top_probs, top_indices = torch.topk(probs, top_k)\n",
    "\n",
    "        result = {}\n",
    "        for prob, idx in zip(top_probs, top_indices):\n",
    "            result[idx2char[idx.item()]] = round(prob.item(), 4)\n",
    "\n",
    "        return result\n",
    "\n",
    "# Load pretrained NLP model\n",
    "nlp_model = load_model()\n",
    "\n",
    "\n",
    "# prefix = context before the target char\n",
    "\n",
    "# nlp_prob_dict = predict_next_chars(nlp_model, prefix, top_k=6)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fee7df9628f0876a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
