{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-31T13:31:05.048569300Z",
     "start_time": "2025-05-31T13:30:42.311006200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded characters from ../data/characters.txt: ABCDEFGHIJKLMNOPQRSTUVWXYZ123456789_\n",
      "\n",
      "Initialized combined samples dictionary for characters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, 1, 2, 3, 4, 5, 6, 7, 8, 9, _\n",
      "\n",
      "==================== Processing Contributor: I ====================\n",
      "Loading training data from: ../data/Contributor_I_Train.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): EAEVQTDOJG8RBRGONCEDHCTUIDBPUHMEM6OUXOCFOUKWA4VJEF...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      "Train Data Info (Initial):\n",
      "  Contributor: I\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor I. Added 15300 samples.\n",
      "\n",
      "==================== Processing Contributor: II ====================\n",
      "Loading training data from: ../data/Contributor_II_Train.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): VGREAAH8TVRHBYN_UGCOLO4EUERDOOHCIFOMDNU6LQCPKEIREK...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      "Train Data Info (Initial):\n",
      "  Contributor: II\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor II. Added 15300 samples.\n",
      "\n",
      "==================== Finished Processing All Contributors ====================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "# Define a dummy print_data if DataCraft is not available/needed\n",
    "def print_data(signals, words_string, contributor, freq):\n",
    "    print(f\"  Contributor: {contributor}\")\n",
    "    print(f\"  Signals shape: {signals.shape}\")\n",
    "    if words_string is not None:\n",
    "        print(f\"  Target Characters String Length: {len(words_string) if isinstance(words_string, str) else 'N/A'}\")\n",
    "    print(f\"  Sampling Frequency (initial): {freq}\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "contributors_to_process = [\"I\", \"II\"] # List of contributors to process\n",
    "characters_file_path = \"../data/characters.txt\" # Updated path\n",
    "channels = list(range(64))\n",
    "initial_sampling_frequency = 240\n",
    "down_sampling_frequency = 120\n",
    "WINDOW_DURATION = 650 # ms\n",
    "\n",
    "# --- File Checks & Character Loading ---\n",
    "try:\n",
    "    with open(characters_file_path, \"r\") as f:\n",
    "        characters = f.read().strip()\n",
    "    print(f\"Loaded characters from {characters_file_path}: {characters}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Characters file not found at {characters_file_path}\")\n",
    "    exit()\n",
    "\n",
    "# --- Initialize Combined Samples Dictionary --- \n",
    "# Initialize dictionary before the loop\n",
    "samples = {char: [] for char in characters}\n",
    "print(f\"\\nInitialized combined samples dictionary for characters: {', '.join(samples.keys())}\")\n",
    "\n",
    "# --- Loop Through Contributors --- \n",
    "for contributor_selected in contributors_to_process:\n",
    "    print(f\"\\n{'='*20} Processing Contributor: {contributor_selected} {'='*20}\")\n",
    "    contributor_train_file_path = f\"../data/Contributor_{contributor_selected}_Train.mat\"\n",
    "\n",
    "    # Check if data file exists for the current contributor\n",
    "    if not os.path.exists(contributor_train_file_path):\n",
    "        print(f\"Warning: Train data file not found for contributor {contributor_selected} at {contributor_train_file_path}. Skipping this contributor.\")\n",
    "        continue # Skip to the next contributor\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(f\"Loading training data from: {contributor_train_file_path}\")\n",
    "    try:\n",
    "        data_train = loadmat(contributor_train_file_path)\n",
    "        signals_train = data_train[\"Signal\"] # Shape: (Trials, Samples, Channels)\n",
    "        flashing_train = data_train[\"Flashing\"]\n",
    "        stimulus_train = data_train[\"StimulusType\"]\n",
    "        word_train_raw = data_train[\"TargetChar\"] # Expected: String of target chars\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for contributor {contributor_selected}: {e}. Skipping this contributor.\")\n",
    "        continue\n",
    "\n",
    "    # --- Data Consistency Checks & Target Character Extraction ---\n",
    "    num_signal_trials = signals_train.shape[0]\n",
    "    print(f\"Number of trials based on Signal array: {num_signal_trials}\")\n",
    "\n",
    "    target_char_string = None\n",
    "    if isinstance(word_train_raw, np.ndarray):\n",
    "        if word_train_raw.size == 1:\n",
    "            target_char_string = str(word_train_raw.item()).strip()\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected shape for TargetChar: {word_train_raw.shape}. Trying to flatten.\")\n",
    "            try:\n",
    "                target_char_string = \"\".join(map(str, word_train_raw.flatten()))\n",
    "            except Exception as e:\n",
    "                 print(f\"Error processing TargetChar array: {e}\")\n",
    "    elif isinstance(word_train_raw, str):\n",
    "        target_char_string = word_train_raw.strip()\n",
    "    else:\n",
    "        print(f\"Error: Unexpected type for TargetChar: {type(word_train_raw)}\")\n",
    "\n",
    "    if target_char_string is None:\n",
    "        print(\"Error: Could not extract target character string. Skipping this contributor.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Extracted Target Character String (length {len(target_char_string)}): {target_char_string[:50]}...\")\n",
    "\n",
    "    if len(target_char_string) != num_signal_trials:\n",
    "        print(f\"Error: Length of TargetChar string ({len(target_char_string)}) does not match number of trials in Signal ({num_signal_trials}). Please check data integrity.\")\n",
    "        print(\"Warning: Proceeding with minimum of the two lengths for processing this contributor.\")\n",
    "        trials_to_process = min(len(target_char_string), num_signal_trials)\n",
    "    else:\n",
    "        trials_to_process = num_signal_trials\n",
    "        print(\"TargetChar string length matches number of signal trials.\")\n",
    "\n",
    "    print(\"\\nTrain Data Info (Initial):\")\n",
    "    print_data(signals_train, target_char_string, contributor_selected, initial_sampling_frequency)\n",
    "\n",
    "    # --- Butterworth Filter ---\n",
    "    print(\"Applying Butterworth filter...\")\n",
    "    sampling_frequency = initial_sampling_frequency\n",
    "    b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], \"bandpass\")\n",
    "    for trial in range(num_signal_trials):\n",
    "        try:\n",
    "            signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "        except IndexError:\n",
    "            print(f\"Error: IndexError during filtering trial {trial}. Check signal dimensions.\")\n",
    "            continue\n",
    "    print(\"Filtering complete.\")\n",
    "\n",
    "    # --- Down-sampling ---\n",
    "    print(f\"Downsampling signals from {initial_sampling_frequency}Hz to {down_sampling_frequency}Hz...\")\n",
    "    SCALE_FACTOR = round(initial_sampling_frequency / down_sampling_frequency)\n",
    "    sampling_frequency = down_sampling_frequency\n",
    "\n",
    "    print(f\"# Samples of EEG signals before downsampling: {signals_train.shape[1]}\")\n",
    "    try:\n",
    "        signals_train = signals_train[:, ::SCALE_FACTOR, :]\n",
    "        flashing_train = flashing_train[:, ::SCALE_FACTOR]\n",
    "        stimulus_train = stimulus_train[:, ::SCALE_FACTOR]\n",
    "    except IndexError as e:\n",
    "        print(f\"Error during downsampling: {e}. Check array dimensions after filtering.\")\n",
    "        continue # Skip feature extraction for this contributor if downsampling fails\n",
    "    print(f\"# Samples of EEG signals after downsampling: {signals_train.shape[1]}\")\n",
    "    print(\"Downsampling complete.\")\n",
    "\n",
    "    # --- Feature Extraction & Grouping ---\n",
    "    print(\"Extracting features and grouping by character...\")\n",
    "    N_CHANNELS = signals_train.shape[2]\n",
    "    WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "    SAMPLES_PER_TRIAL = signals_train.shape[1]\n",
    "\n",
    "    contributor_samples_collected = 0\n",
    "    for trial in range(trials_to_process):\n",
    "        target_char = target_char_string[trial]\n",
    "\n",
    "        if target_char not in samples:\n",
    "            print(f\"Warning: Target character '{target_char}' from trial {trial} not found in characters.txt. Skipping this trial.\")\n",
    "            continue\n",
    "\n",
    "        if trial >= flashing_train.shape[0] or trial >= stimulus_train.shape[0]:\n",
    "            print(f\"Warning: Trial index {trial} out of bounds for flashing/stimulus arrays. Stopping processing for this contributor.\")\n",
    "            break\n",
    "\n",
    "        for sample_idx in range(SAMPLES_PER_TRIAL):\n",
    "            is_flash_start = False\n",
    "            try:\n",
    "                if sample_idx >= flashing_train.shape[1]: break\n",
    "                if sample_idx == 0 and flashing_train[trial, sample_idx] == 1:\n",
    "                    is_flash_start = True\n",
    "                elif sample_idx > 0:\n",
    "                     if sample_idx - 1 >= flashing_train.shape[1]: break\n",
    "                     if flashing_train[trial, sample_idx - 1] == 0 and flashing_train[trial, sample_idx] == 1:\n",
    "                         is_flash_start = True\n",
    "            except IndexError:\n",
    "                print(f\"Warning: IndexError accessing flashing_train at trial {trial}, sample {sample_idx}. Skipping sample.\")\n",
    "                continue\n",
    "\n",
    "            if is_flash_start:\n",
    "                lower_sample = sample_idx\n",
    "                upper_sample = sample_idx + WINDOW_SAMPLES\n",
    "                if upper_sample > SAMPLES_PER_TRIAL: continue\n",
    "\n",
    "                try:\n",
    "                    if upper_sample > signals_train.shape[1]: continue\n",
    "                    window = signals_train[trial, lower_sample:upper_sample, :]\n",
    "                except IndexError:\n",
    "                     print(f\"Warning: IndexError extracting window at trial {trial}, samples {lower_sample}:{upper_sample}. Skipping window.\")\n",
    "                     continue\n",
    "\n",
    "                if window.shape[0] != WINDOW_SAMPLES: continue\n",
    "\n",
    "                try:\n",
    "                    if window.size == 0 or np.all(np.std(window, axis=0) == 0):\n",
    "                        normalized_window = window\n",
    "                    else:\n",
    "                        normalized_window = scale(window, axis=0)\n",
    "                except ValueError as e:\n",
    "                     print(f\"Warning: ValueError during scaling window at trial {trial}, sample {sample_idx}: {e}. Skipping window.\")\n",
    "                     continue\n",
    "\n",
    "                # Append to the main combined dictionary\n",
    "                samples[target_char].append(normalized_window)\n",
    "                contributor_samples_collected += 1\n",
    "\n",
    "    print(f\"Feature extraction and grouping complete for contributor {contributor_selected}. Added {contributor_samples_collected} samples.\")\n",
    "\n",
    "# --- End of Contributor Loop ---\n",
    "print(f\"\\n{'='*20} Finished Processing All Contributors {'='*20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57bc85c26e0286fa"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification (Combined Data) --- \n",
      "Total number of samples per character across all contributors:\n",
      "Character 'A': 720 samples\n",
      "Character 'B': 1080 samples\n",
      "Character 'C': 1080 samples\n",
      "Character 'D': 1440 samples\n",
      "Character 'E': 2880 samples\n",
      "Character 'F': 720 samples\n",
      "Character 'G': 720 samples\n",
      "Character 'H': 1440 samples\n",
      "Character 'I': 1080 samples\n",
      "Character 'J': 720 samples\n",
      "Character 'K': 1080 samples\n",
      "Character 'L': 720 samples\n",
      "Character 'M': 720 samples\n",
      "Character 'N': 720 samples\n",
      "Character 'O': 3240 samples\n",
      "Character 'P': 720 samples\n",
      "Character 'Q': 1080 samples\n",
      "Character 'R': 1800 samples\n",
      "Character 'S': 0 samples\n",
      "Character 'T': 1440 samples\n",
      "Character 'U': 1800 samples\n",
      "Character 'V': 720 samples\n",
      "Character 'W': 1080 samples\n",
      "Character 'X': 720 samples\n",
      "Character 'Y': 720 samples\n",
      "Character 'Z': 720 samples\n",
      "Character '1': 0 samples\n",
      "Character '2': 0 samples\n",
      "Character '3': 0 samples\n",
      "Character '4': 360 samples\n",
      "Character '5': 0 samples\n",
      "Character '6': 360 samples\n",
      "Character '7': 0 samples\n",
      "Character '8': 360 samples\n",
      "Character '9': 0 samples\n",
      "Character '_': 360 samples\n",
      "\n",
      "Total samples collected across all characters and contributors: 30600\n",
      "\n",
      "--- Samples for Character 'A' (First 5 from Combined Data) ---\n",
      "Printing the first 5 samples for character 'A':\n",
      "\n",
      "--- Sample 1 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 1.5269679   2.4468462   2.3820097  ... -0.41612127  0.74596584\n",
      "  -0.85115325]\n",
      " [ 1.1498553   2.036791    2.020591   ... -0.0151797   0.99634045\n",
      "  -0.54752934]\n",
      " [ 0.83037424  1.6841043   1.7092923  ...  0.3432492   1.2098042\n",
      "  -0.28544807]\n",
      " ...\n",
      " [-1.4881009   0.23393261  0.03583406 ... -0.49983054 -1.1561428\n",
      "  -1.725664  ]\n",
      " [-1.2441964   0.4488636   0.17224243 ... -0.45400757 -1.0678238\n",
      "  -1.7058161 ]\n",
      " [-0.88442945  0.6771115   0.36175698 ... -0.3258581  -0.955218\n",
      "  -1.6446475 ]]\n",
      "\n",
      "--- Sample 2 for 'A' (Shape: (78, 64)) ---\n",
      "[[-0.03063992  0.2040172   0.19283181 ... -2.1880515   0.6813935\n",
      "  -0.78624475]\n",
      " [-0.1025832  -0.10047792 -0.02324355 ... -2.278403    0.44569674\n",
      "  -0.9310363 ]\n",
      " [-0.13414295 -0.30414206 -0.12324184 ... -2.1832821   0.3652425\n",
      "  -0.92979723]\n",
      " ...\n",
      " [-1.189907   -1.2798034  -1.5616812  ...  0.5695779   0.38371864\n",
      "  -1.1463373 ]\n",
      " [-1.2787447  -1.4600979  -1.6258503  ...  0.47741395  0.29938695\n",
      "  -1.1447345 ]\n",
      " [-1.3986615  -1.6861359  -1.7340046  ...  0.42391255  0.25460255\n",
      "  -1.1190511 ]]\n",
      "\n",
      "--- Sample 3 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 0.13859431  0.05241255  0.8652654  ... -1.4675186  -0.9315167\n",
      "   0.257902  ]\n",
      " [ 0.28765824  0.09510172  0.8036224  ... -1.3630553  -1.1135559\n",
      "   0.30780232]\n",
      " [ 0.38454396  0.02131936  0.6002111  ... -1.3128492  -1.3038813\n",
      "   0.3512704 ]\n",
      " ...\n",
      " [-1.0349932  -0.9172451  -0.86691123 ...  1.1220946   1.4228058\n",
      "  -0.40579128]\n",
      " [-0.9455943  -0.88881797 -0.8296465  ...  1.1585256   1.3169163\n",
      "  -0.3774847 ]\n",
      " [-0.82498336 -0.78416765 -0.69584185 ...  1.1530987   1.1998022\n",
      "  -0.33178157]]\n",
      "\n",
      "--- Sample 4 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 2.214405    0.6064221   0.01331997 ...  0.5486795   0.6457513\n",
      "   3.3742282 ]\n",
      " [ 2.3737302   0.73148936  0.1607854  ...  0.64488405  0.68530154\n",
      "   3.1818576 ]\n",
      " [ 2.5207098   0.94624865  0.45206037 ...  0.68123066  0.64031607\n",
      "   2.8813903 ]\n",
      " ...\n",
      " [-0.3724019   0.91215     0.34721586 ...  2.346756    0.49281257\n",
      "   1.2366227 ]\n",
      " [-0.02189189  1.1651404   0.5980035  ...  2.632333    0.92121047\n",
      "   1.7060342 ]\n",
      " [ 0.22769828  1.2532082   0.69456947 ...  2.7608378   1.1177416\n",
      "   1.8545113 ]]\n",
      "\n",
      "--- Sample 5 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 0.4864067   0.4514187   0.82185227 ... -1.4679005  -1.0954056\n",
      "  -0.09513493]\n",
      " [ 0.07037575 -0.04995427  0.2727303  ... -1.5070484  -1.1786046\n",
      "  -0.19518209]\n",
      " [-0.28611878 -0.48634702 -0.2466987  ... -1.5195847  -1.2128444\n",
      "  -0.287148  ]\n",
      " ...\n",
      " [ 1.6783068   1.729134    1.3418999  ...  1.9038633   2.0230122\n",
      "   1.186196  ]\n",
      " [ 1.7235836   1.7502573   1.2436447  ...  1.8149129   1.9813803\n",
      "   1.2579296 ]\n",
      " [ 1.820859    1.7835704   1.1584477  ...  1.679367    1.9383786\n",
      "   1.3387889 ]]\n"
     ]
    }
   ],
   "source": [
    "# --- Verification (Combined Data) ---\n",
    "print(\"\\n--- Verification (Combined Data) --- \")\n",
    "print(\"Total number of samples per character across all contributors:\")\n",
    "total_samples_collected = 0\n",
    "for char, char_samples in samples.items():\n",
    "    count = len(char_samples)\n",
    "    print(f\"Character '{char}': {count} samples\")\n",
    "    total_samples_collected += count\n",
    "\n",
    "print(f\"\\nTotal samples collected across all characters and contributors: {total_samples_collected}\")\n",
    "\n",
    "# --- Print First 5 Samples for Character 'A' (Combined Data) --- \n",
    "print(\"\\n--- Samples for Character 'A' (First 5 from Combined Data) ---\")\n",
    "char_to_print = 'A'\n",
    "num_samples_to_print = 5\n",
    "\n",
    "if char_to_print in samples and samples[char_to_print]:\n",
    "    print(f\"Printing the first {min(num_samples_to_print, len(samples[char_to_print]))} samples for character '{char_to_print}':\")\n",
    "    for i, sample_data in enumerate(samples[char_to_print][:num_samples_to_print]):\n",
    "        print(f\"\\n--- Sample {i+1} for '{char_to_print}' (Shape: {sample_data.shape}) ---\")\n",
    "        # np.set_printoptions(threshold=np.inf) # Uncomment to print full array\n",
    "        print(sample_data)\n",
    "        # np.set_printoptions(threshold=1000) # Reset if needed\n",
    "else:\n",
    "    print(f\"No samples found for character '{char_to_print}' in the combined data or the list is empty.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T13:31:05.106266900Z",
     "start_time": "2025-05-31T13:31:05.053659900Z"
    }
   },
   "id": "d2a80453dcce8992"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Group samples according to three strategies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8880bd7955777d34"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting sample grouping process...\n",
      "\n",
      "Processing character: \t\t'A' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'B' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'C' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'D' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'E' (2880 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 16 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 32 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 24 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'F' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'G' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'H' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'I' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'J' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'K' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'L' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'M' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'N' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'O' (3240 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 18 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 36 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 27 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'P' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Q' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'R' (1800 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 10 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 20 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 15 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'S' (0 samples)\n",
      "  Skipping character \t\t'S' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'T' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'U' (1800 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 10 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 20 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 15 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'V' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'W' (1080 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'X' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Y' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Z' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'1' (0 samples)\n",
      "  Skipping character \t\t'1' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'2' (0 samples)\n",
      "  Skipping character \t\t'2' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'3' (0 samples)\n",
      "  Skipping character \t\t'3' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'4' (360 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'5' (0 samples)\n",
      "  Skipping character \t\t'5' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'6' (360 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'7' (0 samples)\n",
      "  Skipping character \t\t'7' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'8' (360 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'9' (0 samples)\n",
      "  Skipping character \t\t'9' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'_' (360 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Finished sample grouping.\n",
      "\n",
      "Saving sample groups dictionary to: ../data/sample_groups.pkl\n",
      "Successfully saved sample groups to ../data/sample_groups.pkl.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# 180 360 540 720 900 1080 1260 1440 1620 1800\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "output_filepath = \"../data/sample_groups.pkl\"    \n",
    "\n",
    "strategies = {\n",
    "    \"set1\": {\"chunk_size\": 180, \"random\": False},\n",
    "    \"set2\": {\"chunk_size\": 90, \"random\": False},\n",
    "    \"set3\": {\"chunk_size\": 120, \"random\": True}\n",
    "}\n",
    "\n",
    "print(\"\\nStarting sample grouping process...\")\n",
    "sample_groups = {} # Initialize the main dictionary to store grouped samples\n",
    "\n",
    "# Process each character\n",
    "for char, char_samples_list in samples.items():\n",
    "    print(f\"\\nProcessing character: \t\t'{char}' ({len(char_samples_list)} samples)\")\n",
    "    if not char_samples_list:\n",
    "        print(f\"  Skipping character \t\t'{char}' as it has no samples.\")\n",
    "        sample_groups[char] = {} # Still add char key, but with empty dict\n",
    "        continue\n",
    "\n",
    "    sample_groups[char] = {} # Initialize dictionary for this character's sets\n",
    "\n",
    "    # Process each strategy for the current character\n",
    "    for set_name, params in strategies.items():\n",
    "        print(f\"  Grouping for {set_name} (chunk size: {params['chunk_size']}, random: {params['random']})...\")\n",
    "        \n",
    "        chunk_size = params['chunk_size']\n",
    "        is_random = params['random']\n",
    "        current_samples_list = list(char_samples_list) # Make a copy for potential shuffling\n",
    "\n",
    "        if is_random:\n",
    "            random.shuffle(current_samples_list)\n",
    "\n",
    "        num_samples = len(current_samples_list)\n",
    "        num_chunks = math.ceil(num_samples / chunk_size)\n",
    "        \n",
    "        set_chunks = [] # List to hold the chunks for this set\n",
    "        \n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size\n",
    "            # Slice the list of samples (which are numpy arrays)\n",
    "            chunk = current_samples_list[start_idx:end_idx]\n",
    "            \n",
    "            if chunk: # Ensure chunk is not empty\n",
    "                set_chunks.append(chunk) # Append the list of numpy arrays\n",
    "        \n",
    "        sample_groups[char][set_name] = set_chunks\n",
    "        print(f\"    Created {len(set_chunks)} chunks for {set_name}.\")\n",
    "\n",
    "print(\"\\nFinished sample grouping.\")\n",
    "\n",
    "# --- Save Sample Groups Dictionary --- \n",
    "print(f\"\\nSaving sample groups dictionary to: {output_filepath}\")\n",
    "try:\n",
    "    with open(output_filepath, \"wb\") as f:\n",
    "        pickle.dump(sample_groups, f)\n",
    "    print(f\"Successfully saved sample groups to {output_filepath}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving sample groups dictionary: {e}\")\n",
    "\n",
    "\n",
    "print(\"\\nScript finished.\") # Final finished print"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T13:31:06.973150300Z",
     "start_time": "2025-05-31T13:31:05.184137300Z"
    }
   },
   "id": "846981811b9ef29f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
