{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Character Identity "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c586d2047dd892ab"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded characters from ../data/characters.txt: ABCDEFGHIJKLMNOPQRSTUVWXYZ123456789_\n",
      "\n",
      "Initialized combined samples dictionary for characters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, 1, 2, 3, 4, 5, 6, 7, 8, 9, _\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "\n",
    "# Define a dummy print_data if DataCraft is not available/needed\n",
    "def print_data(signals, words_string, contributor, freq):\n",
    "    print(f\"  Contributor: {contributor}\")\n",
    "    print(f\"  Signals shape: {signals.shape}\")\n",
    "    if words_string is not None:\n",
    "        print(f\"  Target Characters String Length: {len(words_string) if isinstance(words_string, str) else 'N/A'}\")\n",
    "    print(f\"  Sampling Frequency (initial): {freq}\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "contributors_to_process = [\"I\", \"II\"]  # List of contributors to process\n",
    "characters_file_path = \"../data/characters.txt\"  # Updated path\n",
    "channels = list(range(64))\n",
    "initial_sampling_frequency = 240\n",
    "down_sampling_frequency = 120\n",
    "WINDOW_DURATION = 650  # ms\n",
    "\n",
    "# --- File Checks & Character Loading ---\n",
    "try:\n",
    "    with open(characters_file_path, \"r\") as f:\n",
    "        characters = f.read().strip()\n",
    "    print(f\"Loaded characters from {characters_file_path}: {characters}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Characters file not found at {characters_file_path}\")\n",
    "    exit()\n",
    "\n",
    "# --- Initialize Combined Samples Dictionary --- \n",
    "# Initialize dictionary before the loop\n",
    "samples = {char: [] for char in characters}\n",
    "print(f\"\\nInitialized combined samples dictionary for characters: {', '.join(samples.keys())}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:36:56.928569900Z",
     "start_time": "2025-05-31T15:36:56.860054600Z"
    }
   },
   "id": "fc9a70b2966eb0ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loop Through Contributors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84049b6958260a5"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Processing Contributor: I ====================\n",
      "Loading data from: ../data/Contributor_I.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): EAEVQTDOJG8RBRGONCEDHCTUIDBPUHMEM6OUXOCFOUKWA4VJEF...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      " Data Info (Initial):\n",
      "  Contributor: I\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor I. Added 30600 samples.\n",
      "\n",
      "==================== Processing Contributor: II ====================\n",
      "Loading data from: ../data/Contributor_II.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): VGREAAH8TVRHBYN_UGCOLO4EUERDOOHCIFOMDNU6LQCPKEIREK...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      " Data Info (Initial):\n",
      "  Contributor: II\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor II. Added 30600 samples.\n",
      "\n",
      "==================== Finished Processing All Contributors ====================\n"
     ]
    }
   ],
   "source": [
    "for contributor_selected in contributors_to_process:\n",
    "    print(f\"\\n{'=' * 20} Processing Contributor: {contributor_selected} {'=' * 20}\")\n",
    "    contributor_data_file_path = f\"../data/Contributor_{contributor_selected}.mat\"\n",
    "\n",
    "    # Check if data file exists for the current contributor\n",
    "    if not os.path.exists(contributor_data_file_path):\n",
    "        print(\n",
    "            f\"Warning: data file not found for contributor {contributor_selected} at {contributor_data_file_path}. Skipping this contributor.\")\n",
    "        continue  # Skip to the next contributor\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(f\"Loading data from: {contributor_data_file_path}\")\n",
    "    try:\n",
    "        data = loadmat(contributor_data_file_path)\n",
    "        signals = data[\"Signal\"]  # Shape: (Trials, Samples, Channels)\n",
    "        flashing = data[\"Flashing\"]\n",
    "        stimulus = data[\"StimulusType\"]\n",
    "        word_raw = data[\"TargetChar\"]  # Expected: String of target chars\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for contributor {contributor_selected}: {e}. Skipping this contributor.\")\n",
    "        continue\n",
    "        \n",
    "    # --- Data Consistency Checks & Target Character Extraction ---\n",
    "    num_signal_trials = signals.shape[0]\n",
    "    print(f\"Number of trials based on Signal array: {num_signal_trials}\")\n",
    "\n",
    "    target_char_string = None\n",
    "    if isinstance(word_raw, np.ndarray):\n",
    "        if word_raw.size == 1:\n",
    "            target_char_string = str(word_raw.item()).strip()\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected shape for TargetChar: {word_raw.shape}. Trying to flatten.\")\n",
    "            try:\n",
    "                target_char_string = \"\".join(map(str, word_raw.flatten()))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing TargetChar array: {e}\")\n",
    "    elif isinstance(word_raw, str):\n",
    "        target_char_string = word_raw.strip()\n",
    "    else:\n",
    "        print(f\"Error: Unexpected type for TargetChar: {type(word_raw)}\")\n",
    "\n",
    "    if target_char_string is None:\n",
    "        print(\"Error: Could not extract target character string. Skipping this contributor.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Extracted Target Character String (length {len(target_char_string)}): {target_char_string[:50]}...\")\n",
    "\n",
    "##    print(f\" target_char_string ====> {target_char_string}\")\n",
    "##    print(f\" num_signal_trials ====> {num_signal_trials}\")\n",
    "    if len(target_char_string) != num_signal_trials:\n",
    "        print(\n",
    "            f\"Error: Length of TargetChar string ({len(target_char_string)}) does not match number of trials in Signal ({num_signal_trials}). Please check data integrity.\")\n",
    "        print(\"Warning: Proceeding with minimum of the two lengths for processing this contributor.\")\n",
    "        trials_to_process = min(len(target_char_string), num_signal_trials)\n",
    "    else:\n",
    "        trials_to_process = num_signal_trials\n",
    "        print(\"TargetChar string length matches number of signal trials.\")\n",
    "\n",
    "    print(f\"\\n Data Info (Initial):\")\n",
    "    print_data(signals, target_char_string, contributor_selected, initial_sampling_frequency)\n",
    "\n",
    "    # --- Butterworth Filter ---\n",
    "    print(\"Applying Butterworth filter...\")\n",
    "    sampling_frequency = initial_sampling_frequency\n",
    "    b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], \"bandpass\")\n",
    "    for trial in range(num_signal_trials):\n",
    "        try:\n",
    "            signals[trial, :, :] = signal.filtfilt(b, a, signals[trial, :, :], axis=0)\n",
    "        except IndexError:\n",
    "            print(f\"Error: IndexError during filtering trial {trial}. Check signal dimensions.\")\n",
    "            continue\n",
    "    print(\"Filtering complete.\")\n",
    "\n",
    "    # --- Down-sampling ---\n",
    "    print(f\"Downsampling signals from {initial_sampling_frequency}Hz to {down_sampling_frequency}Hz...\")\n",
    "    SCALE_FACTOR = round(initial_sampling_frequency / down_sampling_frequency)\n",
    "    sampling_frequency = down_sampling_frequency\n",
    "\n",
    "    print(f\"# Samples of EEG signals before downsampling: {signals.shape[1]}\")\n",
    "    try:\n",
    "        signals = signals[:, ::SCALE_FACTOR, :]\n",
    "        flashing = flashing[:, ::SCALE_FACTOR]\n",
    "        stimulus = stimulus[:, ::SCALE_FACTOR]\n",
    "    except IndexError as e:\n",
    "        print(f\"Error during downsampling: {e}. Check array dimensions after filtering.\")\n",
    "        continue  # Skip feature extraction for this contributor if downsampling fails\n",
    "    print(f\"# Samples of EEG signals after downsampling: {signals.shape[1]}\")\n",
    "    print(\"Downsampling complete.\")\n",
    "\n",
    "    # --- Feature Extraction & Grouping ---\n",
    "    print(\"Extracting features and grouping by character...\")\n",
    "    N_CHANNELS = signals.shape[2]\n",
    "    WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "    SAMPLES_PER_TRIAL = signals.shape[1]\n",
    "##    print(f\"Samples per trials = {SAMPLES_PER_TRIAL}\")\n",
    "\n",
    "    contributor_samples_collected = 0\n",
    "    for trial in range(trials_to_process):  # 1 - 85 \n",
    "        target_char = target_char_string[trial]\n",
    "\n",
    "        if target_char not in samples:\n",
    "            print(\n",
    "                f\"Warning: Target character '{target_char}' from trial {trial} not found in characters.txt. Skipping this trial.\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if trial >= flashing.shape[0] or trial >= stimulus.shape[0]:\n",
    "            print(\n",
    "                f\"Warning: Trial index {trial} out of bounds for flashing/stimulus arrays. Stopping processing for this contributor.\")\n",
    "            break\n",
    "\n",
    "        for sample_idx in range(SAMPLES_PER_TRIAL):  # 0 - 3394\n",
    "            is_flash_start = False\n",
    "            \n",
    "            if stimulus[trial,sample_idx]==0:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                if sample_idx >= flashing.shape[1]: break\n",
    "                if sample_idx == 0 and flashing[trial, sample_idx] == 1:\n",
    "                    is_flash_start = True\n",
    "                elif sample_idx > 0:\n",
    "                    if sample_idx - 1 >= flashing.shape[1]: break\n",
    "                    if flashing[trial, sample_idx - 1] == 0 and flashing[trial, sample_idx] == 1:\n",
    "                        is_flash_start = True\n",
    "            except IndexError:\n",
    "                print(f\"Warning: IndexError accessing flashing at trial {trial}, sample {sample_idx}. Skipping sample.\")\n",
    "                continue\n",
    "\n",
    "            if is_flash_start:\n",
    "                lower_sample = sample_idx\n",
    "                upper_sample = sample_idx + WINDOW_SAMPLES\n",
    "                if upper_sample > SAMPLES_PER_TRIAL: continue\n",
    "\n",
    "                try:\n",
    "                    if upper_sample > signals.shape[1]: continue\n",
    "                    window = signals[trial, lower_sample:upper_sample, :]\n",
    "                except IndexError:\n",
    "                    print(\n",
    "                        f\"Warning: IndexError extracting window at trial {trial}, samples {lower_sample}:{upper_sample}. Skipping window.\")\n",
    "                    continue\n",
    "\n",
    "                if window.shape[0] != WINDOW_SAMPLES: continue\n",
    "\n",
    "                try:\n",
    "                    if window.size == 0 or np.all(np.std(window, axis=0) == 0):\n",
    "                        normalized_window = window\n",
    "                    else:\n",
    "                        normalized_window = scale(window, axis=0)\n",
    "                except ValueError as e:\n",
    "                    print(\n",
    "                        f\"Warning: ValueError during scaling window at trial {trial}, sample {sample_idx}: {e}. Skipping window.\")\n",
    "                    continue\n",
    "\n",
    "                # Append to the main combined dictionary\n",
    "                samples[target_char].append(normalized_window)\n",
    "                contributor_samples_collected += 1\n",
    "\n",
    "    print(\n",
    "        f\"Feature extraction and grouping complete for contributor {contributor_selected}. Added {contributor_samples_collected} samples.\")\n",
    "\n",
    "# --- End of Contributor Loop ---\n",
    "print(f\"\\n{'=' * 20} Finished Processing All Contributors {'=' * 20}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:37:41.918862300Z",
     "start_time": "2025-05-31T15:36:56.894330100Z"
    }
   },
   "id": "b1b9c226b30c2c21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57bc85c26e0286fa"
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification (Combined Data) --- \n",
      "Total number of samples per character across all contributors:\n",
      "Character 'A': 1440 samples\n",
      "Character 'B': 2160 samples\n",
      "Character 'C': 2160 samples\n",
      "Character 'D': 2880 samples\n",
      "Character 'E': 5760 samples\n",
      "Character 'F': 1440 samples\n",
      "Character 'G': 1440 samples\n",
      "Character 'H': 2880 samples\n",
      "Character 'I': 2160 samples\n",
      "Character 'J': 1440 samples\n",
      "Character 'K': 2160 samples\n",
      "Character 'L': 1440 samples\n",
      "Character 'M': 1440 samples\n",
      "Character 'N': 1440 samples\n",
      "Character 'O': 6480 samples\n",
      "Character 'P': 1440 samples\n",
      "Character 'Q': 2160 samples\n",
      "Character 'R': 3600 samples\n",
      "Character 'S': 0 samples\n",
      "Character 'T': 2880 samples\n",
      "Character 'U': 3600 samples\n",
      "Character 'V': 1440 samples\n",
      "Character 'W': 2160 samples\n",
      "Character 'X': 1440 samples\n",
      "Character 'Y': 1440 samples\n",
      "Character 'Z': 1440 samples\n",
      "Character '1': 0 samples\n",
      "Character '2': 0 samples\n",
      "Character '3': 0 samples\n",
      "Character '4': 720 samples\n",
      "Character '5': 0 samples\n",
      "Character '6': 720 samples\n",
      "Character '7': 0 samples\n",
      "Character '8': 720 samples\n",
      "Character '9': 0 samples\n",
      "Character '_': 720 samples\n",
      "\n",
      "Total samples collected across all characters and contributors: 61200\n",
      "\n",
      "--- Samples for Character 'A' (First 5 from Combined Data) ---\n",
      "Printing the first 5 samples for character 'A':\n",
      "\n",
      "--- Sample 1 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 1.5269679   2.4468462   2.3820097  ... -0.41612127  0.74596584\n",
      "  -0.85115325]\n",
      " [ 1.1498553   2.036791    2.020591   ... -0.0151797   0.99634045\n",
      "  -0.54752934]\n",
      " [ 0.83037424  1.6841043   1.7092923  ...  0.3432492   1.2098042\n",
      "  -0.28544807]\n",
      " ...\n",
      " [-1.4881009   0.23393261  0.03583406 ... -0.49983054 -1.1561428\n",
      "  -1.725664  ]\n",
      " [-1.2441964   0.4488636   0.17224243 ... -0.45400757 -1.0678238\n",
      "  -1.7058161 ]\n",
      " [-0.88442945  0.6771115   0.36175698 ... -0.3258581  -0.955218\n",
      "  -1.6446475 ]]\n",
      "\n",
      "--- Sample 2 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 1.1923571   2.1308413   2.1193001  ... -0.01853927  1.0159131\n",
      "  -0.53260267]\n",
      " [ 0.8684475   1.7655375   1.7965698  ...  0.3402344   1.2292165\n",
      "  -0.27334824]\n",
      " [ 0.6355536   1.4945054   1.5573628  ...  0.62998676  1.3831196\n",
      "  -0.08084709]\n",
      " ...\n",
      " [-1.2348804   0.4861079   0.20307513 ... -0.45778933 -1.0467\n",
      "  -1.6783956 ]\n",
      " [-0.8701264   0.722521    0.3995492  ... -0.32951656 -0.93417895\n",
      "  -1.6178868 ]\n",
      " [-0.49720284  0.90298957  0.58240896 ... -0.15895976 -0.8193338\n",
      "  -1.5365788 ]]\n",
      "\n",
      "--- Sample 3 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 0.8945771   1.8232437   1.864035   ...  0.33986354  1.2559309\n",
      "  -0.25852117]\n",
      " [ 0.65956426  1.5456953   1.6183631  ...  0.6296162   1.4103775\n",
      "  -0.06823   ]\n",
      " [ 0.5276083   1.3852198   1.4748516  ...  0.84327364  1.5019032\n",
      "   0.04928882]\n",
      " ...\n",
      " [-0.8598155   0.7551506   0.42925748 ... -0.32988846 -0.91510326\n",
      "  -1.5876241 ]\n",
      " [-0.48349872  0.93995804  0.61705935 ... -0.15933147 -0.7998525\n",
      "  -1.5072496 ]\n",
      " [-0.20034593  1.0071723   0.72052085 ...  0.01006394 -0.703318\n",
      "  -1.4265375 ]]\n",
      "\n",
      "--- Sample 4 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 0.6755785   1.5826827   1.6660305  ...  0.6325328   1.4459819\n",
      "  -0.0534631 ]\n",
      " [ 0.5429356   1.4195873   1.5196569  ...  0.84632134  1.5382268\n",
      "   0.06274046]\n",
      " [ 0.5018499   1.3755128   1.4740062  ...  0.99514437  1.5825557\n",
      "   0.11880806]\n",
      " ...\n",
      " [-0.47343522  0.967056    0.64475757 ... -0.15689923 -0.7816213\n",
      "  -1.4763774 ]\n",
      " [-0.18880832  1.0353677   0.7502824  ...  0.01260016 -0.684328\n",
      "  -1.3965687 ]\n",
      " [-0.06947366  0.9481039   0.7160259  ...  0.14525756 -0.62210554\n",
      "  -1.3338805 ]]\n",
      "\n",
      "--- Sample 5 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 0.5550374   1.4498405   1.5598485  ...  0.8535294   1.5826696\n",
      "   0.07789775]\n",
      " [ 0.51383424  1.4051827   1.5134405  ...  1.0026907   1.6275135\n",
      "   0.13337526]\n",
      " [ 0.533793    1.4586941   1.5464091  ...  1.1088212   1.6514106\n",
      "   0.15404409]\n",
      " ...\n",
      " [-0.17879824  1.0605354   0.77770823 ...  0.01791363 -0.66570723\n",
      "  -1.3660525 ]\n",
      " [-0.05912246  0.9721166   0.7428833  ...  0.1508725  -0.6027618\n",
      "  -1.3040242 ]\n",
      " [-0.13482359  0.7209537   0.5404624  ...  0.22703995 -0.5832186\n",
      "  -1.2708558 ]]\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print(\"\\n--- Verification (Combined Data) --- \")\n",
    "print(\"Total number of samples per character across all contributors:\")\n",
    "total_samples_collected = 0\n",
    "for char, char_samples in samples.items():\n",
    "    count = len(char_samples)\n",
    "    print(f\"Character '{char}': {count} samples\")\n",
    "    total_samples_collected += count\n",
    "\n",
    "print(f\"\\nTotal samples collected across all characters and contributors: {total_samples_collected}\")\n",
    "\n",
    "# --- Print First 5 Samples for Character 'A' (Combined Data) --- \n",
    "print(\"\\n--- Samples for Character 'A' (First 5 from Combined Data) ---\")\n",
    "char_to_print = 'A'\n",
    "num_samples_to_print = 5\n",
    "\n",
    "if char_to_print in samples and samples[char_to_print]:\n",
    "    print(\n",
    "        f\"Printing the first {min(num_samples_to_print, len(samples[char_to_print]))} samples for character '{char_to_print}':\")\n",
    "    for i, sample_data in enumerate(samples[char_to_print][:num_samples_to_print]):\n",
    "        print(f\"\\n--- Sample {i + 1} for '{char_to_print}' (Shape: {sample_data.shape}) ---\")\n",
    "        # np.set_printoptions(threshold=np.inf) # Uncomment to print full array\n",
    "        print(sample_data)\n",
    "        # np.set_printoptions(threshold=1000) # Reset if needed\n",
    "else:\n",
    "    print(f\"No samples found for character '{char_to_print}' in the combined data or the list is empty.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:37:41.976123700Z",
     "start_time": "2025-05-31T15:37:41.922872600Z"
    }
   },
   "id": "d2a80453dcce8992"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Group samples according to three strategies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8880bd7955777d34"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting sample grouping process...\n",
      "\n",
      "Processing character: \t\t'A' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'B' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'C' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'D' (2880 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 16 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 32 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 24 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'E' (5760 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 32 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 64 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 48 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'F' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'G' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'H' (2880 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 16 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 32 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 24 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'I' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'J' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'K' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'L' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'M' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'N' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'O' (6480 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 36 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 72 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 54 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'P' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Q' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'R' (3600 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 20 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 40 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 30 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'S' (0 samples)\n",
      "  Skipping character \t\t'S' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'T' (2880 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 16 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 32 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 24 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'U' (3600 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 20 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 40 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 30 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'V' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'W' (2160 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 12 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 24 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 18 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'X' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Y' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Z' (1440 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'1' (0 samples)\n",
      "  Skipping character \t\t'1' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'2' (0 samples)\n",
      "  Skipping character \t\t'2' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'3' (0 samples)\n",
      "  Skipping character \t\t'3' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'4' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'5' (0 samples)\n",
      "  Skipping character \t\t'5' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'6' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'7' (0 samples)\n",
      "  Skipping character \t\t'7' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'8' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'9' (0 samples)\n",
      "  Skipping character \t\t'9' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'_' (720 samples)\n",
      "  Grouping for set1 (chunk size: 180, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 90, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 120, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Finished sample grouping.\n",
      "\n",
      "Saving sample groups dictionary to: ../data/sample_groups.pkl\n",
      "Successfully saved sample groups to ../data/sample_groups.pkl.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# 180 360 540 720 900 1080 1260 1440 1620 1800\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "output_filepath = f\"../data/sample_groups.pkl\"\n",
    "\n",
    "strategies = {\n",
    "    \"set1\": {\"chunk_size\": 180, \"random\": False},\n",
    "    \"set2\": {\"chunk_size\": 90, \"random\": False},\n",
    "    \"set3\": {\"chunk_size\": 120, \"random\": True}\n",
    "}\n",
    "\n",
    "print(\"\\nStarting sample grouping process...\")\n",
    "sample_groups = {}  # Initialize the main dictionary to store grouped samples\n",
    "\n",
    "# Process each character\n",
    "for char, char_samples_list in samples.items():\n",
    "    print(f\"\\nProcessing character: \t\t'{char}' ({len(char_samples_list)} samples)\")\n",
    "    if not char_samples_list:\n",
    "        print(f\"  Skipping character \t\t'{char}' as it has no samples.\")\n",
    "        sample_groups[char] = {}  # Still add char key, but with empty dict\n",
    "        continue\n",
    "\n",
    "    sample_groups[char] = {}  # Initialize dictionary for this character's sets\n",
    "\n",
    "    # Process each strategy for the current character\n",
    "    for set_name, params in strategies.items():\n",
    "        print(f\"  Grouping for {set_name} (chunk size: {params['chunk_size']}, random: {params['random']})...\")\n",
    "\n",
    "        chunk_size = params['chunk_size']\n",
    "        is_random = params['random']\n",
    "        current_samples_list = list(char_samples_list)  # Make a copy for potential shuffling\n",
    "\n",
    "        if is_random:\n",
    "            random.shuffle(current_samples_list)\n",
    "\n",
    "        num_samples = len(current_samples_list)\n",
    "        num_chunks = math.ceil(num_samples / chunk_size)\n",
    "\n",
    "        set_chunks = []  # List to hold the chunks for this set\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size\n",
    "            # Slice the list of samples (which are numpy arrays)\n",
    "            chunk = current_samples_list[start_idx:end_idx]\n",
    "\n",
    "            if chunk:  # Ensure chunk is not empty\n",
    "                set_chunks.append(chunk)  # Append the list of numpy arrays\n",
    "\n",
    "        sample_groups[char][set_name] = set_chunks\n",
    "        print(f\"    Created {len(set_chunks)} chunks for {set_name}.\")\n",
    "\n",
    "print(\"\\nFinished sample grouping.\")\n",
    "\n",
    "# --- Save Sample Groups Dictionary --- \n",
    "print(f\"\\nSaving sample groups dictionary to: {output_filepath}\")\n",
    "try:\n",
    "    with open(output_filepath, \"wb\") as f:\n",
    "        pickle.dump(sample_groups, f)\n",
    "    print(f\"Successfully saved sample groups to {output_filepath}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving sample groups dictionary: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")  # Final finished print"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:37:55.324445800Z",
     "start_time": "2025-05-31T15:37:41.936308300Z"
    }
   },
   "id": "846981811b9ef29f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
