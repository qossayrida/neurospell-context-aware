{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Character Identity "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c586d2047dd892ab"
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded characters from ../data/characters.txt: ABCDEFGHIJKLMNOPQRSTUVWXYZ123456789_\n",
      "\n",
      "Initialized combined samples dictionary for characters: A, B, C, D, E, F, G, H, I, J, K, L, M, N, O, P, Q, R, S, T, U, V, W, X, Y, Z, 1, 2, 3, 4, 5, 6, 7, 8, 9, _\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "\n",
    "# Define a dummy print_data if DataCraft is not available/needed\n",
    "def print_data(signals, words_string, contributor, freq):\n",
    "    print(f\"  Contributor: {contributor}\")\n",
    "    print(f\"  Signals shape: {signals.shape}\")\n",
    "    if words_string is not None:\n",
    "        print(f\"  Target Characters String Length: {len(words_string) if isinstance(words_string, str) else 'N/A'}\")\n",
    "    print(f\"  Sampling Frequency (initial): {freq}\")\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Configuration ---\n",
    "contributors_to_process = [\"I\", \"II\"]  # List of contributors to process\n",
    "characters_file_path = \"../../data/characters.txt\"  # Updated path\n",
    "channels = list(range(64))\n",
    "initial_sampling_frequency = 240\n",
    "down_sampling_frequency = 120\n",
    "WINDOW_DURATION = 650  # ms\n",
    "\n",
    "# --- File Checks & Character Loading ---\n",
    "try:\n",
    "    with open(characters_file_path, \"r\") as f:\n",
    "        characters = f.read().strip()\n",
    "    print(f\"Loaded characters from {characters_file_path}: {characters}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Characters file not found at {characters_file_path}\")\n",
    "    exit()\n",
    "\n",
    "# --- Initialize Combined Samples Dictionary --- \n",
    "# Initialize dictionary before the loop\n",
    "samples = {char: [] for char in characters}\n",
    "print(f\"\\nInitialized combined samples dictionary for characters: {', '.join(samples.keys())}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:44:50.416642300Z",
     "start_time": "2025-05-31T15:44:50.320643800Z"
    }
   },
   "id": "fc9a70b2966eb0ca"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loop Through Contributors"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f84049b6958260a5"
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Processing Contributor: I ====================\n",
      "Loading data from: ../data/Contributor_I.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): EAEVQTDOJG8RBRGONCEDHCTUIDBPUHMEM6OUXOCFOUKWA4VJEF...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      " Data Info (Initial):\n",
      "  Contributor: I\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor I. Added 2550 samples.\n",
      "\n",
      "==================== Processing Contributor: II ====================\n",
      "Loading data from: ../data/Contributor_II.mat\n",
      "Number of trials based on Signal array: 85\n",
      "Extracted Target Character String (length 85): VGREAAH8TVRHBYN_UGCOLO4EUERDOOHCIFOMDNU6LQCPKEIREK...\n",
      "TargetChar string length matches number of signal trials.\n",
      "\n",
      " Data Info (Initial):\n",
      "  Contributor: II\n",
      "  Signals shape: (85, 7794, 64)\n",
      "  Target Characters String Length: 85\n",
      "  Sampling Frequency (initial): 240\n",
      "Applying Butterworth filter...\n",
      "Filtering complete.\n",
      "Downsampling signals from 240Hz to 120Hz...\n",
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n",
      "Downsampling complete.\n",
      "Extracting features and grouping by character...\n",
      "Feature extraction and grouping complete for contributor II. Added 2550 samples.\n",
      "\n",
      "==================== Finished Processing All Contributors ====================\n"
     ]
    }
   ],
   "source": [
    "for contributor_selected in contributors_to_process:\n",
    "    print(f\"\\n{'=' * 20} Processing Contributor: {contributor_selected} {'=' * 20}\")\n",
    "    contributor_data_file_path = f\"../../data/Contributor_{contributor_selected}.mat\"\n",
    "\n",
    "    # Check if data file exists for the current contributor\n",
    "    if not os.path.exists(contributor_data_file_path):\n",
    "        print(\n",
    "            f\"Warning: data file not found for contributor {contributor_selected} at {contributor_data_file_path}. Skipping this contributor.\")\n",
    "        continue  # Skip to the next contributor\n",
    "\n",
    "    # --- Load Data ---\n",
    "    print(f\"Loading data from: {contributor_data_file_path}\")\n",
    "    try:\n",
    "        data = loadmat(contributor_data_file_path)\n",
    "        signals = data[\"Signal\"]  # Shape: (Trials, Samples, Channels)\n",
    "        flashing = data[\"Flashing\"]\n",
    "        stimulus = data[\"StimulusType\"]\n",
    "        word_raw = data[\"TargetChar\"]  # Expected: String of target chars\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for contributor {contributor_selected}: {e}. Skipping this contributor.\")\n",
    "        continue\n",
    "        \n",
    "    # --- Data Consistency Checks & Target Character Extraction ---\n",
    "    num_signal_trials = signals.shape[0]\n",
    "    print(f\"Number of trials based on Signal array: {num_signal_trials}\")\n",
    "\n",
    "    target_char_string = None\n",
    "    if isinstance(word_raw, np.ndarray):\n",
    "        if word_raw.size == 1:\n",
    "            target_char_string = str(word_raw.item()).strip()\n",
    "        else:\n",
    "            print(f\"Warning: Unexpected shape for TargetChar: {word_raw.shape}. Trying to flatten.\")\n",
    "            try:\n",
    "                target_char_string = \"\".join(map(str, word_raw.flatten()))\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing TargetChar array: {e}\")\n",
    "    elif isinstance(word_raw, str):\n",
    "        target_char_string = word_raw.strip()\n",
    "    else:\n",
    "        print(f\"Error: Unexpected type for TargetChar: {type(word_raw)}\")\n",
    "\n",
    "    if target_char_string is None:\n",
    "        print(\"Error: Could not extract target character string. Skipping this contributor.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"Extracted Target Character String (length {len(target_char_string)}): {target_char_string[:50]}...\")\n",
    "\n",
    "##    print(f\" target_char_string ====> {target_char_string}\")\n",
    "##    print(f\" num_signal_trials ====> {num_signal_trials}\")\n",
    "    if len(target_char_string) != num_signal_trials:\n",
    "        print(\n",
    "            f\"Error: Length of TargetChar string ({len(target_char_string)}) does not match number of trials in Signal ({num_signal_trials}). Please check data integrity.\")\n",
    "        print(\"Warning: Proceeding with minimum of the two lengths for processing this contributor.\")\n",
    "        trials_to_process = min(len(target_char_string), num_signal_trials)\n",
    "    else:\n",
    "        trials_to_process = num_signal_trials\n",
    "        print(\"TargetChar string length matches number of signal trials.\")\n",
    "\n",
    "    print(f\"\\n Data Info (Initial):\")\n",
    "    print_data(signals, target_char_string, contributor_selected, initial_sampling_frequency)\n",
    "\n",
    "    # --- Butterworth Filter ---\n",
    "    print(\"Applying Butterworth filter...\")\n",
    "    sampling_frequency = initial_sampling_frequency\n",
    "    b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], \"bandpass\")\n",
    "    for trial in range(num_signal_trials):\n",
    "        try:\n",
    "            signals[trial, :, :] = signal.filtfilt(b, a, signals[trial, :, :], axis=0)\n",
    "        except IndexError:\n",
    "            print(f\"Error: IndexError during filtering trial {trial}. Check signal dimensions.\")\n",
    "            continue\n",
    "    print(\"Filtering complete.\")\n",
    "\n",
    "    # --- Down-sampling ---\n",
    "    print(f\"Downsampling signals from {initial_sampling_frequency}Hz to {down_sampling_frequency}Hz...\")\n",
    "    SCALE_FACTOR = round(initial_sampling_frequency / down_sampling_frequency)\n",
    "    sampling_frequency = down_sampling_frequency\n",
    "\n",
    "    print(f\"# Samples of EEG signals before downsampling: {signals.shape[1]}\")\n",
    "    try:\n",
    "        signals = signals[:, ::SCALE_FACTOR, :]\n",
    "        flashing = flashing[:, ::SCALE_FACTOR]\n",
    "        stimulus = stimulus[:, ::SCALE_FACTOR]\n",
    "    except IndexError as e:\n",
    "        print(f\"Error during downsampling: {e}. Check array dimensions after filtering.\")\n",
    "        continue  # Skip feature extraction for this contributor if downsampling fails\n",
    "    print(f\"# Samples of EEG signals after downsampling: {signals.shape[1]}\")\n",
    "    print(\"Downsampling complete.\")\n",
    "\n",
    "    # --- Feature Extraction & Grouping ---\n",
    "    print(\"Extracting features and grouping by character...\")\n",
    "    N_CHANNELS = signals.shape[2]\n",
    "    WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "    SAMPLES_PER_TRIAL = signals.shape[1]\n",
    "##    print(f\"Samples per trials = {SAMPLES_PER_TRIAL}\")\n",
    "\n",
    "    contributor_samples_collected = 0\n",
    "    for trial in range(trials_to_process):  # 1 - 85 \n",
    "        target_char = target_char_string[trial]\n",
    "\n",
    "        if target_char not in samples:\n",
    "            print(\n",
    "                f\"Warning: Target character '{target_char}' from trial {trial} not found in characters.txt. Skipping this trial.\")\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if trial >= flashing.shape[0] or trial >= stimulus.shape[0]:\n",
    "            print(\n",
    "                f\"Warning: Trial index {trial} out of bounds for flashing/stimulus arrays. Stopping processing for this contributor.\")\n",
    "            break\n",
    "\n",
    "        for sample_idx in range(SAMPLES_PER_TRIAL):  # 0 - 3394\n",
    "            is_flash_start = False\n",
    "            \n",
    "            try:\n",
    "                if sample_idx >= flashing.shape[1]: break\n",
    "                if sample_idx == 0 and flashing[trial, sample_idx] == 1:\n",
    "                    is_flash_start = True\n",
    "                elif sample_idx > 0:\n",
    "                    if sample_idx - 1 >= flashing.shape[1]: break\n",
    "                    if flashing[trial, sample_idx - 1] == 0 and flashing[trial, sample_idx] == 1:\n",
    "                        is_flash_start = True\n",
    "            except IndexError:\n",
    "                print(f\"Warning: IndexError accessing flashing at trial {trial}, sample {sample_idx}. Skipping sample.\")\n",
    "                continue\n",
    "                \n",
    "            if stimulus[trial,sample_idx]==0:\n",
    "                continue\n",
    "\n",
    "            if is_flash_start:\n",
    "                lower_sample = sample_idx\n",
    "                upper_sample = sample_idx + WINDOW_SAMPLES\n",
    "                if upper_sample > SAMPLES_PER_TRIAL: continue\n",
    "\n",
    "                try:\n",
    "                    if upper_sample > signals.shape[1]: continue\n",
    "                    window = signals[trial, lower_sample:upper_sample, :]\n",
    "                except IndexError:\n",
    "                    print(\n",
    "                        f\"Warning: IndexError extracting window at trial {trial}, samples {lower_sample}:{upper_sample}. Skipping window.\")\n",
    "                    continue\n",
    "\n",
    "                if window.shape[0] != WINDOW_SAMPLES: continue\n",
    "\n",
    "                try:\n",
    "                    if window.size == 0 or np.all(np.std(window, axis=0) == 0):\n",
    "                        normalized_window = window\n",
    "                    else:\n",
    "                        normalized_window = scale(window, axis=0)\n",
    "                except ValueError as e:\n",
    "                    print(\n",
    "                        f\"Warning: ValueError during scaling window at trial {trial}, sample {sample_idx}: {e}. Skipping window.\")\n",
    "                    continue\n",
    "\n",
    "                # Append to the main combined dictionary\n",
    "                samples[target_char].append(normalized_window)\n",
    "                contributor_samples_collected += 1\n",
    "\n",
    "    print(\n",
    "        f\"Feature extraction and grouping complete for contributor {contributor_selected}. Added {contributor_samples_collected} samples.\")\n",
    "\n",
    "# --- End of Contributor Loop ---\n",
    "print(f\"\\n{'=' * 20} Finished Processing All Contributors {'=' * 20}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:45:02.350595500Z",
     "start_time": "2025-05-31T15:44:50.360934900Z"
    }
   },
   "id": "b1b9c226b30c2c21"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Verification"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "57bc85c26e0286fa"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verification (Combined Data) --- \n",
      "Total number of samples per character across all contributors:\n",
      "Character 'A': 120 samples\n",
      "Character 'B': 180 samples\n",
      "Character 'C': 180 samples\n",
      "Character 'D': 240 samples\n",
      "Character 'E': 480 samples\n",
      "Character 'F': 120 samples\n",
      "Character 'G': 120 samples\n",
      "Character 'H': 240 samples\n",
      "Character 'I': 180 samples\n",
      "Character 'J': 120 samples\n",
      "Character 'K': 180 samples\n",
      "Character 'L': 120 samples\n",
      "Character 'M': 120 samples\n",
      "Character 'N': 120 samples\n",
      "Character 'O': 540 samples\n",
      "Character 'P': 120 samples\n",
      "Character 'Q': 180 samples\n",
      "Character 'R': 300 samples\n",
      "Character 'S': 0 samples\n",
      "Character 'T': 240 samples\n",
      "Character 'U': 300 samples\n",
      "Character 'V': 120 samples\n",
      "Character 'W': 180 samples\n",
      "Character 'X': 120 samples\n",
      "Character 'Y': 120 samples\n",
      "Character 'Z': 120 samples\n",
      "Character '1': 0 samples\n",
      "Character '2': 0 samples\n",
      "Character '3': 0 samples\n",
      "Character '4': 60 samples\n",
      "Character '5': 0 samples\n",
      "Character '6': 60 samples\n",
      "Character '7': 0 samples\n",
      "Character '8': 60 samples\n",
      "Character '9': 0 samples\n",
      "Character '_': 60 samples\n",
      "\n",
      "Total samples collected across all characters and contributors: 5100\n",
      "\n",
      "--- Samples for Character 'A' (First 5 from Combined Data) ---\n",
      "Printing the first 5 samples for character 'A':\n",
      "\n",
      "--- Sample 1 for 'A' (Shape: (78, 64)) ---\n",
      "[[ 1.5269679   2.4468462   2.3820097  ... -0.41612127  0.74596584\n",
      "  -0.85115325]\n",
      " [ 1.1498553   2.036791    2.020591   ... -0.0151797   0.99634045\n",
      "  -0.54752934]\n",
      " [ 0.83037424  1.6841043   1.7092923  ...  0.3432492   1.2098042\n",
      "  -0.28544807]\n",
      " ...\n",
      " [-1.4881009   0.23393261  0.03583406 ... -0.49983054 -1.1561428\n",
      "  -1.725664  ]\n",
      " [-1.2441964   0.4488636   0.17224243 ... -0.45400757 -1.0678238\n",
      "  -1.7058161 ]\n",
      " [-0.88442945  0.6771115   0.36175698 ... -0.3258581  -0.955218\n",
      "  -1.6446475 ]]\n",
      "\n",
      "--- Sample 2 for 'A' (Shape: (78, 64)) ---\n",
      "[[-0.03063992  0.2040172   0.19283181 ... -2.1880515   0.6813935\n",
      "  -0.78624475]\n",
      " [-0.1025832  -0.10047792 -0.02324355 ... -2.278403    0.44569674\n",
      "  -0.9310363 ]\n",
      " [-0.13414295 -0.30414206 -0.12324184 ... -2.1832821   0.3652425\n",
      "  -0.92979723]\n",
      " ...\n",
      " [-1.189907   -1.2798034  -1.5616812  ...  0.5695779   0.38371864\n",
      "  -1.1463373 ]\n",
      " [-1.2787447  -1.4600979  -1.6258503  ...  0.47741395  0.29938695\n",
      "  -1.1447345 ]\n",
      " [-1.3986615  -1.6861359  -1.7340046  ...  0.42391255  0.25460255\n",
      "  -1.1190511 ]]\n",
      "\n",
      "--- Sample 3 for 'A' (Shape: (78, 64)) ---\n",
      "[[-3.21164519e-01  1.72668982e+00  1.23890805e+00 ...  3.35930735e-01\n",
      "  -1.03609115e-01 -3.26334536e-01]\n",
      " [-2.17503170e-03  1.85039568e+00  1.46950603e+00 ...  3.05327564e-01\n",
      "  -2.50161499e-01 -3.99520755e-01]\n",
      " [ 3.25615466e-01  1.94159496e+00  1.67679656e+00 ...  3.16775590e-01\n",
      "  -2.82785833e-01 -4.32724953e-01]\n",
      " ...\n",
      " [ 1.98350382e+00 -4.79109526e-01 -1.00105727e+00 ... -1.43545425e+00\n",
      "  -1.26300752e+00 -9.87055719e-01]\n",
      " [ 2.26852489e+00 -4.06145006e-01 -1.03597105e+00 ... -1.51404810e+00\n",
      "  -1.16436982e+00 -1.23911691e+00]\n",
      " [ 2.45549583e+00 -3.87217045e-01 -1.16536081e+00 ... -1.47422516e+00\n",
      "  -9.04376447e-01 -1.31817102e+00]]\n",
      "\n",
      "--- Sample 4 for 'A' (Shape: (78, 64)) ---\n",
      "[[-2.3980103  -0.85211927 -0.42803907 ... -0.80184704 -1.6342857\n",
      "  -1.7187842 ]\n",
      " [-2.3972032  -0.971038   -0.6200828  ... -0.64730865 -1.4306933\n",
      "  -0.9929661 ]\n",
      " [-2.332972   -1.0066952  -0.7530197  ... -0.44961342 -1.1453563\n",
      "  -0.07800394]\n",
      " ...\n",
      " [ 0.07449727 -0.10921974 -0.11885891 ...  0.56107336  0.43426314\n",
      "  -0.29164913]\n",
      " [ 0.14664818  0.05438744  0.05249652 ...  0.6446729   0.5434804\n",
      "  -0.1001844 ]\n",
      " [ 0.2027078   0.22123167  0.22223683 ...  0.71781665  0.60542285\n",
      "   0.07936677]]\n",
      "\n",
      "--- Sample 5 for 'A' (Shape: (78, 64)) ---\n",
      "[[-1.4316727   0.49801776  0.57855785 ... -1.3937538   1.1070515\n",
      "  -1.2863404 ]\n",
      " [-1.5173284   0.01414278 -0.07705036 ... -1.3390865   1.1858156\n",
      "  -1.1500673 ]\n",
      " [-1.603652   -0.4697208  -0.6974529  ... -1.2947226   1.200313\n",
      "  -1.0663683 ]\n",
      " ...\n",
      " [ 0.4801761  -0.74232537 -1.1502578  ...  0.4527912  -0.08188596\n",
      "   0.7073202 ]\n",
      " [ 0.15811674 -1.4417368  -1.9230837  ...  0.18598327 -0.4142934\n",
      "   0.00676287]\n",
      " [-0.04583384 -1.908158   -2.489336   ...  0.06266    -0.66953236\n",
      "  -0.5720575 ]]\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "print(\"\\n--- Verification (Combined Data) --- \")\n",
    "print(\"Total number of samples per character across all contributors:\")\n",
    "total_samples_collected = 0\n",
    "for char, char_samples in samples.items():\n",
    "    count = len(char_samples)\n",
    "    print(f\"Character '{char}': {count} samples\")\n",
    "    total_samples_collected += count\n",
    "\n",
    "print(f\"\\nTotal samples collected across all characters and contributors: {total_samples_collected}\")\n",
    "\n",
    "# --- Print First 5 Samples for Character 'A' (Combined Data) --- \n",
    "print(\"\\n--- Samples for Character 'A' (First 5 from Combined Data) ---\")\n",
    "char_to_print = 'A'\n",
    "num_samples_to_print = 5\n",
    "\n",
    "if char_to_print in samples and samples[char_to_print]:\n",
    "    print(\n",
    "        f\"Printing the first {min(num_samples_to_print, len(samples[char_to_print]))} samples for character '{char_to_print}':\")\n",
    "    for i, sample_data in enumerate(samples[char_to_print][:num_samples_to_print]):\n",
    "        print(f\"\\n--- Sample {i + 1} for '{char_to_print}' (Shape: {sample_data.shape}) ---\")\n",
    "        # np.set_printoptions(threshold=np.inf) # Uncomment to print full array\n",
    "        print(sample_data)\n",
    "        # np.set_printoptions(threshold=1000) # Reset if needed\n",
    "else:\n",
    "    print(f\"No samples found for character '{char_to_print}' in the combined data or the list is empty.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:45:02.409560800Z",
     "start_time": "2025-05-31T15:45:02.354110900Z"
    }
   },
   "id": "d2a80453dcce8992"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#  Group samples according to three strategies"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8880bd7955777d34"
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting sample grouping process...\n",
      "\n",
      "Processing character: \t\t'A' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'B' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'C' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'D' (240 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'E' (480 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 16 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 32 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 24 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'F' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'G' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'H' (240 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'I' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'J' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'K' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'L' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'M' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'N' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'O' (540 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 18 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 36 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 27 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'P' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Q' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'R' (300 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 10 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 20 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 15 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'S' (0 samples)\n",
      "  Skipping character \t\t'S' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'T' (240 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 8 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 16 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 12 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'U' (300 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 10 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 20 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 15 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'V' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'W' (180 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 6 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 12 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 9 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'X' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Y' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'Z' (120 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 4 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 8 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 6 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'1' (0 samples)\n",
      "  Skipping character \t\t'1' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'2' (0 samples)\n",
      "  Skipping character \t\t'2' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'3' (0 samples)\n",
      "  Skipping character \t\t'3' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'4' (60 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'5' (0 samples)\n",
      "  Skipping character \t\t'5' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'6' (60 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'7' (0 samples)\n",
      "  Skipping character \t\t'7' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'8' (60 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Processing character: \t\t'9' (0 samples)\n",
      "  Skipping character \t\t'9' as it has no samples.\n",
      "\n",
      "Processing character: \t\t'_' (60 samples)\n",
      "  Grouping for set1 (chunk size: 30, random: False)...\n",
      "    Created 2 chunks for set1.\n",
      "  Grouping for set2 (chunk size: 15, random: False)...\n",
      "    Created 4 chunks for set2.\n",
      "  Grouping for set3 (chunk size: 20, random: True)...\n",
      "    Created 3 chunks for set3.\n",
      "\n",
      "Finished sample grouping.\n",
      "\n",
      "Saving sample groups dictionary to: ../data/sample_groups.pkl\n",
      "Successfully saved sample groups to ../data/sample_groups.pkl.\n",
      "\n",
      "Script finished.\n"
     ]
    }
   ],
   "source": [
    "# 180 360 540 720 900 1080 1260 1440 1620 1800\n",
    "# 30 60 90 120 150 180 210 240 270 300 330 360\n",
    "\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "output_filepath = f\"../../data/sample_groups.pkl\"\n",
    "\n",
    "strategies = {\n",
    "    \"set1\": {\"chunk_size\": 30, \"random\": False},\n",
    "    \"set2\": {\"chunk_size\": 20, \"random\": True}\n",
    "}\n",
    "\n",
    "print(\"\\nStarting sample grouping process...\")\n",
    "sample_groups = {}  # Initialize the main dictionary to store grouped samples\n",
    "\n",
    "# Process each character\n",
    "for char, char_samples_list in samples.items():\n",
    "    print(f\"\\nProcessing character: \t\t'{char}' ({len(char_samples_list)} samples)\")\n",
    "    if not char_samples_list:\n",
    "        print(f\"  Skipping character \t\t'{char}' as it has no samples.\")\n",
    "        sample_groups[char] = {}  # Still add char key, but with empty dict\n",
    "        continue\n",
    "\n",
    "    sample_groups[char] = {}  # Initialize dictionary for this character's sets\n",
    "\n",
    "    # Process each strategy for the current character\n",
    "    for set_name, params in strategies.items():\n",
    "        print(f\"  Grouping for {set_name} (chunk size: {params['chunk_size']}, random: {params['random']})...\")\n",
    "\n",
    "        chunk_size = params['chunk_size']\n",
    "        is_random = params['random']\n",
    "        current_samples_list = list(char_samples_list)  # Make a copy for potential shuffling\n",
    "\n",
    "        if is_random:\n",
    "            random.shuffle(current_samples_list)\n",
    "\n",
    "        num_samples = len(current_samples_list)\n",
    "        num_chunks = math.ceil(num_samples / chunk_size)\n",
    "\n",
    "        set_chunks = []  # List to hold the chunks for this set\n",
    "\n",
    "        for i in range(num_chunks):\n",
    "            start_idx = i * chunk_size\n",
    "            end_idx = start_idx + chunk_size\n",
    "            # Slice the list of samples (which are numpy arrays)\n",
    "            chunk = current_samples_list[start_idx:end_idx]\n",
    "\n",
    "            if chunk:  # Ensure chunk is not empty\n",
    "                set_chunks.append(chunk)  # Append the list of numpy arrays\n",
    "\n",
    "        sample_groups[char][set_name] = set_chunks\n",
    "        print(f\"    Created {len(set_chunks)} chunks for {set_name}.\")\n",
    "\n",
    "print(\"\\nFinished sample grouping.\")\n",
    "\n",
    "# --- Save Sample Groups Dictionary --- \n",
    "print(f\"\\nSaving sample groups dictionary to: {output_filepath}\")\n",
    "try:\n",
    "    with open(output_filepath, \"wb\") as f:\n",
    "        pickle.dump(sample_groups, f)\n",
    "    print(f\"Successfully saved sample groups to {output_filepath}.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving sample groups dictionary: {e}\")\n",
    "\n",
    "print(\"\\nScript finished.\")  # Final finished print"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-31T15:45:02.658358100Z",
     "start_time": "2025-05-31T15:45:02.373796400Z"
    }
   },
   "id": "846981811b9ef29f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
