{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create EEG Sentences\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8e74093c265c6a51"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import numpy as np                \n",
    "import warnings\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "contributor_selected = \"I\"                                 \n",
    "contributor_train_file_path = '../data/Contributor_' + contributor_selected + '_Train.mat'\n",
    "contributor_test_file_path = '../data/Contributor_' + contributor_selected + '_Test.mat'\n",
    "channel_name_file_path = '../data/channels.csv'\n",
    "channels = [i for i in range(64)]\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-13T09:39:21.495286Z",
     "start_time": "2025-05-13T09:39:20.996891700Z"
    }
   },
   "id": "189d752f13e679c2"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Contributor     Sampling Freq. (Hz)  Recording (min)      Trials     Spelled Word                  \n",
      "==============================================================================================================\n",
      "I               240.00               46.01                85         EAEVQTDOJG8RBRGONCEDHCTUIDBPUH\n",
      "                                                                     MEM6OUXOCFOUKWA4VJEFRZROLHYNQD\n",
      "                                                                     W_EKTLBWXEPOUIKZERYOOTHQI     \n"
     ]
    }
   ],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "from bundle.DataCraft import * \n",
    "\n",
    "\n",
    "data_train = loadmat(contributor_train_file_path)\n",
    "signals_train = data_train['Signal']\n",
    "flashing_train = data_train['Flashing']\n",
    "stimulus_train = data_train['StimulusType']\n",
    "word_train = data_train['TargetChar']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_train = (len(signals_train)) * (len(signals_train[0])) / (sampling_frequency * 60)\n",
    "trials_train = len(word_train[0])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print_data(signals_train, word_train, contributor_selected, sampling_frequency)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-13T09:39:22.108679900Z",
     "start_time": "2025-05-13T09:39:21.499710800Z"
    }
   },
   "id": "374de39660eac2dd"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n"
     ]
    }
   ],
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_train[0])))\n",
    "\n",
    "signals_train = signals_train[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_train = flashing_train[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_train = stimulus_train[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_train[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-13T09:39:25.888106700Z",
     "start_time": "2025-05-13T09:39:22.109688800Z"
    }
   },
   "id": "a22fcd76032f5d23"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: (15300, 78, 64)\n"
     ]
    }
   ],
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 64, 78)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-13T09:39:35.972209100Z",
     "start_time": "2025-05-13T09:39:25.901576600Z"
    }
   },
   "id": "c1ed8d5edff2a1bd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# =======================================================\n",
    "# =======================================================\n",
    "# ======================================================="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48782c33112f2e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: (15300, 78, 64)\n",
      "Generated 50 sentences from 728 characters.\n",
      "Sentence 0: THEY MAKE FOOD.\n",
      "Sentence 1: YOU SEE POEMS.\n",
      "Sentence 2: THEY PLAY WATER.\n",
      "Sentence 3: SHE SEES EMAILS\n",
      "Sentence 4: HE EATS GAMES.\n",
      "Character to sentence mapping (first 10 characters):\n",
      "Char 0: 'T', Sentence 0, Position 0\n",
      "Char 1: 'H', Sentence 0, Position 1\n",
      "Char 2: 'E', Sentence 0, Position 2\n",
      "Char 3: 'Y', Sentence 0, Position 3\n",
      "Char 4: ' ', Sentence 0, Position 4\n",
      "Char 5: 'M', Sentence 0, Position 5\n",
      "Char 6: 'A', Sentence 0, Position 6\n",
      "Char 7: 'K', Sentence 0, Position 7\n",
      "Char 8: 'E', Sentence 0, Position 8\n",
      "Char 9: ' ', Sentence 0, Position 9\n",
      "\n",
      "Mapping 728 P300 responses to characters in sentences\n",
      "Created ECD training dataset with 728 samples\n",
      "Created 678 context samples for NLP model training\n",
      "Created 678 combined samples for DFM training\n",
      "\n",
      "Example of ECD training data:\n",
      "Target character: 'T'\n",
      "From sentence 0: 'THEY MAKE FOOD.'\n",
      "At position 0\n",
      "Feature shape: (78, 64)\n",
      "\n",
      "Example of NLP context data:\n",
      "Context: 'T'\n",
      "Target: 'H'\n",
      "From sentence 0: 'THEY MAKE FOOD.'\n",
      "\n",
      "Example of combined DFM data:\n",
      "Context: 'T'\n",
      "Target: 'H'\n",
      "From sentence 0: 'THEY MAKE FOOD.'\n",
      "Feature shape: (78, 64)\n"
     ]
    }
   ],
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 64, 78)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)\n",
    "\n",
    "# Generate synthetic sentences from collected data\n",
    "import random\n",
    "import string\n",
    "\n",
    "def generate_sentences(num_sentences=50, mean_length=10, std_length=3):\n",
    "    \"\"\"Generate random but somewhat plausible sentences.\"\"\"\n",
    "    subjects = ['I', 'YOU', 'HE', 'SHE', 'THEY', 'WE']\n",
    "    verbs = ['WRITE', 'LIKE', 'EAT', 'READ', 'SEE', 'USE', 'HELP', 'MAKE', 'PLAY', 'WANT']\n",
    "    verbs_s = ['WRITES', 'LIKES', 'EATS', 'READS', 'SEES', 'USES', 'HELPS', 'MAKES', 'PLAYS', 'WANTS']\n",
    "    objects = ['FOOD', 'MUSIC', 'BOOKS', 'GAMES', 'POEMS', 'PIZZA', 'WATER', 'PAPER', 'PHONES', 'EMAILS']\n",
    "    \n",
    "    sentences = []\n",
    "    for _ in range(num_sentences):\n",
    "        subject = random.choice(subjects)\n",
    "        if subject in ['I', 'YOU', 'THEY', 'WE']:\n",
    "            verb = random.choice(verbs)\n",
    "        else:\n",
    "            verb = random.choice(verbs_s)\n",
    "        obj = random.choice(objects)\n",
    "        \n",
    "        sentence = f\"{subject} {verb} {obj}\"\n",
    "        # Add a period sometimes\n",
    "        if random.random() > 0.3:\n",
    "            sentence += \".\"\n",
    "            \n",
    "        sentences.append(sentence)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "# Generate sentences\n",
    "num_sentences = 50\n",
    "sentences = generate_sentences(num_sentences)\n",
    "\n",
    "# Flatten sentences into a character stream\n",
    "all_chars = []\n",
    "char_to_sentence_map = []  # Will store (sentence_idx, position_in_sentence) for each char\n",
    "\n",
    "for sent_idx, sentence in enumerate(sentences):\n",
    "    for pos_idx, char in enumerate(sentence):\n",
    "        all_chars.append(char)\n",
    "        char_to_sentence_map.append((sent_idx, pos_idx))\n",
    "\n",
    "# Print the first few sentences\n",
    "print(f\"Generated {len(sentences)} sentences from {len(all_chars)} characters.\")\n",
    "for i in range(min(5, len(sentences))):\n",
    "    print(f\"Sentence {i}: {sentences[i]}\")\n",
    "\n",
    "# Print the character to sentence mapping for the first 10 characters\n",
    "print(\"Character to sentence mapping (first 10 characters):\")\n",
    "for i in range(min(10, len(all_chars))):\n",
    "    sent_idx, pos_idx = char_to_sentence_map[i]\n",
    "    print(f\"Char {i}: '{all_chars[i]}', Sentence {sent_idx}, Position {pos_idx}\")\n",
    "\n",
    "# Now let's prepare the data structure for the ECD model training\n",
    "# Based on the Wadsworth BCI dataset documentation, we use the proper variable names\n",
    "# In the dataset, StimulusCode indicates which row/column was flashed:\n",
    "# 1-6 for columns, 7-12 for rows\n",
    "\n",
    "# First, let's create a mapping of our extracted P300 responses to characters for the ECD training\n",
    "# We'll create a simplified version since we don't have access to the exact StimulusCode values\n",
    "\n",
    "# Create a random mapping of P300 responses to characters for demonstration\n",
    "# In a real implementation, we'd use the actual target characters from the dataset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming we have a certain number of P300 responses identified by train_labels == 1\n",
    "p300_indices = np.where(train_labels == 1)[0]\n",
    "\n",
    "# Map a subset of P300 responses to characters in our generated sentences\n",
    "num_chars_to_map = min(len(p300_indices), len(all_chars))\n",
    "print(f\"\\nMapping {num_chars_to_map} P300 responses to characters in sentences\")\n",
    "\n",
    "ecd_train_data = []\n",
    "for i in range(num_chars_to_map):\n",
    "    ecd_train_data.append({\n",
    "        'feature': train_features[p300_indices[i]],\n",
    "        'target_char': all_chars[i],\n",
    "        'sentence_idx': char_to_sentence_map[i][0],\n",
    "        'position_idx': char_to_sentence_map[i][1]\n",
    "    })\n",
    "\n",
    "print(f\"Created ECD training dataset with {len(ecd_train_data)} samples\")\n",
    "\n",
    "# Create context data for the NLP model\n",
    "sentence_contexts = []\n",
    "for i in range(len(sentences)):\n",
    "    # For each position in the sentence, create a context\n",
    "    sentence = sentences[i]\n",
    "    for j in range(len(sentence)):\n",
    "        context = sentence[:j]\n",
    "        target = sentence[j]\n",
    "        if context:  # Only add if there's actual context\n",
    "            sentence_contexts.append({\n",
    "                'context': context,\n",
    "                'target': target,\n",
    "                'sentence_idx': i,\n",
    "                'position_idx': j\n",
    "            })\n",
    "\n",
    "print(f\"Created {len(sentence_contexts)} context samples for NLP model training\")\n",
    "\n",
    "# Finally, create the combined dataset for the Decision Fusion Model\n",
    "dfm_train_data = []\n",
    "for ecd_sample in ecd_train_data:\n",
    "    sent_idx = ecd_sample['sentence_idx']\n",
    "    pos_idx = ecd_sample['position_idx']\n",
    "    \n",
    "    # Find matching NLP context\n",
    "    matching_contexts = [c for c in sentence_contexts \n",
    "                         if c['sentence_idx'] == sent_idx and c['position_idx'] == pos_idx]\n",
    "    \n",
    "    if matching_contexts:\n",
    "        nlp_sample = matching_contexts[0]\n",
    "        dfm_train_data.append({\n",
    "            'ecd_feature': ecd_sample['feature'],\n",
    "            'nlp_context': nlp_sample['context'],\n",
    "            'target_char': ecd_sample['target_char'],\n",
    "            'sentence_idx': sent_idx,\n",
    "            'position_idx': pos_idx\n",
    "        })\n",
    "\n",
    "print(f\"Created {len(dfm_train_data)} combined samples for DFM training\")\n",
    "\n",
    "# Save a few examples of the generated data\n",
    "print(\"\\nExample of ECD training data:\")\n",
    "if ecd_train_data:\n",
    "    example = ecd_train_data[0]\n",
    "    print(f\"Target character: '{example['target_char']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "    print(f\"At position {example['position_idx']}\")\n",
    "    print(f\"Feature shape: {example['feature'].shape}\")\n",
    "\n",
    "print(\"\\nExample of NLP context data:\")\n",
    "if sentence_contexts:\n",
    "    example = sentence_contexts[0]\n",
    "    print(f\"Context: '{example['context']}'\")\n",
    "    print(f\"Target: '{example['target']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "\n",
    "print(\"\\nExample of combined DFM data:\")\n",
    "if dfm_train_data:\n",
    "    example = dfm_train_data[0]\n",
    "    print(f\"Context: '{example['nlp_context']}'\")\n",
    "    print(f\"Target: '{example['target_char']}'\")\n",
    "    print(f\"From sentence {example['sentence_idx']}: '{sentences[example['sentence_idx']]}'\")\n",
    "    print(f\"Feature shape: {example['ecd_feature'].shape}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-13T09:39:41.517456400Z",
     "start_time": "2025-05-13T09:39:35.985419700Z"
    }
   },
   "id": "af5439f909b4474d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
