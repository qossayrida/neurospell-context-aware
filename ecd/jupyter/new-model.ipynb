{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-15T11:29:09.819943800Z",
     "start_time": "2025-05-15T11:29:01.374931200Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 85 is out of bounds for axis 0 with size 85",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 323\u001B[0m\n\u001B[0;32m    320\u001B[0m train_file_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m../data/Contributor_I_Train.mat\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    322\u001B[0m \u001B[38;5;66;03m# Create dataset\u001B[39;00m\n\u001B[1;32m--> 323\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mcreate_sentence_eeg_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_file_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset)\n\u001B[0;32m    326\u001B[0m \u001B[38;5;66;03m# Train model\u001B[39;00m\n\u001B[0;32m    327\u001B[0m \u001B[38;5;66;03m# model = train_sequential_ecd(dataset, epochs=10, batch_size=16, seq_length=5)\u001B[39;00m\n\u001B[0;32m    328\u001B[0m \n\u001B[0;32m    329\u001B[0m \u001B[38;5;66;03m# Save model\u001B[39;00m\n\u001B[0;32m    330\u001B[0m \u001B[38;5;66;03m# torch.save(model.state_dict(), \"../model/sequential_ecd_model.pth\")\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[1], line 223\u001B[0m, in \u001B[0;36mcreate_sentence_eeg_dataset\u001B[1;34m(train_file_path, test_file_path)\u001B[0m\n\u001B[0;32m    220\u001B[0m sentence_map, sentences \u001B[38;5;241m=\u001B[39m create_sentence_mapping(word_train, trials_train)\n\u001B[0;32m    222\u001B[0m \u001B[38;5;66;03m# Extract features with sentence context\u001B[39;00m\n\u001B[1;32m--> 223\u001B[0m features, labels, sentence_indices, char_indices \u001B[38;5;241m=\u001B[39m \u001B[43mextract_sentence_features\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    224\u001B[0m \u001B[43m    \u001B[49m\u001B[43msignals_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflashing_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstimulus_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msentences\u001B[49m\n\u001B[0;32m    225\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    227\u001B[0m \u001B[38;5;66;03m# Normalize features\u001B[39;00m\n\u001B[0;32m    228\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(features)):\n",
      "Cell \u001B[1;32mIn[1], line 71\u001B[0m, in \u001B[0;36mextract_sentence_features\u001B[1;34m(signals, flashing, stimulus_type, sentence_trials, window_duration_ms, sampling_rate)\u001B[0m\n\u001B[0;32m     68\u001B[0m trial_features \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m     69\u001B[0m trial_labels \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(\u001B[43mflashing\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m]\u001B[49m)):\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m (sample \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m (flashing[trial, sample\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m flashing[trial, sample] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     73\u001B[0m         \u001B[38;5;66;03m# Extract window following flash onset\u001B[39;00m\n\u001B[0;32m     74\u001B[0m         lower_sample \u001B[38;5;241m=\u001B[39m sample\n",
      "\u001B[1;31mIndexError\u001B[0m: index 85 is out of bounds for axis 0 with size 85"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load and preprocess the EEG data\n",
    "def load_eeg_data(file_path):\n",
    "    data = loadmat(file_path)\n",
    "    signals = data['Signal']\n",
    "    flashing = data['Flashing']\n",
    "    stimulus_type = data['StimulusType']\n",
    "    target_chars = data['TargetChar']\n",
    "    \n",
    "    return signals, flashing, stimulus_type, target_chars\n",
    "\n",
    "# Create sentence mapping - associate each trial with its correct character\n",
    "def create_sentence_mapping(target_chars, trials):\n",
    "    sentence_map = {}\n",
    "    for i in range(trials):\n",
    "        char = ''.join([c for c in target_chars[0][i] if c.strip()])\n",
    "        sentence_map[i] = char\n",
    "    \n",
    "    # Group trials by original sentences\n",
    "    sentences = defaultdict(list)\n",
    "    current_sentence = []\n",
    "    current_word = \"\"\n",
    "    \n",
    "    for i in range(trials):\n",
    "        char = sentence_map[i]\n",
    "        if char == '_':  # Space character\n",
    "            if current_word:\n",
    "                current_sentence.append(current_word)\n",
    "                current_word = \"\"\n",
    "        elif char in ['.', '!', '?']:  # End of sentence\n",
    "            if current_word:\n",
    "                current_sentence.append(current_word)\n",
    "                current_word = \"\"\n",
    "            if current_sentence:\n",
    "                sentence_key = ' '.join(current_sentence)\n",
    "                sentences[sentence_key].extend(list(range(i-len(sentence_key)+1, i+1)))\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            current_word += char\n",
    "    \n",
    "    # Handle last sentence if exists\n",
    "    if current_word:\n",
    "        current_sentence.append(current_word)\n",
    "    if current_sentence:\n",
    "        sentence_key = ' '.join(current_sentence)\n",
    "        sentences[sentence_key].extend(list(range(trials-len(sentence_key)+1, trials+1)))\n",
    "    \n",
    "    return sentence_map, sentences\n",
    "\n",
    "# Extract features from EEG signals for each character in the sentence context\n",
    "def extract_sentence_features(signals, flashing, stimulus_type, sentence_trials, window_duration_ms=650, sampling_rate=120):\n",
    "    window_samples = round(sampling_rate * (window_duration_ms / 1000))\n",
    "    features = []\n",
    "    labels = []\n",
    "    sentence_indices = []\n",
    "    char_indices = []\n",
    "    \n",
    "    for sentence_idx, trials in enumerate(sentence_trials.values()):\n",
    "        for char_idx, trial in enumerate(trials):\n",
    "            trial_features = []\n",
    "            trial_labels = []\n",
    "            \n",
    "            for sample in range(len(flashing[trial])):\n",
    "                if (sample == 0) or (flashing[trial, sample-1] == 0 and flashing[trial, sample] == 1):\n",
    "                    # Extract window following flash onset\n",
    "                    lower_sample = sample\n",
    "                    upper_sample = min(sample + window_samples, len(flashing[trial]))\n",
    "                    window = signals[trial, lower_sample:upper_sample, :]\n",
    "                    \n",
    "                    # Add to feature list\n",
    "                    trial_features.append(window)\n",
    "                    \n",
    "                    # Add label (1 for P300, 0 for no P300)\n",
    "                    if stimulus_type[trial, sample] == 1:\n",
    "                        trial_labels.append(1)\n",
    "                    else:\n",
    "                        trial_labels.append(0)\n",
    "            \n",
    "            features.extend(trial_features)\n",
    "            labels.extend(trial_labels)\n",
    "            sentence_indices.extend([sentence_idx] * len(trial_features))\n",
    "            char_indices.extend([char_idx] * len(trial_features))\n",
    "    \n",
    "    return np.array(features), np.array(labels), np.array(sentence_indices), np.array(char_indices)\n",
    "\n",
    "# Create sequential batches for training\n",
    "def create_sequential_batches(features, labels, sentence_indices, char_indices, seq_length=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Create batches with sequential character information\n",
    "    seq_length: number of characters to include in each sequence\n",
    "    \"\"\"\n",
    "    unique_sentences = np.unique(sentence_indices)\n",
    "    X_batches = []\n",
    "    y_batches = []\n",
    "    \n",
    "    for sent_idx in unique_sentences:\n",
    "        sent_mask = (sentence_indices == sent_idx)\n",
    "        sent_features = features[sent_mask]\n",
    "        sent_labels = labels[sent_mask]\n",
    "        sent_char_indices = char_indices[sent_mask]\n",
    "        \n",
    "        # Group by character\n",
    "        unique_chars = np.unique(sent_char_indices)\n",
    "        char_data = []\n",
    "        \n",
    "        for char_idx in unique_chars:\n",
    "            char_mask = (sent_char_indices == char_idx)\n",
    "            char_features = sent_features[char_mask]\n",
    "            char_labels = sent_labels[char_mask]\n",
    "            char_data.append((char_features, char_labels))\n",
    "        \n",
    "        # Create sequences\n",
    "        for i in range(len(char_data) - seq_length + 1):\n",
    "            seq_features = []\n",
    "            seq_labels = []\n",
    "            \n",
    "            for j in range(seq_length):\n",
    "                seq_features.append(char_data[i+j][0])\n",
    "                seq_labels.append(char_data[i+j][1])\n",
    "            \n",
    "            # Add to batches\n",
    "            X_batches.append(seq_features)\n",
    "            y_batches.append(seq_labels)\n",
    "    \n",
    "    # Convert to appropriate format and create mini-batches\n",
    "    X_batches = np.array(X_batches)\n",
    "    y_batches = np.array(y_batches)\n",
    "    \n",
    "    # Shuffle and create mini-batches\n",
    "    indices = np.arange(len(X_batches))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for i in range(0, len(indices), batch_size):\n",
    "        batch_indices = indices[i:i+batch_size]\n",
    "        yield X_batches[batch_indices], y_batches[batch_indices]\n",
    "\n",
    "# Define a sequence-based ECD model\n",
    "class SequentialECDModel(nn.Module):\n",
    "    def __init__(self, input_channels=64, seq_length=5, hidden_dim=128):\n",
    "        super(SequentialECDModel, self).__init__()\n",
    "        \n",
    "        # CNN layers for spatial feature extraction\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, input_channels), padding=(0, 0))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(5, 1), padding=(2, 0))\n",
    "        \n",
    "        # Calculate flattened feature size\n",
    "        self.feature_size = self._calculate_conv_output_size()\n",
    "        \n",
    "        # LSTM for sequence modeling\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_size,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc1 = nn.Linear(hidden_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def _calculate_conv_output_size(self):\n",
    "        # This function calculates the output size after convolution\n",
    "        # Placeholder value - should be calculated based on actual dimensions\n",
    "        return 2048\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: [batch_size, seq_length, time_points, channels]\n",
    "        batch_size, seq_length = x.shape[0], x.shape[1]\n",
    "        \n",
    "        # Process each sequence element independently\n",
    "        sequence_outputs = []\n",
    "        \n",
    "        for i in range(seq_length):\n",
    "            # Get current sequence element\n",
    "            x_i = x[:, i]  # [batch_size, time_points, channels]\n",
    "            \n",
    "            # Reshape for CNN\n",
    "            x_i = x_i.unsqueeze(1)  # Add channel dimension [batch_size, 1, time_points, channels]\n",
    "            \n",
    "            # Apply CNNs\n",
    "            x_i = F.relu(self.conv1(x_i))\n",
    "            x_i = F.max_pool2d(x_i, kernel_size=(2, 1))\n",
    "            x_i = F.relu(self.conv2(x_i))\n",
    "            x_i = F.max_pool2d(x_i, kernel_size=(2, 1))\n",
    "            \n",
    "            # Flatten\n",
    "            x_i = x_i.view(batch_size, -1)\n",
    "            sequence_outputs.append(x_i)\n",
    "        \n",
    "        # Stack sequence outputs\n",
    "        sequence = torch.stack(sequence_outputs, dim=1)  # [batch_size, seq_length, feature_size]\n",
    "        \n",
    "        # Apply LSTM\n",
    "        lstm_out, _ = self.lstm(sequence)\n",
    "        \n",
    "        # Use final output for classification\n",
    "        x = F.relu(self.fc1(lstm_out[:, -1, :]))\n",
    "        x = self.fc2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        \n",
    "        return x.squeeze(-1)\n",
    "\n",
    "# Main function to create sentence-based EEG dataset and train model\n",
    "def create_sentence_eeg_dataset(train_file_path, test_file_path=None):\n",
    "    # Load training data\n",
    "    signals_train, flashing_train, stimulus_train, word_train = load_eeg_data(train_file_path)\n",
    "    trials_train = len(word_train[0])\n",
    "    \n",
    "    # Create sentence mapping\n",
    "    sentence_map, sentences = create_sentence_mapping(word_train, trials_train)\n",
    "    \n",
    "    # Extract features with sentence context\n",
    "    features, labels, sentence_indices, char_indices = extract_sentence_features(\n",
    "        signals_train, flashing_train, stimulus_train, sentences\n",
    "    )\n",
    "    \n",
    "    # Normalize features\n",
    "    for i in range(len(features)):\n",
    "        features[i] = (features[i] - np.mean(features[i], axis=0)) / (np.std(features[i], axis=0) + 1e-8)\n",
    "    \n",
    "    # Train-validation split\n",
    "    X_train, X_val, y_train, y_val, sent_idx_train, sent_idx_val, char_idx_train, char_idx_val = train_test_split(\n",
    "        features, labels, sentence_indices, char_indices, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Features shape: {features.shape}\")\n",
    "    print(f\"Training samples: {len(X_train)}, Validation samples: {len(X_val)}\")\n",
    "    print(f\"Number of sentences: {len(sentences)}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train,\n",
    "        'sent_idx_train': sent_idx_train,\n",
    "        'char_idx_train': char_idx_train,\n",
    "        'X_val': X_val,\n",
    "        'y_val': y_val,\n",
    "        'sent_idx_val': sent_idx_val,\n",
    "        'char_idx_val': char_idx_val,\n",
    "        'sentences': sentences,\n",
    "        'sentence_map': sentence_map\n",
    "    }\n",
    "\n",
    "# Training loop\n",
    "def train_sequential_ecd(dataset, epochs=10, batch_size=32, seq_length=5, learning_rate=0.001):\n",
    "    # Initialize model\n",
    "    model = SequentialECDModel(input_channels=64, seq_length=seq_length)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Create sequential batches\n",
    "        batch_generator = create_sequential_batches(\n",
    "            dataset['X_train'], dataset['y_train'], \n",
    "            dataset['sent_idx_train'], dataset['char_idx_train'],\n",
    "            seq_length=seq_length, batch_size=batch_size\n",
    "        )\n",
    "        \n",
    "        for X_batch, y_batch in batch_generator:\n",
    "            # Convert to PyTorch tensors\n",
    "            X_batch = torch.tensor(X_batch, dtype=torch.float32)\n",
    "            y_batch = torch.tensor(y_batch, dtype=torch.float32)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            batch_count += 1\n",
    "        \n",
    "        # Print epoch results\n",
    "        avg_loss = total_loss / batch_count\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_batch_generator = create_sequential_batches(\n",
    "                dataset['X_val'], dataset['y_val'],\n",
    "                dataset['sent_idx_val'], dataset['char_idx_val'],\n",
    "                seq_length=seq_length, batch_size=batch_size\n",
    "            )\n",
    "            \n",
    "            val_loss = 0\n",
    "            val_count = 0\n",
    "            for X_val, y_val in val_batch_generator:\n",
    "                X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "                y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "                \n",
    "                outputs = model(X_val)\n",
    "                val_loss += criterion(outputs, y_val).item()\n",
    "                val_count += 1\n",
    "            \n",
    "            avg_val_loss = val_loss / val_count\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Set paths\n",
    "train_file_path = '../data/Contributor_I_Train.mat'\n",
    "\n",
    "# Create dataset\n",
    "dataset = create_sentence_eeg_dataset(train_file_path)\n",
    "\n",
    "print(dataset)\n",
    "# Train model\n",
    "# model = train_sequential_ecd(dataset, epochs=10, batch_size=16, seq_length=5)\n",
    "\n",
    "# Save model\n",
    "# torch.save(model.state_dict(), \"../model/sequential_ecd_model.pth\")\n",
    "print(\"Model saved successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
