{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "MCNN1.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YvvoQrh5C9Lj",
    "colab_type": "text"
   },
   "source": [
    "# MCNN1 SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7LLYRPYzCq7N",
    "colab_type": "code",
    "outputId": "8310c7f5-18bd-48f8-cc3b-c9c200465509",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426686709,
     "user_tz": -60,
     "elapsed": 30397,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:06.650423100Z",
     "start_time": "2024-12-30T22:08:02.184687800Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np     \n",
    "import random           \n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import warnings\n",
    "import string\n",
    "import os\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import mne\n",
    "\n",
    "contributor_selected = \"I\"                                  \n",
    "\n",
    "contributor_train_file_path = '../data/Contributor_' + contributor_selected + '_Train.mat'\n",
    "contributor_test_file_path = '../data/Contributor_' + contributor_selected + '_Test.mat'\n",
    "channel_name_file_path = '../data/channels.csv'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Channel selection\n",
    "CHANNELS = [i for i in range(64)]"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iv09xtd1QpF8",
    "colab_type": "text"
   },
   "source": [
    "# Training set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signals from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 64)**;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wKXUKaedCvJh",
    "colab_type": "code",
    "outputId": "8553aab4-93dc-435f-c722-805d89f01a55",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426688221,
     "user_tz": -60,
     "elapsed": 31892,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:06.784464Z",
     "start_time": "2024-12-30T22:08:06.655427900Z"
    }
   },
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "from bundle.DataCraft import * \n",
    "\n",
    "data_train = loadmat(contributor_train_file_path)\n",
    "\n",
    "signals_train = data_train['Signal']\n",
    "flashing_train = data_train['Flashing']\n",
    "stimulus_train = data_train['StimulusType']\n",
    "word_train = data_train['TargetChar']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_train = (len(signals_train)) * (len(signals_train[0])) / (sampling_frequency * 60)\n",
    "trials_train = len(word_train[0])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print_data(signals_train, word_train, contributor_selected, sampling_frequency)"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "Contributor     Sampling Freq. (Hz)  Recording (min)      Trials     Spelled Word                  \n",
      "==============================================================================================================\n",
      "I               240.00               46.01                85         EAEVQTDOJG8RBRGONCEDHCTUIDBPUH\n",
      "                                                                     MEM6OUXOCFOUKWA4VJEFRZROLHYNQD\n",
      "                                                                     W_EKTLBWXEPOUIKZERYOOTHQI     \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UzlOaTiJCzjo",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:10.133769800Z",
     "start_time": "2024-12-30T22:08:06.785464600Z"
    }
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "DOWNSAMPLING_FREQUENCY = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / DOWNSAMPLING_FREQUENCY)\n",
    "sampling_frequency = DOWNSAMPLING_FREQUENCY\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_train[0])))\n",
    "\n",
    "signals_train = signals_train[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_train = flashing_train[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_train = stimulus_train[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_train[0])))"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Samples of EEG signals before downsampling: 7794\n",
      "# Samples of EEG signals after downsampling: 3897\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RhoMycKjC5m2",
    "colab_type": "code",
    "outputId": "6b9fae97-2b4b-45c0-c5a4-50517efbd1b6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426695610,
     "user_tz": -60,
     "elapsed": 39246,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:10.850838500Z",
     "start_time": "2024-12-30T22:08:10.133769800Z"
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = 64\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Convert list to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Tensor dimensions (SAMPLES, 78, 64)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features tensor shape: (15300, 78, 64)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eHjUdK0XFWEM",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:18.869929500Z",
     "start_time": "2024-12-30T22:08:10.849841700Z"
    }
   },
   "source": [
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Kr1Ofo4Fama",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:19.516199Z",
     "start_time": "2024-12-30T22:08:18.867930700Z"
    }
   },
   "source": [
    "# Divide data into positive and negative samples\n",
    "tableData = np.array([(train_features[i], train_labels[i]) for i in range(len(train_features))])\n",
    "np.random.shuffle(tableData)\n",
    "\n",
    "positiveTable = []\n",
    "negativeTable = []\n",
    "\n",
    "for sample in tableData:\n",
    "    if sample[1] == 1:\n",
    "        positiveTable.append(sample)\n",
    "    else:\n",
    "        negativeTable.append(sample)\n",
    "\n",
    "positiveTable = np.array(positiveTable)\n",
    "negativeTable = np.array(negativeTable)    "
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (15300, 2) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Divide data into positive and negative samples\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m tableData \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_features\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_labels\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mrange\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtrain_features\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      3\u001B[0m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mshuffle(tableData)\n\u001B[0;32m      5\u001B[0m positiveTable \u001B[38;5;241m=\u001B[39m []\n",
      "\u001B[1;31mValueError\u001B[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 2 dimensions. The detected shape was (15300, 2) + inhomogeneous part."
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kSVk31fVMiZ3",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:19.519200100Z",
     "start_time": "2024-12-30T22:08:19.517199200Z"
    }
   },
   "source": [
    "# Split negative set in 5 subsets\n",
    "np.random.shuffle(negativeTable)\n",
    "win_size = round(len(negativeTable)/5)\n",
    "start_index = 0\n",
    "\n",
    "for i in range(5):\n",
    "    if i==0:\n",
    "        train_features1 = negativeTable[start_index : start_index + win_size, 0]\n",
    "        train_labels1 = negativeTable[start_index : start_index + win_size, 1]    \n",
    "    elif i==1:\n",
    "        train_features2 = negativeTable[start_index : start_index + win_size, 0]\n",
    "        train_labels2 = negativeTable[start_index : start_index + win_size, 1]\n",
    "    elif i==2:\n",
    "        train_features3 = negativeTable[start_index : start_index + win_size, 0]\n",
    "        train_labels3 = negativeTable[start_index : start_index + win_size, 1]    \n",
    "    elif i==3:\n",
    "        train_features4 = negativeTable[start_index : start_index + win_size, 0]\n",
    "        train_labels4 = negativeTable[start_index : start_index + win_size, 1]    \n",
    "    elif i==4:\n",
    "        train_features5 = negativeTable[start_index : , 0]\n",
    "        train_labels5 = negativeTable[start_index : , 1]    \n",
    "    start_index += win_size\n",
    "\n",
    "# Reshape samples to be a 3D tensor (n_samples, 78, 64)\n",
    "train_features1 = np.stack(train_features1, axis=0)\n",
    "train_features2 = np.stack(train_features2, axis=0)\n",
    "train_features3 = np.stack(train_features3, axis=0)\n",
    "train_features4 = np.stack(train_features4, axis=0)\n",
    "train_features5 = np.stack(train_features5, axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "V9HC9yfeHJdc",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.518199400Z"
    }
   },
   "source": [
    "# Obtain positive samples to concatenate\n",
    "pos_features = []\n",
    "pos_labels = []\n",
    "for sample in positiveTable:\n",
    "    pos_features.append(sample[0])\n",
    "    pos_labels.append(sample[1])\n",
    "pos_features = np.array(pos_features)\n",
    "\n",
    "# Concatenate positive samples and negative samples\n",
    "train_features1 = np.concatenate((pos_features, train_features1), axis=0)\n",
    "train_features2 = np.concatenate((pos_features, train_features2), axis=0)\n",
    "train_features3 = np.concatenate((pos_features, train_features3), axis=0)\n",
    "train_features4 = np.concatenate((pos_features, train_features4), axis=0)\n",
    "train_features5 = np.concatenate((pos_features, train_features5), axis=0)\n",
    "\n",
    "train_labels1 = np.concatenate((pos_labels, train_labels1), axis=0)\n",
    "train_labels2 = np.concatenate((pos_labels, train_labels2), axis=0)\n",
    "train_labels3 = np.concatenate((pos_labels, train_labels3), axis=0)\n",
    "train_labels4 = np.concatenate((pos_labels, train_labels4), axis=0)\n",
    "train_labels5 = np.concatenate((pos_labels, train_labels5), axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFV-Yc7qR7as",
    "colab_type": "text"
   },
   "source": [
    "# Testing set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signas from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 64)**;\n",
    "6.   Calculate weights vector to balance samples importance and obtain correct accuracy estimation;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TxUAAM76QtPK",
    "colab_type": "code",
    "outputId": "1689847d-4109-435f-b558-96590061b4ae",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426705760,
     "user_tz": -60,
     "elapsed": 49368,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:19.540199100Z",
     "start_time": "2024-12-30T22:08:19.521199400Z"
    }
   },
   "source": [
    "data_test = loadmat(contributor_test_file_path)\n",
    "\n",
    "\n",
    "signals_test = data_test['Signal']\n",
    "flashing_test = data_test['Flashing']\n",
    "word_test =  data_test['TargetChar']\n",
    "stimulus_code_test = data_test['StimulusCode']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_test = (len(signals_test)) * (len(signals_test[0])) / (sampling_frequency * 60)\n",
    "trials_test = len(word_test[0])\n",
    "samples_per_trial_test = len(signals_test[0])\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print_data(signals_test, word_test, contributor_selected, sampling_frequency)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bEWa2QFMQuKG",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.526199100Z"
    }
   },
   "source": [
    "# Create characters matrix\n",
    "char_matrix = [[0 for j in range(6)] for i in range(6)]\n",
    "s = string.ascii_uppercase + '1' + '2' + '3' + '4' + '5' + '6' + '7' + '8' + '9' + '_'\n",
    "\n",
    "# Append cols and rows in a list\n",
    "list_matrix = []\n",
    "for i in range(6):\n",
    "    col = [s[j] for j in range(i, 36, 6)]\n",
    "    list_matrix.append(col)\n",
    "for i in range(6):\n",
    "    row = [s[j] for j in range(i * 6, i * 6 + 6)]\n",
    "    list_matrix.append(row)\n",
    "\n",
    "# Create StimulusType array for the test set (missing from the given database)\n",
    "stimulus_test = [[0 for j in range(samples_per_trial_test)] for i in range(trials_test)]\n",
    "stimulus_test = np.array(stimulus_test)\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    counter=0\n",
    "    for sample in range(samples_per_trial_test):\n",
    "        index = int(stimulus_code_test[trial, sample]) - 1\n",
    "        if not index == -1:\n",
    "            if word_test[0][trial] in list_matrix[index]:\n",
    "                stimulus_test[trial, sample] = 1\n",
    "            else:\n",
    "                stimulus_test[trial, sample] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9HxcU7sRQw-R",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.526199100Z"
    }
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_test[trial, :, :] = signal.filtfilt(b, a, signals_test[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "DOWNSAMPLING_FREQUENCY = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / DOWNSAMPLING_FREQUENCY)\n",
    "sampling_frequency = DOWNSAMPLING_FREQUENCY\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_test[0])))\n",
    "\n",
    "signals_test = signals_test[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_test = flashing_test[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_test = stimulus_test[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_test[0])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "poZhBVnzQ1T4",
    "colab_type": "code",
    "outputId": "0624df9d-a42d-4fd0-e01d-1e8adb3cfbae",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426710840,
     "user_tz": -60,
     "elapsed": 54415,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.532199400Z"
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = 64\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "samples_per_trial_test = len(signals_train[0])\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    for sample in (range(samples_per_trial_test)):\n",
    "        if (sample == 0) or (flashing_test[trial, sample-1] == 0 and flashing_test[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_test[trial, lower_sample:upper_sample, :]\n",
    "            # Features extraction\n",
    "            test_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_test[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                test_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                test_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get test weights to take into account the number of classes \n",
    "test_weights = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 1:\n",
    "        test_weights.append(len(test_labels)/count_positive)\n",
    "    else:\n",
    "        test_weights.append(len(test_labels)/count_negative)\n",
    "test_weights = np.array(test_weights)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# 3D tensor (SAMPLES, 78, 64)\n",
    "dim_test = test_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_test))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(test_features)):\n",
    "    test_features[pattern] = scale(test_features[pattern], axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hO1AQqNeST_p",
    "colab_type": "text"
   },
   "source": [
    "# MCNN1 model definition, training and testing\n",
    "\n",
    "1.   Function definiton for randomization of weights and biases;\n",
    "2.   **Scaled_tanh(x)** activation function definition;\n",
    "3.   ANN model definition (2 Conv1D layers, 2 dense layers);\n",
    "4.   Training of 5 identical nets over 5 different datasets obtained as such:\n",
    "\n",
    "\n",
    "\n",
    ">*   Split negative dataset (containing only no-P300) into 5 subsets randomly;\n",
    ">*   Compose 5 datasets with all the positive samples plus one of the negative subsets;\n",
    ">*   Train each CNN1 over one of the balanced subset;\n",
    ">*   Predictions are obtained by calculating the mean of the outputs of the nets;\n",
    "\n",
    "\n",
    "\n",
    "5.   Mean of all networks' outputs for prediction assignment; \n",
    "6.   MCNN1 performance assessment;\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYvMhLt1w0dK",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.533198200Z"
    }
   },
   "source": [
    "# Randomizing function for bias and weights of the network\n",
    "def cecotti_normal(shape, dtype = None, partition_info = None):\n",
    "    if len(shape) == 1:\n",
    "        fan_in = shape[0]\n",
    "    elif len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        receptive_field_size = 1\n",
    "        for dim in shape[:-2]:\n",
    "            receptive_field_size *= dim\n",
    "        fan_in = shape[-2] * receptive_field_size\n",
    "    return K.random_normal(shape, mean = 0.0, stddev = (1.0 / fan_in))\n",
    "\n",
    "# Custom tanh activation function\n",
    "def scaled_tanh(x):\n",
    "    return 1.7159 * K.tanh((2.0 / 3.0) * x)\n",
    "\n",
    "# Build the model\n",
    "def CNN1_model(channels=64, filters=10):\n",
    "    model = Sequential([\n",
    "        Conv1D(\n",
    "            filters = filters,\n",
    "            kernel_size = 1,\n",
    "            padding = \"same\",\n",
    "            bias_initializer = cecotti_normal,\n",
    "            kernel_initializer = cecotti_normal,\n",
    "            use_bias = True,\n",
    "            activation = scaled_tanh,\n",
    "            input_shape = (78, channels)\n",
    "        ),\n",
    "        Conv1D(\n",
    "            filters = 50,\n",
    "            kernel_size = 13,\n",
    "            padding = \"valid\",\n",
    "            strides = 11,\n",
    "            bias_initializer = cecotti_normal,\n",
    "            kernel_initializer = cecotti_normal,\n",
    "            use_bias = True,\n",
    "            activation = scaled_tanh,\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"sigmoid\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer = 'adam', loss = 'mean_squared_error', metrics = ['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 200\n",
    "VALID_SPLIT = 0.05\n",
    "SHUFFLE = 1 # set to 1 to shuffle subsets during training"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dTeIypusyJg_",
    "colab_type": "code",
    "outputId": "e9c7f105-9b3d-4db7-c9d5-8f18cc93a163",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426745600,
     "user_tz": -60,
     "elapsed": 89138,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.535198200Z"
    }
   },
   "source": [
    "model1 = CNN1_model(channels=64, filters=10)\n",
    "\n",
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          mode = 'min', \n",
    "                          patience = 50, \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history1 = model1.fit(x=train_features1, \n",
    "                     y=train_labels1, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS, \n",
    "                     validation_split=VALID_SPLIT, \n",
    "                     callbacks=[earlystop],\n",
    "                     shuffle=SHUFFLE,\n",
    "                     verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kOXU8xX3ybpM",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.538201300Z"
    }
   },
   "source": [
    "model2 = CNN1_model(channels=64, filters=10)\n",
    "\n",
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          mode = 'min', \n",
    "                          patience = 50, \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history2 = model2.fit(x=train_features2, \n",
    "                     y=train_labels2, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS, \n",
    "                     validation_split=VALID_SPLIT, \n",
    "                     callbacks=[earlystop],\n",
    "                     shuffle=SHUFFLE,\n",
    "                     verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O7bnZFQcyeJD",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "end_time": "2024-12-30T22:08:19.585525Z",
     "start_time": "2024-12-30T22:08:19.541199800Z"
    }
   },
   "source": [
    "# Third CNN1 model definition\n",
    "model3 = CNN1_model(channels=64, filters=10)\n",
    "\n",
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          mode = 'min', \n",
    "                          patience = 50, \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history3 = model3.fit(x=train_features3, \n",
    "                     y=train_labels3, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS, \n",
    "                     validation_split=VALID_SPLIT, \n",
    "                     callbacks=[earlystop],\n",
    "                     shuffle=SHUFFLE,\n",
    "                     verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kKp2fhIdyeYW",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.542200700Z"
    }
   },
   "source": [
    "# Fourth CNN1 model definition\n",
    "model4 = CNN1_model(channels=64, filters=10)\n",
    "\n",
    "\n",
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          mode = 'min', \n",
    "                          patience = 50, \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history4 = model4.fit(x=train_features4, \n",
    "                     y=train_labels4, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS, \n",
    "                     validation_split=VALID_SPLIT, \n",
    "                     callbacks=[earlystop],\n",
    "                     shuffle=SHUFFLE,\n",
    "                     verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UjvPW-l_yejy",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.545524500Z"
    }
   },
   "source": [
    "# Fifth CNN1 model definition\n",
    "model5 = CNN1_model(channels=64, filters=10)\n",
    "\n",
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          mode = 'min', \n",
    "                          patience = 50, \n",
    "                          restore_best_weights = True)\n",
    "\n",
    "# Callback to keep track of model statistics\n",
    "history5 = model5.fit(x=train_features5, \n",
    "                     y=train_labels5, \n",
    "                     batch_size=BATCH_SIZE, \n",
    "                     epochs=EPOCHS, \n",
    "                     validation_split=VALID_SPLIT, \n",
    "                     callbacks=[earlystop],\n",
    "                     shuffle=SHUFFLE,\n",
    "                     verbose=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GHfDGgAxIYLA",
    "colab_type": "code",
    "outputId": "40d9d9b2-8146-431d-886b-372f5021b810",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426798369,
     "user_tz": -60,
     "elapsed": 141881,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.546524900Z"
    }
   },
   "source": [
    "# Prepare figure\n",
    "plt.figure(figsize=(20,8))\n",
    "plt.subplots_adjust(hspace=0.3)\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "plt.subplot(2,3,1)\n",
    "plt.plot(history1.history['acc'], label='training loss')\n",
    "plt.plot(history1.history['val_acc'], label='validation loss')\n",
    "# Set up labels and titles\n",
    "plt.title('Model 1')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(2,3,2)\n",
    "plt.plot(history2.history['acc'], label='training loss')\n",
    "plt.plot(history2.history['val_acc'], label='validation loss')\n",
    "# Set up labels and titles\n",
    "plt.title('Model 2')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(2,3,3)\n",
    "plt.plot(history3.history['acc'], label='training loss')\n",
    "plt.plot(history3.history['val_acc'], label='validation loss')\n",
    "# Set up labels and titles\n",
    "plt.title('Model 3')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(2,3,4)\n",
    "plt.plot(history4.history['acc'], label='training loss')\n",
    "plt.plot(history4.history['val_acc'], label='validation loss')\n",
    "# Set up labels and titles\n",
    "plt.title('Model 4')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.subplot(2,3,5)\n",
    "plt.plot(history5.history['acc'], label='training loss')\n",
    "plt.plot(history5.history['val_acc'], label='validation loss')\n",
    "# Set up labels and titles\n",
    "plt.title('Model 5')\n",
    "plt.xlabel('# Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TSAwDaBZUI4X",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.548524400Z"
    }
   },
   "source": [
    "# Multiclassifier prediction function\n",
    "def majorityPrediction(models, inputs):\n",
    "    pred = []\n",
    "    # Get models predictions\n",
    "    for model in models:\n",
    "        pred.append(model.predict(inputs))\n",
    "    pred = np.array(pred)\n",
    "    return pred.mean(axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jqpblTxmoekF",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.549525600Z"
    }
   },
   "source": [
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history1.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history1.history['val_loss'][best_epoch]}\")\n",
    "best_weights = model1.get_weights()\n",
    "model1.set_weights(best_weights)\n",
    "best_model1 = CNN1_model(channels=64, filters=10)\n",
    "best_model1.set_weights(best_weights)\n",
    "best_model1.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history2.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history2.history['val_loss'][best_epoch]}\")\n",
    "best_weights = model2.get_weights()\n",
    "model2.set_weights(best_weights)\n",
    "best_model2 = CNN1_model(channels=64, filters=10)\n",
    "best_model2.set_weights(best_weights)\n",
    "best_model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history3.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history3.history['val_loss'][best_epoch]}\")\n",
    "best_weights = model3.get_weights()\n",
    "model3.set_weights(best_weights)\n",
    "best_model3 = CNN1_model(channels=64, filters=10)\n",
    "best_model3.set_weights(best_weights)\n",
    "best_model3.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history4.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history4.history['val_loss'][best_epoch]}\")\n",
    "best_weights = model4.get_weights()\n",
    "model4.set_weights(best_weights)\n",
    "best_model4 = CNN1_model(channels=64, filters=10)\n",
    "best_model4.set_weights(best_weights)\n",
    "best_model4.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history5.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history5.history['val_loss'][best_epoch]}\")\n",
    "best_weights = model5.get_weights()\n",
    "model5.set_weights(best_weights)\n",
    "best_model5 = CNN1_model(channels=64, filters=10)\n",
    "best_model5.set_weights(best_weights)\n",
    "best_model5.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "\n",
    "# Create dictionary to store models\n",
    "model_dict = {1:best_model1, 2:best_model2, 3:best_model3, 4:best_model4, 5:best_model5}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "11_xN8k3Vqp4",
    "colab_type": "code",
    "outputId": "95344489-4fae-4c38-faf1-9db6492852de",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426812730,
     "user_tz": -60,
     "elapsed": 156229,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 522
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.551525400Z"
    }
   },
   "source": [
    "\n",
    "xycoord = []\n",
    "# Read file content\n",
    "with open(\"../data/coordinates.csv\", \"r\") as f:\n",
    "    file_content = f.read()\n",
    "    # Loop over all rows\n",
    "    for row in file_content.split(\"\\n\"): \n",
    "    # Skip missing rows\n",
    "        if row == '':\n",
    "            continue\n",
    "        xycoord.append(row)\n",
    "\n",
    "# Create a list with x,y coordinates of each electrodes well formatted\n",
    "coord = []\n",
    "for x in xycoord:\n",
    "    coord.append(x.split(','))\n",
    "coord = np.array(coord)\n",
    "\n",
    "# Flip y axis in order to plot in the right way the electrodes positions\n",
    "for i in range(len(coord)):\n",
    "    coord[i][1] = 681 -int(coord[i][1])\n",
    "\n",
    "# Create a list of weights for all filters\n",
    "nf = []\n",
    "for net in range(5):\n",
    "    for filt in range(10):\n",
    "        nf.append(abs(np.array(model_dict[net+1].get_weights()[0])[0, :, filt]))\n",
    "nf = np.array(nf, dtype=float)\n",
    "\n",
    "# Plot topoplot of the filters in the first convolutional layer\n",
    "fig = plt.figure(figsize=(22,8))\n",
    "for tot in range(50):\n",
    "    ax = fig.add_subplot(5, 10, tot+1)\n",
    "    ax.set_title(\"Net {} #{}\".format((tot//10)+1, (tot)%10+1))\n",
    "    mne.viz.plot_topomap(nf[tot], coord, cmap='Spectral_r', axes=ax, show=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K9uvScAnYbG1",
    "colab_type": "code",
    "outputId": "f4c8083a-6540-4c59-9281-a02f42c12886",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426823716,
     "user_tz": -60,
     "elapsed": 167201,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.553524800Z"
    }
   },
   "source": [
    "# MCNN1 accuracy\n",
    "predictions = majorityPrediction(models=[model1, model2, model3, model4, model5], inputs=test_features).round()\n",
    "score = 0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == test_labels[i]:\n",
    "        score += 1\n",
    "\n",
    "# Single model evaluation\n",
    "print(\"CNN1_1 test set accuracy: {}%\".format(round(model1.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights)[1] * 100, 4)))\n",
    "print(\"CNN1_2 test set accuracy: {}%\".format(round(model2.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights)[1] * 100, 4)))\n",
    "print(\"CNN1_3 test set accuracy: {}%\".format(round(model3.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights)[1] * 100, 4)))\n",
    "print(\"CNN1_4 test set accuracy: {}%\".format(round(model4.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights)[1] * 100, 4)))\n",
    "print(\"CNN1_5 test set accuracy: {}%\".format(round(model5.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights)[1] * 100, 4)))\n",
    "\n",
    "# Multiclassifier evaluation\n",
    "print(\"-----------------------------------\")\n",
    "print(\"MCNN1  test set accuracy: {}%\".format(round(score/len(predictions) * 100, 4)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "K447gLheIDNR",
    "colab_type": "code",
    "outputId": "a64a25ac-f2b2-49db-9e0f-6ccdf658d103",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426824308,
     "user_tz": -60,
     "elapsed": 167782,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.555525500Z"
    }
   },
   "source": [
    "# Weighted confusion matrix (noP300: 80%, P300: 20%)\n",
    "data_train = confusion_matrix(y_true=test_labels, y_pred=predictions, sample_weight=test_weights)\n",
    "\n",
    "# Normalized confusion matrix (values in range 0-1)\n",
    "data_norm = data_train / np.full(data_train.shape, len(test_labels))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "df_cm = pd.DataFrame(data_norm, columns=np.unique(test_labels), index = np.unique(test_labels))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (6,5))\n",
    "sns.set(font_scale = 1.4)\n",
    "cm = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws = {\"size\": 16}, vmin=0, vmax=1)\n",
    "cm.axes.set_title(\"MCNN1 confusion matrix\\n\", fontsize=20)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hXgGDAqwrDpb",
    "colab_type": "code",
    "colab": {},
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.557525200Z"
    }
   },
   "source": [
    "# Model metrics (sens, spec, ppv, npv)\n",
    "def model_metrics(conf_matrix):\n",
    "    tn, fp, fn, tp = list(data_norm.flatten())\n",
    "    sens = round(tp/(tp+fn),4) # Sensitivity\n",
    "    spec = round(tn/(tn+fp),4) # Specificity\n",
    "    ppv = round(tp/(tp+fp),4) # Positive Predicted Value\n",
    "    npv = round(tn/(tn+fn),4) # Negative Predicted Value\n",
    "    return {\"Sensitivity\":sens, \"Specificity\":spec, \"PPV\":ppv, \"NPV\":npv}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "whUJeC0irD8X",
    "colab_type": "code",
    "outputId": "5b55fb82-8743-4753-caa9-c6e562dc0290",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576426824310,
     "user_tz": -60,
     "elapsed": 167774,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "ExecuteTime": {
     "start_time": "2024-12-30T22:08:19.558525600Z"
    }
   },
   "source": [
    "# Put model metrics into a table\n",
    "metrics = model_metrics(data_norm)\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(5,1))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Hide graph outlines\n",
    "for item in [fig, ax]:\n",
    "    item.patch.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Table definition\n",
    "table = ax.table(cellText=[list(metrics.values())], \n",
    "                     colLabels=list(metrics.keys()),\n",
    "                     loc=\"center\",\n",
    "                     cellLoc=\"center\",\n",
    "                     colColours=[\"c\"]*4)\n",
    "table.set_fontsize(16)\n",
    "table.scale(2,2)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
