{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "CNN1.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "svppQZlrES-H",
    "colab_type": "text"
   },
   "source": [
    "# CNN1 SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np                \n",
    "import warnings\n",
    "import string\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "\n",
    "\n",
    "contributor_selected = \"II\"                                 \n",
    "contributor_train_file_path = '../data/Contributor_' + contributor_selected + '_Train.mat'\n",
    "contributor_test_file_path = '../data/Contributor_' + contributor_selected + '_Test.mat'\n",
    "channel_name_file_path = '../data/channels.csv'\n",
    "channels = [i for i in range(64)]\n",
    "warnings.filterwarnings('ignore')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CVrJuHCMFCSl",
    "colab_type": "text"
   },
   "source": [
    "# Training set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signals from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 64)**;\n",
    "6.   Obtain **(noP300 / P300)** class ratio to balance out dataset during training;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "78aU0w6-Lp0y",
    "colab_type": "code",
    "outputId": "d1b22ce0-1858-4007-85a8-e169316ae6ef",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424445216,
     "user_tz": -60,
     "elapsed": 3901,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    }
   },
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "from bundle.DataCraft import * \n",
    "\n",
    "data_train = loadmat(contributor_train_file_path)\n",
    "signals_train = data_train['Signal']\n",
    "flashing_train = data_train['Flashing']\n",
    "stimulus_train = data_train['StimulusType']\n",
    "word_train = data_train['TargetChar']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_train = (len(signals_train)) * (len(signals_train[0])) / (sampling_frequency * 60)\n",
    "trials_train = len(word_train[0])\n",
    "\n",
    "print(\"Train Data:\")\n",
    "print_data(signals_train, word_train, contributor_selected, sampling_frequency)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IXuVC7puKcLt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_train):\n",
    "    signals_train[trial, :, :] = signal.filtfilt(b, a, signals_train[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_train[0])))\n",
    "\n",
    "signals_train = signals_train[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_train = flashing_train[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_train = stimulus_train[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_train[0])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bGaRKwu9MIvP",
    "colab_type": "code",
    "outputId": "12e9ec3d-fac7-46c8-d21c-64e302f8399e",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424452130,
     "user_tz": -60,
     "elapsed": 2050,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "SAMPLES_PER_TRIAL = len(signals_train[0])\n",
    "\n",
    "train_features = []\n",
    "train_labels = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_train):\n",
    "    for sample in (range(SAMPLES_PER_TRIAL)):\n",
    "        if (sample == 0) or (flashing_train[trial, sample - 1] == 0 and flashing_train[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_train[trial, lower_sample:upper_sample, :]                \n",
    "            # Features extraction\n",
    "            train_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_train[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                train_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                train_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get negative-positive classes ratio\n",
    "train_ratio = count_negative/count_positive\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "train_features = np.array(train_features)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# 3D Tensor shape (SAMPLES, 64, 78)\n",
    "dim_train = train_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_train))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(train_features)):\n",
    "    train_features[pattern] = scale(train_features[pattern], axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTVrfKzJH2aE",
    "colab_type": "text"
   },
   "source": [
    "# Testing set processing\n",
    "\n",
    "1.   Application of bandpass filter **(0.1-20Hz)**;\n",
    "2.   Down-sampling signas from 240Hz to 120Hz;\n",
    "3.   Obtain windows of 650ms at the start of every flashing (175ms)\n",
    "4.   Normalization of samples in each window: **Zi = (Xi - mu) / sigma**;\n",
    "5.   Reshape each window to be a 3D tensor with dimensions: **(N_SAMPLES, 78, 64)**;\n",
    "6.   Calculate weights vector to balance samples importance and obtain correct accuracy estimation;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Nm-bZZsKTliz",
    "colab_type": "code",
    "outputId": "06dcbd08-67c2-4826-de3d-99dc9b2a24d6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424466616,
     "user_tz": -60,
     "elapsed": 11382,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 134
    }
   },
   "source": [
    "from bundle.DataCraft import *  \n",
    "\n",
    "data_test = loadmat(contributor_test_file_path)\n",
    "signals_test = data_test['Signal']\n",
    "flashing_test = data_test['Flashing']\n",
    "word_test =  data_test['TargetChar']\n",
    "stimulus_code_test = data_test['StimulusCode']\n",
    "sampling_frequency = 240\n",
    "repetitions = 15\n",
    "recording_duration_test = (len(signals_test)) * (len(signals_test[0])) / (sampling_frequency * 60)\n",
    "trials_test = len(word_test[0])\n",
    "samples_per_trial_test = len(signals_test[0])\n",
    "\n",
    "print(\"Test Data:\")\n",
    "print_data(signals_test, word_test, contributor_selected, sampling_frequency)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SvCresQJm25A",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "char_matrix = [[0 for j in range(6)] for i in range(6)]\n",
    "s = string.ascii_uppercase + '1' + '2' + '3' + '4' + '5' + '6' + '7' + '8' + '9' + '_'\n",
    "\n",
    "# Append cols and rows in a list\n",
    "list_matrix = []\n",
    "for i in range(6):\n",
    "    col = [s[j] for j in range(i, 36, 6)]\n",
    "    list_matrix.append(col)\n",
    "for i in range(6):\n",
    "    row = [s[j] for j in range(i * 6, i * 6 + 6)]\n",
    "    list_matrix.append(row)\n",
    "\n",
    "# Create StimulusType array for the test set (missing from the given database)\n",
    "stimulus_test = [[0 for j in range(samples_per_trial_test)] for i in range(trials_test)]\n",
    "stimulus_test = np.array(stimulus_test)\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    counter=0\n",
    "    for sample in range(samples_per_trial_test):\n",
    "        index = int(stimulus_code_test[trial, sample]) - 1\n",
    "        if not index == -1:\n",
    "            if word_test[0][trial] in list_matrix[index]:\n",
    "                stimulus_test[trial, sample] = 1\n",
    "            else:\n",
    "                stimulus_test[trial, sample] = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UNy5oI8ooMAw",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Application of butterworth filter\n",
    "b, a = signal.butter(4, [0.1 / sampling_frequency, 20 / sampling_frequency], 'bandpass')\n",
    "for trial in range(trials_test):\n",
    "    signals_test[trial, :, :] = signal.filtfilt(b, a, signals_test[trial, :, :], axis=0)\n",
    "    \n",
    "# Down-sampling of the signals from 240Hz to 120Hz\n",
    "down_sampling_frequency = 120\n",
    "SCALE_FACTOR = round(sampling_frequency / down_sampling_frequency)\n",
    "sampling_frequency = down_sampling_frequency\n",
    "\n",
    "print(\"# Samples of EEG signals before downsampling: {}\".format(len(signals_test[0])))\n",
    "\n",
    "signals_test = signals_test[:, 0:-1:SCALE_FACTOR, :]\n",
    "flashing_test = flashing_test[:, 0:-1:SCALE_FACTOR]\n",
    "stimulus_test = stimulus_test[:, 0:-1:SCALE_FACTOR]\n",
    "\n",
    "print(\"# Samples of EEG signals after downsampling: {}\".format(len(signals_test[0])))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "48sta55OVXFy",
    "colab_type": "code",
    "outputId": "fa6aa9d1-0f93-418b-ff8b-5fd0fa9cab82",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424471791,
     "user_tz": -60,
     "elapsed": 7564,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "# Number of EEG channels\n",
    "N_CHANNELS = len(channels)\n",
    "# Window duration after each flashing [ms]\n",
    "WINDOW_DURATION = 650\n",
    "# Number of samples of each window\n",
    "WINDOW_SAMPLES = round(sampling_frequency * (WINDOW_DURATION / 1000))\n",
    "# Number of samples for each character in trials\n",
    "samples_per_trial_test = len(signals_train[0])\n",
    "\n",
    "test_features = []\n",
    "test_labels = []\n",
    "windowed_stimulus = []\n",
    "\n",
    "count_positive = 0\n",
    "count_negative = 0\n",
    "\n",
    "for trial in range(trials_test):\n",
    "    for sample in (range(samples_per_trial_test)):\n",
    "        if (sample == 0) or (flashing_test[trial, sample-1] == 0 and flashing_test[trial, sample] == 1):\n",
    "            lower_sample = sample\n",
    "            upper_sample = sample + WINDOW_SAMPLES\n",
    "            window = signals_test[trial, lower_sample:upper_sample, :]\n",
    "            # Extracting number of row/col in a window\n",
    "            number_stimulus = int(stimulus_code_test[trial, sample])\n",
    "            windowed_stimulus.append(number_stimulus)\n",
    "            # Features extraction\n",
    "            test_features.append(window)\n",
    "            # Labels extraction\n",
    "            if stimulus_test[trial, sample] == 1:\n",
    "                count_positive += 1\n",
    "                test_labels.append(1) # Class P300\n",
    "            else:\n",
    "                count_negative += 1\n",
    "                test_labels.append(0) # Class no-P300\n",
    "\n",
    "# Get test weights to take into account the number of classes \n",
    "test_weights = []\n",
    "for i in range(len(test_labels)):\n",
    "    if test_labels[i] == 1:\n",
    "        test_weights.append(len(test_labels)/count_positive)\n",
    "    else:\n",
    "        test_weights.append(len(test_labels)/count_negative)\n",
    "test_weights = np.array(test_weights)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "test_features = np.array(test_features)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# 3D tensor (SAMPLES, 64, 78)\n",
    "dim_test = test_features.shape\n",
    "print(\"Features tensor shape: {}\".format(dim_test))\n",
    "\n",
    "# Data normalization Zi = (Xi - mu) / sigma\n",
    "for pattern in range(len(test_features)):\n",
    "    test_features[pattern] = scale(test_features[pattern], axis=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VK4MFXTjHoM",
    "colab_type": "text"
   },
   "source": [
    "# CNN1 model definition, training and testing\n",
    "\n",
    "1.   Function definiton for randomization of weights and biases;\n",
    "2.   **Scaled_tanh(x)** activation function definition;\n",
    "3.   ANN model definition (2 Conv1D layers, 2 dense layers);\n",
    "4.   Training of the network over weighted dataset;\n",
    "5.   CNN1 performance assessment;"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jVMdFs4quaZL",
    "colab_type": "code",
    "outputId": "101cc1d0-a9f7-4430-bc3b-a0c4c3884283",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424483408,
     "user_tz": -60,
     "elapsed": 8234,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 505
    }
   },
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Randomizing function for bias and weights of the network\n",
    "def cecotti_normal(shape, dtype=None):\n",
    "    if len(shape) == 1:\n",
    "        fan_in = shape[0]\n",
    "    elif len(shape) == 2:\n",
    "        fan_in = shape[0]\n",
    "    else:\n",
    "        receptive_field_size = 1\n",
    "        for dim in shape[:-2]:\n",
    "            receptive_field_size *= dim\n",
    "        fan_in = shape[-2] * receptive_field_size\n",
    "    stddev = 1.0 / fan_in\n",
    "    return K.random_normal(shape, mean=0.0, stddev=stddev, dtype=dtype)\n",
    "\n",
    "# Custom tanh activation function\n",
    "def scaled_tanh(z):\n",
    "    return 1.7159 * K.tanh((2.0 / 3.0) * z)\n",
    "\n",
    "# Build the model\n",
    "def CNN1_model(channels=64, filters=10):\n",
    "    model = Sequential([\n",
    "        Conv1D(\n",
    "            filters=filters,\n",
    "            kernel_size=1,\n",
    "            padding=\"same\",\n",
    "            bias_initializer=cecotti_normal,\n",
    "            kernel_initializer=cecotti_normal,\n",
    "            use_bias=True,\n",
    "            activation=lambda x: scaled_tanh(x),\n",
    "            input_shape=(78, channels)\n",
    "        ),\n",
    "        Conv1D(\n",
    "            filters=50,\n",
    "            kernel_size=13,\n",
    "            padding=\"valid\",\n",
    "            strides=11,\n",
    "            bias_initializer=cecotti_normal,\n",
    "            kernel_initializer=cecotti_normal,\n",
    "            use_bias=True,\n",
    "            activation=lambda x: scaled_tanh(x),\n",
    "        ),\n",
    "        Flatten(),\n",
    "        Dense(100, activation=\"sigmoid\"),\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 256\n",
    "EPOCHS = 200\n",
    "VALID_SPLIT = 0.05\n",
    "SHUFFLE = 1  # set to 1 to shuffle subsets during training\n",
    "\n",
    "# Model summary\n",
    "model = CNN1_model(channels=64, filters=10)\n",
    "model.summary()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GEPpgyxyTl-u",
    "colab_type": "code",
    "outputId": "295e6ca1-c6fd-4c0b-b073-acfdbd5d72cb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576419828720,
     "user_tz": -60,
     "elapsed": 88347,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "# Callback to stop when loss on validation set doesn't decrease in 50 epochs\n",
    "earlystop = EarlyStopping(monitor='val_loss', \n",
    "                          mode='min', \n",
    "                          patience=50, \n",
    "                          restore_best_weights=True)\n",
    "\n",
    "# Train the model and store history\n",
    "history = model.fit(x=train_features, \n",
    "                    y=train_labels, \n",
    "                    batch_size=BATCH_SIZE, \n",
    "                    epochs=EPOCHS, \n",
    "                    validation_split=VALID_SPLIT, \n",
    "                    callbacks=[earlystop],\n",
    "                    shuffle=SHUFFLE,\n",
    "                    class_weight={0: 1., 1: train_ratio})\n",
    "\n",
    "# Find the epoch with the lowest validation loss\n",
    "best_epoch = np.argmin(history.history['val_loss'])\n",
    "print(f\"Best epoch: {best_epoch + 1} with validation loss: {history.history['val_loss'][best_epoch]}\")\n",
    "\n",
    "# Reload the best weights manually\n",
    "best_weights = model.get_weights()\n",
    "model.set_weights(best_weights)\n",
    "\n",
    "# Define a new model to reuse the best configuration\n",
    "best_model = CNN1_model(channels=64, filters=10)\n",
    "best_model.set_weights(best_weights)\n",
    "\n",
    "# Compile the best model\n",
    "best_model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "eJHaj2f1VFYZ",
    "colab_type": "code",
    "outputId": "be2c6164-20e5-49b0-85cb-8df2814b654c",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576419829416,
     "user_tz": -60,
     "elapsed": 89032,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15,4))\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "# Plots of loss curves during training\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label=\"training loss\")\n",
    "plt.plot(history.history['val_loss'], label=\"validation loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plots of accuracy curves during training\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['accuracy'], label=\"training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'], label=\"validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_j_iqu0tYlbS",
    "colab_type": "code",
    "outputId": "8335bee2-ed79-420e-8205-44bfffb48d9f",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424503761,
     "user_tz": -60,
     "elapsed": 3662,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    }
   },
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mne\n",
    "import csv\n",
    "from bundle.DataCraft import electrode_names_to_remove\n",
    "\n",
    "# Read CSV file into a Python list\n",
    "electrode_names = []\n",
    "\n",
    "with open(channel_name_file_path, \"r\") as file:\n",
    "    csv_reader = csv.reader(file)\n",
    "    for row in csv_reader:\n",
    "        electrode_names.extend(row)  # Add each row to the list (handles single-column CSV)\n",
    "\n",
    "print(electrode_names)\n",
    "\n",
    "\n",
    "# Create array has the original index for electrode_names_to_remove in electrode_names\n",
    "index = []\n",
    "for i in range(len(electrode_names_to_remove)):\n",
    "    index.append(electrode_names.index(electrode_names_to_remove[i]))\n",
    "print(index)\n",
    "\n",
    "electrode_names = [x for x in electrode_names if x not in electrode_names_to_remove]\n",
    "print(electrode_names)\n",
    "    \n",
    "    \n",
    "montage = mne.channels.make_standard_montage(\"standard_1020\")\n",
    "\n",
    "# Create random weights for 10 filters (20 channels)\n",
    "n_channels = len(electrode_names)\n",
    "n_filters = 10\n",
    "\n",
    "# Create a list of weights for all filters\n",
    "nf = []\n",
    "for filt in range(10):\n",
    "    nf.append(abs(np.array(best_model.get_weights()[0])[0, :, filt]))\n",
    "# remove the channels from the weights \n",
    "nf = np.delete(nf, index, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Create an Info object with EEG channel names\n",
    "info = mne.create_info(ch_names=electrode_names, sfreq=1000, ch_types=\"eeg\")\n",
    "info.set_montage(montage)\n",
    "\n",
    "# Plot topomap for each filter\n",
    "# Set give me array of background color\n",
    "fig, axes = plt.subplots(2, 5, figsize=(15, 6), facecolor=\"#000000\")  # Set overall background to black\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_filters):\n",
    "    ax = axes[i]\n",
    "    mne.viz.plot_topomap(\n",
    "        nf[i], info, axes=ax, cmap=\"Spectral_r\", sphere=1.2, show=False  # Adjust `sphere` to scale brain size\n",
    "    )\n",
    "    ax.set_title(f\"Filter #{i + 1}\", color=\"white\")  # Set title color to white for better visibility\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "EzbVkDAwUmfa",
    "colab_type": "code",
    "outputId": "3ae0a45e-7222-49a3-893d-aa9395db4296",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424508945,
     "user_tz": -60,
     "elapsed": 8821,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    }
   },
   "source": [
    "# Accuracy over the testing set\n",
    "predictions = best_model.predict(test_features)\n",
    "predictions = np.round(predictions)\n",
    "\n",
    "score = np.array(best_model.evaluate(test_features, test_labels, verbose=0, sample_weight=test_weights))\n",
    "print(\"Model performance on test set:\\t[ Loss: {}\\tAccuracy: {} ]\".format(*score.round(4)))\n",
    "print(\"\\nPredictions: {}\\nSolutions:   {}\".format(list(map(int, predictions))[:50], list(map(int, test_labels))[:50]))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "C_2LTwzNXOtY",
    "colab_type": "code",
    "outputId": "6acc408d-6c51-4cde-8f93-4a981803dd5b",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424509519,
     "user_tz": -60,
     "elapsed": 9378,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 392
    }
   },
   "source": [
    "# Weighted confusion matrix (noP300: 80%, P300: 20%)\n",
    "data_train = confusion_matrix(y_true=test_labels, y_pred=predictions, sample_weight=test_weights)\n",
    "\n",
    "# Normalized confusion matrix (values in range 0-1)\n",
    "data_norm = data_train / np.full(data_train.shape, len(test_labels))\n",
    "\n",
    "# Plot the confusion matrix\n",
    "df_cm = pd.DataFrame(data_norm, columns=np.unique(test_labels), index = np.unique(test_labels))\n",
    "df_cm.index.name = 'Actual'\n",
    "df_cm.columns.name = 'Predicted'\n",
    "plt.figure(figsize = (6,5))\n",
    "sns.set(font_scale = 1.4)\n",
    "cm = sns.heatmap(df_cm, cmap=\"Blues\", annot=True, annot_kws = {\"size\": 16}, vmin=0, vmax=1)\n",
    "cm.axes.set_title(\"CNN1 confusion matrix\\n\", fontsize=20)\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qA23jYMhGIXt",
    "colab_type": "code",
    "colab": {}
   },
   "source": [
    "# Model metrics (sens, spec, ppv, npv)\n",
    "def model_metrics(conf_matrix):\n",
    "    tn, fp, fn, tp = list(data_norm.flatten())\n",
    "    sens = round(tp/(tp+fn),4) # Sensitivity\n",
    "    spec = round(tn/(tn+fp),4) # Specificity\n",
    "    ppv = round(tp/(tp+fp),4) # Positive Predicted Value\n",
    "    npv = round(tn/(tn+fn),4) # Negative Predicted Value\n",
    "    return {\"Sensitivity\":sens, \"Specificity\":spec, \"PPV\":ppv, \"NPV\":npv}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZfpCK_rFVbhR",
    "colab_type": "code",
    "outputId": "6bc67266-1bcb-4708-98b9-e6243d14af95",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1576424509522,
     "user_tz": -60,
     "elapsed": 9313,
     "user": {
      "displayName": "Lorenzo Gualniera",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mCLpjQsro-Xs6X0aZX_0HuLLblFWB6HZFMeyZLy=s64",
      "userId": "15976103540540623653"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    }
   },
   "source": [
    "# Put model metrics into a table\n",
    "metrics = model_metrics(data_norm)\n",
    "\n",
    "# Create figure\n",
    "fig = plt.figure(figsize=(5,1))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Hide graph outlines\n",
    "for item in [fig, ax]:\n",
    "    item.patch.set_visible(False)\n",
    "ax.xaxis.set_visible(False)\n",
    "ax.yaxis.set_visible(False)\n",
    "ax.spines[\"top\"].set_visible(False)\n",
    "ax.spines[\"bottom\"].set_visible(False)\n",
    "ax.spines[\"left\"].set_visible(False)\n",
    "ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "# Table definition\n",
    "table = ax.table(cellText=[list(metrics.values())], \n",
    "                     colLabels=list(metrics.keys()),\n",
    "                     loc=\"center\",\n",
    "                     cellLoc=\"center\",\n",
    "                     colColours=[\"c\"]*4)\n",
    "table.set_fontsize(16)\n",
    "table.scale(2,2)\n",
    "\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}
